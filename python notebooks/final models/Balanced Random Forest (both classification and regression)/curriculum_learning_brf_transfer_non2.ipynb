{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the necessary libraries and packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('data_train_clean_onset.csv')\n",
    "data_test = pd.read_csv('data_test_clean_onset.csv')\n",
    "data_train.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.concat([data_train, data_test], axis=0)\n",
    "print(data_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot how many nans there are for each participant in data_test\n",
    "\n",
    "test_unique_ids = data_test['Sbj_ID'].unique()\n",
    "test_unique_ids = test_unique_ids.tolist()\n",
    "len(test_unique_ids)\n",
    "\n",
    "# calculate how many nans there are for each participant in data_test\n",
    "nans_per_participant = {}\n",
    "for i in test_unique_ids:\n",
    "    nans_per_participant[i] = data_test[data_test['Sbj_ID'] == i]['delta'].isnull().sum()\n",
    "\n",
    "# get the ids of the pariticpants with the least nans\n",
    "nans_per_participant = {k: v for k, v in sorted(nans_per_participant.items(), key=lambda item: item[1])}\n",
    "print(nans_per_participant)\n",
    "\n",
    "# get the top 20 participants with the least nans\n",
    "top_20 = list(nans_per_participant.keys())[:20]\n",
    "\n",
    "# get data_test with only the top 20 participants with the least nans\n",
    "selected_participants_data = data_test[data_test['Sbj_ID'].isin(top_20)]\n",
    "\n",
    "# save the data_test with only the top 20 participants with the least nans\n",
    "selected_participants_data.to_csv('data_test_clean_onset_top_20.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_participants_data = pd.read_csv('data_test_clean_onset_top_20.csv')\n",
    "selected_participants_data.dropna(inplace=True)\n",
    "selected_participants = selected_participants_data['Sbj_ID'].unique().tolist()\n",
    "selected_participants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_timeline(mydata_no_nan):\n",
    " \n",
    "    # assert that the data has no NaNs\n",
    "    assert mydata_no_nan.isnull().sum().sum() == 0, 'There are NaNs in the data'\n",
    "    \n",
    "\n",
    "    # Create X and y from dataframes to use in scikit-learn (drop Label, SleepStage, Sbj_ID and ifCleanOnset columns)\n",
    "    X_all = mydata_no_nan.drop(['Label', 'Old_label', 'SleepStage', 'Sbj_ID', 'ifCleanOnset', 'Time2Sleep'], axis=1)\n",
    "    y_all = mydata_no_nan['Label']\n",
    "    y_old_all = mydata_no_nan['Old_label']\n",
    "    y_sleepstage_all = mydata_no_nan['SleepStage']\n",
    "\n",
    "    \n",
    "\n",
    "    return X_all, y_all, y_old_all, y_sleepstage_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix, \n",
    "                             roc_curve, roc_auc_score, auc)\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluate_classification(true_labels, predictions, prediction_probabilities,\n",
    "                            index=['10 minutes before sleep', 'Awake'], \n",
    "                            label =['10 minutes before sleep', 'Awake'], \n",
    "                            save_path='confusion_matrix.png', ifsaveplots = False, \n",
    "                            output_path=None, iftest = False, ifplot = True, classes = None, ifvocal = True):\n",
    "    \n",
    "    # Evaluate the performance of the model\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    if iftest:\n",
    "        if ifvocal:\n",
    "            print(f'Accuracy: {accuracy}')\n",
    "\n",
    "    # Classification report \n",
    "    report = classification_report(true_labels, predictions)\n",
    "    if iftest:\n",
    "        if ifvocal:\n",
    "            print(report)\n",
    "    \n",
    "    # Decide on the filename based on iftest\n",
    "    report_filename = 'test_classification_report.txt' if iftest else 'crossval_classification_report.txt'\n",
    "    \n",
    "    # Dictionary to store all metrics\n",
    "    metrics_dict = {}\n",
    "    metrics_dict['accuracy'] = accuracy\n",
    "    # Classification report \n",
    "    true_labels_report = true_labels != 'Awake'\n",
    "    true_labels_report = true_labels_report.astype(int)\n",
    "\n",
    "    predictions_report = predictions != 'Awake'\n",
    "    predictions_report = predictions_report.astype(int)\n",
    "\n",
    "    precision, recall, _, _ = precision_recall_fscore_support(true_labels_report, predictions_report, average='binary')\n",
    "    f1_weighted = f1_score(true_labels_report, predictions_report, average='weighted')\n",
    "    f1_macro = f1_score(true_labels_report, predictions_report, average='macro')\n",
    "    f1_pre_sleep = f1_score(true_labels_report, predictions_report, average=None)[0]\n",
    "    f1_awake = f1_score(true_labels_report, predictions_report, average=None)[1]\n",
    "    \n",
    "    metrics_dict['precision'] = precision\n",
    "    metrics_dict['recall'] = recall\n",
    "    metrics_dict['f1_weighted'] = f1_weighted\n",
    "    metrics_dict['f1_macro'] = f1_macro\n",
    "    metrics_dict['f1_pre_sleep'] = f1_pre_sleep\n",
    "    metrics_dict['f1_awake'] = f1_awake\n",
    "\n",
    "    if iftest:\n",
    "        if ifvocal:\n",
    "            print('F1 pre-sleep: ', f1_pre_sleep)\n",
    "            print('F1 awake: ', f1_awake)\n",
    "    \n",
    "\n",
    "\n",
    "    # Save classification report to a .txt file\n",
    "    if ifsaveplots:\n",
    "        if output_path:\n",
    "            report_filename = os.path.join(output_path, report_filename)\n",
    "        with open(report_filename, 'w') as f:\n",
    "            f.write(report)\n",
    "    # Plot the confusion matrix\n",
    "\n",
    "    # Include the names of the classes in the confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predictions, normalize='true', labels=index)\n",
    "    cm = pd.DataFrame(cm, index=index, columns=label)\n",
    "\n",
    "    # Rename the columns and rows to the names of the classes\n",
    "    cm.index = label\n",
    "    plt.figure(figsize=(5.5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues')\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title('Confusion Matrix for times before sleep')\n",
    "    \n",
    "    if iftest:\n",
    "        saving_prefix = 'test'\n",
    "    else: \n",
    "        saving_prefix = 'crossval'\n",
    "\n",
    "    save_path = f'{saving_prefix}_{save_path}'\n",
    "\n",
    "    if ifsaveplots:\n",
    "        if output_path:\n",
    "            save_path = os.path.join(output_path, save_path)\n",
    "        # Save the confusion matrix to a file\n",
    "        plt.savefig(save_path)\n",
    "    \n",
    "    # Display the confusion matrix\n",
    "    if ifplot:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    roc_path = f'{saving_prefix}_roc_curve.png'\n",
    "    \n",
    "    # ROC and AUC\n",
    "\n",
    "    # if array is 2D\n",
    "    if len(prediction_probabilities.shape) == 2:\n",
    "        class_idx = np.where(classes != 'Awake')[0][0]\n",
    "        prediction_probabilities = prediction_probabilities[:,class_idx]\n",
    "    else:\n",
    "        prediction_probabilities = prediction_probabilities.ravel()\n",
    "\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(true_labels_report, prediction_probabilities.ravel())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    #metrics_dict['roc_curve'] = (fpr, tpr)\n",
    "    metrics_dict['auc'] = roc_auc\n",
    "\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "\n",
    "    if ifsaveplots:\n",
    "        if output_path:\n",
    "            roc_path = os.path.join(output_path, roc_path)\n",
    "        plt.savefig(roc_path)\n",
    "    \n",
    "    if ifplot:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    return metrics_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def preprocess_data(y_pred, y_true, timeline, sleep_stage):\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = np.array(y_true)\n",
    "    timeline = np.array(timeline)\n",
    "    sleep_stage = np.array(sleep_stage)\n",
    "    timeline = np.round(timeline, 1)\n",
    "    return y_pred, y_true, timeline, sleep_stage\n",
    "\n",
    "def get_indices_for_unique_values(timeline):\n",
    "    unique_values = np.unique(timeline)\n",
    "    unique_values_sorted = np.sort(unique_values)\n",
    "    indices = {}\n",
    "    for value in unique_values:\n",
    "        indices[value] = np.where(timeline == value)[0]\n",
    "    return dict(sorted(indices.items())), unique_values_sorted\n",
    "\n",
    "def update_indices_with_missing_values(indices, rnn_window):\n",
    "    timegrid = np.arange(0, 30.1 - rnn_window, 0.1)\n",
    "    timegrid = np.round(timegrid, 1)\n",
    "    missing_values = np.setdiff1d(timegrid, list(indices.keys()))\n",
    "    for value in missing_values:\n",
    "        indices[value] = np.nan\n",
    "    return dict(sorted(indices.items())), timegrid\n",
    "\n",
    "def sort_predictions_by_time(y_pred, indices, timegrid):\n",
    "    y_pred_sorted = [np.mean(y_pred[indices[value]]) if not np.isnan(indices[value]).any() else np.nan for value in timegrid]\n",
    "    y_pred_std = [np.std(y_pred[indices[value]]) if not np.isnan(indices[value]).any() else np.nan for value in timegrid]\n",
    "    return y_pred_sorted, y_pred_std\n",
    "\n",
    "def downsample_data(data, window_for_averaging):\n",
    "    return [np.nanmean(data[i:i+window_for_averaging]) for i in range(0, len(data), window_for_averaging)]\n",
    "\n",
    "def plot_data(y_pred_mean, y_pred_std, sleep_stage_mean, sleep_stage_std, timegrid, window_for_averaging, rnn_window, pre_sleep_window=None, awake_window=None, ifsaveplots=False, output_path=None, ifplot = False, ifsubjecttest = False, change_points_times = None):\n",
    "\n",
    "    # convert to numpy arrays to avoid errors\n",
    "    y_pred_mean = np.array(y_pred_mean)\n",
    "    y_pred_std = np.array(y_pred_std)\n",
    "    sleep_stage_mean = np.array(sleep_stage_mean)\n",
    "    sleep_stage_std = np.array(sleep_stage_std)\n",
    "\n",
    "    def plot_graph(include_std, include_crossings, filename_suffix):\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.xlabel('Time to sleep onset (min)', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('Probability', fontsize=14, fontweight='bold')\n",
    "        xticks = np.arange(0, len(y_pred_mean), 20 // window_for_averaging)\n",
    "        plt.xticks(xticks, timegrid[xticks * window_for_averaging], fontsize=12, fontweight='bold')  \n",
    "        plt.yticks(fontsize=12, fontweight='bold')\n",
    "\n",
    "        plt.plot(y_pred_mean, label='Predicted Probability' if ifsubjecttest else 'Mean Predicted Probability', color='blue')\n",
    "        if include_std:\n",
    "            plt.fill_between(np.arange(len(y_pred_mean)), y_pred_mean - y_pred_std, y_pred_mean + y_pred_std, color='blue', alpha=0.1)\n",
    "\n",
    "        plt.plot(sleep_stage_mean, label='Sleep Stages' if ifsubjecttest else 'Mean Sleep Stages', color='purple', linestyle='--', linewidth=2.5)\n",
    "        if include_std:\n",
    "            plt.fill_between(np.arange(len(sleep_stage_mean)), sleep_stage_mean - sleep_stage_std, sleep_stage_mean + sleep_stage_std, color='purple', alpha=0.1)\n",
    "\n",
    "        plt.hlines(0.5, linestyle='--', color='r', xmin=0, xmax=len(y_pred_mean), label='Threshold for classifying as pre-sleep onset', linewidth=2.5)\n",
    "        if include_crossings and change_points_times is not None:\n",
    "            plt.vlines(change_points_times, ymin=-0.1, ymax=1.1, color='seagreen', label='Detected change points', linewidth=2.5, linestyle='--')\n",
    "\n",
    "        if awake_window:\n",
    "            awake_window_probas = int((awake_window - rnn_window) * 10 // window_for_averaging)\n",
    "            plt.axvspan(awake_window_probas, len(y_pred_mean), facecolor='g', alpha=0.2, label='Awake window')\n",
    "        if pre_sleep_window:\n",
    "            pre_sleep_window_probas = int((pre_sleep_window - rnn_window) * 10 // window_for_averaging)\n",
    "            plt.axvspan(pre_sleep_window_probas, 0, facecolor='r', alpha=0.2, label='Pre-sleep window')\n",
    "\n",
    "        plt.ylim(-0.1, 1.1)\n",
    "        plt.legend(loc='upper right', fontsize=14)\n",
    "        filename = f\"sleep_probabilities_{filename_suffix}_averaged_over_{window_for_averaging}.jpg\"\n",
    "        if ifsaveplots:\n",
    "            if output_path:\n",
    "                filename = os.path.join(output_path, filename)\n",
    "            plt.savefig(filename, format='jpg', dpi=200)\n",
    "        if ifplot:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    plot_graph(False, False, \"without_std\")\n",
    "    plot_graph(True, False, \"with_std\")\n",
    "    plot_graph(False, True, \"without_std_with_crossings\")\n",
    "    plot_graph(True, True, \"with_std_with_crossings\")\n",
    "\n",
    "\n",
    "\n",
    "def calculate_metrics(y_pred, y_true, timegrid, indices):\n",
    "    correct = y_true == y_pred\n",
    "    accuracies = [np.mean(correct[indices[value]]) if not np.isnan(indices[value]).any() else np.nan for value in timegrid]\n",
    "\n",
    "    prediction_dict_by_time = {value: y_pred[indices[value]] if indices[value] is not np.nan else np.nan for value in timegrid}\n",
    "    true_values_dict_by_time = {value: y_true[indices[value]] if indices[value] is not np.nan else np.nan for value in timegrid}\n",
    "\n",
    "    tp = {value: np.sum(np.logical_and(prediction_dict_by_time[value] == 1, true_values_dict_by_time[value] == 1)) if indices[value] is not np.nan else np.nan for value in timegrid}\n",
    "    tn = {value: np.sum(np.logical_and(prediction_dict_by_time[value] == 0, true_values_dict_by_time[value] == 0)) if indices[value] is not np.nan else np.nan for value in timegrid}\n",
    "    fp = {value: np.sum(np.logical_and(prediction_dict_by_time[value] == 1, true_values_dict_by_time[value] == 0)) if indices[value] is not np.nan else np.nan for value in timegrid}\n",
    "    fn = {value: np.sum(np.logical_and(prediction_dict_by_time[value] == 0, true_values_dict_by_time[value] == 1)) if indices[value] is not np.nan else np.nan for value in timegrid}\n",
    "\n",
    "    recall = [tp[value] / (tp[value] + fn[value]) for value in timegrid]\n",
    "    precision = [tp[value] / (tp[value] + fp[value]) for value in timegrid]\n",
    "    f1 = [2 * (precision[i] * recall[i]) / (precision[i] + recall[i]) if precision[i] + recall[i] != 0 else np.nan for i in range(len(precision))]\n",
    "\n",
    "    return accuracies, recall, precision, f1\n",
    "\n",
    "def plot_metric(metric_values, metric_name, color, timegrid, window_for_averaging, rnn_window,\n",
    "                pre_sleep_window=None, awake_window=None, ifsaveplots=False, output_path=None,\n",
    "                ifplot = False):\n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.xlabel('Minutes', fontsize=14)\n",
    "    xticks = np.arange(0, len(metric_values), 10//window_for_averaging)\n",
    "    plt.xticks(xticks, timegrid[xticks*window_for_averaging], fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    plt.plot(metric_values, label=metric_name, color=color)\n",
    "    #plt.hlines(0.5, linestyle='--', color='r', label='Threshold for classifying as pre-sleep onset', xmin=0, xmax=len(metric_values))\n",
    "\n",
    "    if awake_window is not None:\n",
    "        awake_window_probas = int((awake_window - rnn_window)* 10//window_for_averaging)\n",
    "        plt.axvspan(awake_window_probas, len(metric_values), facecolor='g', alpha=0.2, label='Awake window')\n",
    "\n",
    "    if pre_sleep_window is not None:\n",
    "        pre_sleep_window_probas = int((pre_sleep_window - rnn_window) * 10//window_for_averaging)\n",
    "        plt.axvspan(pre_sleep_window_probas, 0, facecolor='r', alpha=0.2, label='Pre-sleep window')\n",
    "\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "    plt.legend(loc='upper right', fontsize=12)\n",
    "    if ifsaveplots:\n",
    "        filename = f\"{metric_name}_metric_averaged_over_{window_for_averaging}.jpg\"\n",
    "        if output_path:\n",
    "            filename = os.path.join(output_path, filename)\n",
    "        plt.savefig(filename, format='jpg', dpi=300)\n",
    "\n",
    "    if ifplot:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_metrics(accuracies, #recall, precision, f1,\n",
    "                 timegrid, window_for_averaging, rnn_window,\n",
    "                 pre_sleep_window=None, awake_window=None,\n",
    "                 ifsaveplots=False, output_path=None, ifplot = False):\n",
    "\n",
    "    plot_metric(accuracies, 'Accuracy', 'blue', timegrid, window_for_averaging, rnn_window,\n",
    "                pre_sleep_window, awake_window, ifsaveplots, output_path, ifplot = ifplot)\n",
    "\n",
    "    #plot_metric(recall, 'Recall', 'green', timegrid, window_for_averaging, rnn_window,\n",
    "                #pre_sleep_window, awake_window, ifsaveplots, output_path, ifplot = ifplot)\n",
    "\n",
    "    #plot_metric(precision, 'Precision', 'purple', timegrid, window_for_averaging, rnn_window,\n",
    "                #pre_sleep_window, awake_window, ifsaveplots, output_path, ifplot = ifplot)\n",
    "\n",
    "    #plot_metric(f1, 'F1', 'red', timegrid, window_for_averaging, rnn_window,\n",
    "                #pre_sleep_window, awake_window, ifsaveplots, output_path, ifplot = ifplot)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_convolve(array, window, mode='valid'):\n",
    "    if mode != 'valid':\n",
    "        raise ValueError(\"Only 'valid' mode is currently supported\")\n",
    "\n",
    "    output = np.zeros(len(array) - len(window) + 1)\n",
    "    for i in range(len(output)):\n",
    "        current_slice = array[i: i + len(window)]\n",
    "        valid_data = current_slice[~np.isnan(current_slice)]\n",
    "        valid_window = window[:len(valid_data)]\n",
    "        output[i] = np.sum(valid_data * valid_window)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_consistency_change_point(data, times, threshold = 0.5, consistency_length=None, window_size=20, proportion=0.7, stay_above_proportion=0.8):\n",
    "    #assert len(data) == len(times), \"Data and times must have the same length\"\n",
    "    \n",
    "    if type(data) == list:\n",
    "        data = np.array(data)\n",
    "    if type(times) == list:\n",
    "        times = np.array(times)\n",
    "    \n",
    "    \n",
    "    data = data[::-1]\n",
    "    \n",
    "    above = data > threshold\n",
    "\n",
    "\n",
    "    change_points_consistent = np.array([])  # Initialize here\n",
    "    # reverse data\n",
    "    \n",
    "    #print(data)\n",
    "    #print(times)\n",
    "\n",
    "    # Check for consistent crossing\n",
    "    if consistency_length:\n",
    "        consistent_above = nan_convolve(above, np.ones(consistency_length), 'valid') == consistency_length\n",
    "        change_points_consistent = np.where(consistent_above)[0] + consistency_length - 1\n",
    "\n",
    "    # Check for proportion in a running window\n",
    "    if not consistency_length or (window_size and proportion):\n",
    "        within_window = nan_convolve(above, np.ones(window_size), 'valid') / window_size\n",
    "        change_points_window = np.where(within_window > proportion)[0] + window_size - 1\n",
    "        change_points_indices = np.sort(np.unique(np.concatenate([change_points_consistent, change_points_window])))\n",
    "    else:\n",
    "        change_points_indices = change_points_consistent\n",
    "\n",
    "    old_indices = change_points_indices.copy()\n",
    "\n",
    "    final_change_points = np.array([])  # Initialize here\n",
    "    last_index = None\n",
    "\n",
    "    \n",
    "    # change to integer\n",
    "    change_points_indices = change_points_indices.astype(int)\n",
    "    #print(\"change_points_indices: \", change_points_indices)\n",
    "    \n",
    "    for index in change_points_indices:\n",
    "        #print(\"index: \", index)\n",
    "        preceding_data = data[:index]\n",
    "        # calculate how many values are above the threshold (>threshold) while omitting nans\n",
    "        percentage =  np.nansum(preceding_data > threshold)/ np.sum(~np.isnan(preceding_data))\n",
    "       #print(\"percentage: \", percentage)\n",
    "        if percentage > stay_above_proportion:\n",
    "           #print('Delete index: ', index)\n",
    "            # delete the last entry from the final change points\n",
    "            if len(final_change_points) > 0:\n",
    "                final_change_points = np.delete(final_change_points, -1)\n",
    "            \n",
    "            change_points_indices = np.delete(change_points_indices, np.where(change_points_indices == index)[0])\n",
    "            #print(\"change_points_indices after deletion: \", change_points_indices)\n",
    "            # add index to final change points\n",
    "\n",
    "            final_change_points = np.append(final_change_points, index)\n",
    "            #print(\"final_change_points after deletion: \", final_change_points)\n",
    "        else:\n",
    "            #print('index that broke consistency: ', index)\n",
    "            # add the remaining change points (change_points_indices) to the final change points \n",
    "            final_change_points = np.concatenate([final_change_points, change_points_indices])\n",
    "            #print(\"final_change_points after concatenation: \", final_change_points)\n",
    "            break\n",
    "\n",
    "   \n",
    "    if len(final_change_points) == 0 and len(old_indices) > 0:\n",
    "        final_change_points = np.array([old_indices[-1]])\n",
    "\n",
    "    # Ensure all indices are integers\n",
    "    final_change_points = final_change_points.astype(int)\n",
    "\n",
    "    # Convert indices to associated times\n",
    "    change_points_times = times[final_change_points].tolist()\n",
    "\n",
    "    # Check if the last timepoint is the largest one in the original time grid\n",
    "    if final_change_points is not None and len(final_change_points) > 0:\n",
    "        # the last index of data that is not nan\n",
    "        last_index = np.where(~np.isnan(data))[0][-1]\n",
    "        if final_change_points[0] == last_index:\n",
    "            return np.array([]), np.array([])\n",
    "        elif change_points_times[0] > 28:\n",
    "            return np.array([]), np.array([])\n",
    "\n",
    "\n",
    "    return final_change_points, change_points_times\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate test data\n",
    "np.random.seed(42)\n",
    "times = np.arange(0, 100)\n",
    "above_threshold = np.random.uniform(0.6, 0.8, 40)\n",
    "above_threshold[15:17] = np.random.uniform(0.3, 0.5, 2)\n",
    "below_threshold = np.random.uniform(0.3, 0.5, 60)\n",
    "data = np.concatenate([above_threshold, below_threshold])\n",
    "data = data[::-1]\n",
    "\n",
    "# Inserting NaNs at random positions\n",
    "nan_indices = np.random.choice(len(data), 10, replace=False)  # 10 random indices to replace with NaN\n",
    "data[nan_indices] = np.nan\n",
    "\n",
    "\n",
    "threshold = 0.55\n",
    "change_points = threshold_consistency_change_point(data, times, threshold, consistency_length=5, window_size=10, proportion=0.6, stay_above_proportion=0.8)\n",
    "print(\"Identified Change Points:\", change_points)\n",
    "# Plot the test data\n",
    "plt.plot(times, data[::-1], label=\"Data\")\n",
    "plt.axhline(y=0.55, color='r', linestyle='--', label=\"Threshold\")\n",
    "plt.vlines(change_points, ymin=0, ymax=1, color='g', label=\"Change Points\")\n",
    "plt.xticks(np.arange(0, 101, 10), np.arange(0, 101, 10)*6/60)\n",
    "plt.xlabel('Minutes to sleep onset', fontsize=11)\n",
    "plt.ylabel('Probability', fontsize=11)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate test data\n",
    "np.random.seed(42)\n",
    "times = np.arange(0, 100)\n",
    "above_threshold1 = np.random.uniform(0.5, 0.8, 40)\n",
    "below_threshold1 = np.random.uniform(0.3, 0.5, 20)\n",
    "above_threshold2 = np.random.uniform(0.5, 0.8, 10)\n",
    "below_threshold2 = np.random.uniform(0.3, 0.5, 30)\n",
    "data = np.concatenate([above_threshold1, below_threshold1, above_threshold2, below_threshold2])\n",
    "data = data[::-1]\n",
    "\n",
    "threshold = 0.55\n",
    "change_points = threshold_consistency_change_point(data, times, threshold, consistency_length=None, window_size=10, proportion=0.6, stay_above_proportion=0.6)\n",
    "print(\"Identified Change Points:\", change_points)\n",
    "# Plot the test data\n",
    "plt.plot(times, data[::-1], label=\"Data\")\n",
    "plt.axhline(y=0.55, color='r', linestyle='--', label=\"Threshold\")\n",
    "plt.vlines(change_points, ymin=0, ymax=1, color='g', label=\"Change Points\")\n",
    "plt.xticks(np.arange(0, 101, 10), np.arange(0, 101, 10)*6/60)\n",
    "plt.xlabel('Minutes to sleep onset', fontsize=11)\n",
    "plt.ylabel('Probability', fontsize=11)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate test data\n",
    "np.random.seed(42)\n",
    "times = np.arange(0, 100)\n",
    "above_threshold1 = np.random.uniform(0.5, 0.8, 40)\n",
    "below_threshold1 = np.random.uniform(0.3, 0.5, 20)\n",
    "above_threshold2 = np.random.uniform(0.5, 0.8, 10)\n",
    "below_threshold2 = np.random.uniform(0.3, 0.5, 30)\n",
    "data = np.concatenate([above_threshold1, below_threshold1, above_threshold2, below_threshold2])\n",
    "data = data[::-1]\n",
    "\n",
    "# Inserting NaNs at random positions\n",
    "nan_indices = np.random.choice(len(data), 10, replace=False)  # 10 random indices to replace with NaN\n",
    "data[nan_indices] = np.nan\n",
    "\n",
    "threshold = 0.55\n",
    "change_points = threshold_consistency_change_point(data, times, threshold, consistency_length=None, window_size=10, proportion=0.6, stay_above_proportion=0.6)\n",
    "print(\"Identified Change Points:\", change_points)\n",
    "\n",
    "# Plot the test data\n",
    "plt.plot(times, data[::-1], label=\"Data\")\n",
    "plt.axhline(y=0.55, color='r', linestyle='--', label=\"Threshold\")\n",
    "plt.vlines(change_points, ymin=0, ymax=1, color='g', label=\"Change Points\")\n",
    "plt.xticks(np.arange(0, 101, 10), np.arange(0, 101, 10)*6/60)\n",
    "plt.xlabel('Minutes to sleep onset', fontsize=11)\n",
    "plt.ylabel('Probability', fontsize=11)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate test data\n",
    "np.random.seed(42)\n",
    "times = np.arange(0, 100)\n",
    "\n",
    "below_threshold = np.random.uniform(0.3, 0.5, 100)\n",
    "below_threshold[50:71] = np.random.uniform(0.8, 0.9, 21)\n",
    "data = np.concatenate([below_threshold])\n",
    "data = data[::-1]\n",
    "# Inserting NaNs at random positions\n",
    "nan_indices = np.random.choice(len(data), 10, replace=False)  # 10 random indices to replace with NaN\n",
    "data[nan_indices] = np.nan\n",
    "\n",
    "\n",
    "threshold = 0.55\n",
    "change_points = threshold_consistency_change_point(data, times, threshold)\n",
    "print(\"Identified Change Points:\", change_points)\n",
    "# Plot the test data\n",
    "plt.plot(times, data[::-1], label=\"Data\")\n",
    "plt.axhline(y=0.55, color='r', linestyle='--', label=\"Threshold\")\n",
    "plt.vlines(change_points, ymin=0, ymax=1, color='g', label=\"Change Points\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(np.arange(0, 101, 10), np.arange(0, 101, 10)*6/60)\n",
    "plt.xlabel('Minutes to sleep onset', fontsize=11)\n",
    "plt.ylabel('Probability', fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate test data\n",
    "np.random.seed(42)\n",
    "times = np.arange(0, 100)\n",
    "\n",
    "below_threshold = np.random.uniform(0.2, 0.3, 100)\n",
    "\n",
    "data = np.concatenate([below_threshold])\n",
    "data = data[::-1]\n",
    "# Inserting NaNs at random positions\n",
    "nan_indices = np.random.choice(len(data), 10, replace=False)  # 10 random indices to replace with NaN\n",
    "data[nan_indices] = np.nan\n",
    "\n",
    "\n",
    "threshold = 0.55\n",
    "change_points = threshold_consistency_change_point(data, times, threshold)\n",
    "print(\"Identified Change Points:\", change_points)\n",
    "# Plot the test data\n",
    "plt.plot(times, data[::-1], label=\"Below Threshold\", color='orange')\n",
    "plt.axhline(y=0.55, color='r', linestyle='--', label=\"Threshold\")\n",
    "plt.vlines(change_points, ymin=0, ymax=1, color='g', label=\"Change Points\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "\n",
    "\n",
    "above_threshold = np.random.uniform(0.6, 0.8, 100)\n",
    "\n",
    "data = np.concatenate([above_threshold])\n",
    "data = data[::-1]\n",
    "# Inserting NaNs at random positions\n",
    "nan_indices = np.random.choice(len(data), 10, replace=False)  # 10 random indices to replace with NaN\n",
    "data[nan_indices] = np.nan\n",
    "\n",
    "\n",
    "threshold = 0.55\n",
    "change_points = threshold_consistency_change_point(data, times, threshold)\n",
    "print(\"Identified Change Points:\", change_points)\n",
    "# Plot the test data\n",
    "plt.plot(times, data[::-1], label=\"Above Threshold\", color='blue')\n",
    "#plt.axhline(y=0.55, color='r', linestyle='--', label=\"Threshold\")\n",
    "#plt.vlines(change_points, ymin=0, ymax=1, color='g', label=\"Change Points\")\n",
    "\n",
    "# add list of change points as annotations\n",
    "#if len(change_points[0]) > 0:\n",
    "#    for change_point in change_points:\n",
    "#        plt.annotate(str(change_point), xy=(change_point, 0.5), xytext=(change_point, 0.5), fontsize=12)\n",
    "#else: \n",
    "#    plt.annotate('Identified Change Points: None', xy=(0.5, 1.1), xytext=(0.5, 1.1), fontsize=12)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylim(-0.2, 1.2)\n",
    "# transform ticks into minute (each tick is 6 seconds)\n",
    "plt.xticks(np.arange(0, 101, 10), np.arange(0, 101, 10)*6/60)\n",
    "plt.xlabel('Minutes to sleep onset', fontsize=11)\n",
    "plt.ylabel('Probability', fontsize=11)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_timeline(y_pred, y_true, timeline, sleep_stage, window_for_averaging=1, \n",
    "                            if_proba=False, classes=None, pre_sleep_window=None, \n",
    "                            awake_window=None, rnn_window=None, ifsaveplots=False, output_path=None,\n",
    "                            ifplot = False, ifsubjecttest = False, ifcrossings = False):\n",
    "    y_pred, y_true, timeline, sleep_stage = preprocess_data(y_pred, y_true, timeline, sleep_stage)\n",
    "    \n",
    "    indices, unique_values_sorted = get_indices_for_unique_values(timeline)\n",
    "    indices, timegrid = update_indices_with_missing_values(indices, rnn_window)\n",
    "    \n",
    "    if if_proba:\n",
    "        if classes is not None:\n",
    "            class_idx = np.where(classes != 'Awake')[0][0]\n",
    "            \n",
    "\n",
    "            y_pred = y_pred[:,class_idx]\n",
    "        \n",
    "        y_pred_mean, y_pred_std = sort_predictions_by_time(y_pred, indices, timegrid)\n",
    "        sleep_stage_mean, sleep_stage_std = sort_predictions_by_time(sleep_stage, indices, timegrid)\n",
    "            \n",
    "            \n",
    "        if window_for_averaging > 1:\n",
    "            y_pred_mean = downsample_data(y_pred_mean, window_for_averaging)\n",
    "            y_pred_std = downsample_data(y_pred_std, window_for_averaging)\n",
    "            sleep_stage_mean= downsample_data(sleep_stage_mean, window_for_averaging)\n",
    "            sleep_stage_std = downsample_data(sleep_stage_std, window_for_averaging)\n",
    "            downsampled_timegrid = downsample_data(timegrid, window_for_averaging)\n",
    "        \n",
    "\n",
    "        change_points_times = np.array([])\n",
    "        final_change_points = np.array([])\n",
    "        \n",
    "        if ifcrossings:\n",
    "            window_size = int(20/window_for_averaging)\n",
    "            if window_for_averaging > 1:\n",
    "                dict_pred_time = {downsampled_timegrid[i]: y_pred_mean[i] for i in range(len(y_pred_mean))}\n",
    "                # sort in descending order according to keys\n",
    "                sorted_dict_pred_time = dict(sorted(dict_pred_time.items(), reverse=True))\n",
    "                # get the values of sorted dictionary\n",
    "                y_probas_sorted = list(sorted_dict_pred_time.values())\n",
    "                \n",
    "            \n",
    "                final_change_points, change_points_times = threshold_consistency_change_point(data = y_probas_sorted,                                                  \n",
    "                                                                        times = downsampled_timegrid, \n",
    "                                                                        threshold=0.5, \n",
    "                                                                        window_size=window_size, proportion=0.8, stay_above_proportion=0.6)\n",
    "            else:\n",
    "                dict_pred_time = {timegrid[i]: y_pred_mean[i] for i in range(len(y_pred_mean))}\n",
    "                # sort in descending order according to keys\n",
    "                sorted_dict_pred_time = dict(sorted(dict_pred_time.items(), reverse=True))\n",
    "                # get the values of sorted dictionary\n",
    "                y_probas_sorted = list(sorted_dict_pred_time.values())\n",
    "            \n",
    "                final_change_points, change_points_times = threshold_consistency_change_point(data = y_probas_sorted,                                                  \n",
    "                                                                        times = timegrid, \n",
    "                                                                        threshold=0.5, \n",
    "                                                                        window_size=window_size, proportion=0.8, stay_above_proportion=0.6)\n",
    "            \n",
    "            num_change_points = len(final_change_points)\n",
    "            if num_change_points == 1:\n",
    "                time_of_prediction = change_points_times[0]\n",
    "            else:\n",
    "                time_of_prediction = np.nan\n",
    "           \n",
    "\n",
    "        plot_data(y_pred_mean, y_pred_std, sleep_stage_mean, sleep_stage_std,\n",
    "                  timegrid, window_for_averaging, rnn_window, \n",
    "                  pre_sleep_window, awake_window, ifsaveplots, \n",
    "                  output_path, ifplot = ifplot, ifsubjecttest = ifsubjecttest, \n",
    "                   change_points_times = final_change_points)\n",
    "\n",
    "        if window_for_averaging > 1:\n",
    "            if ifcrossings:\n",
    "                return num_change_points, time_of_prediction\n",
    "            else:\n",
    "                dict_pred_time = {downsampled_timegrid[i]: y_pred_mean[i] for i in range(len(y_pred_mean))}\n",
    "                return dict_pred_time\n",
    "        elif ifcrossings:\n",
    "            return num_change_points, time_of_prediction \n",
    "        else: \n",
    "            return y_pred_mean\n",
    "\n",
    "    else:\n",
    "        accuracies, recall, precision, f1 = calculate_metrics(y_pred, y_true, timegrid, indices)\n",
    "        if window_for_averaging > 1:\n",
    "            accuracies = downsample_data(accuracies, window_for_averaging)\n",
    "            #recall = downsample_data(recall, window_for_averaging)\n",
    "            #precision = downsample_data(precision, window_for_averaging)\n",
    "            #f1 = downsample_data(f1, window_for_averaging)\n",
    "\n",
    "        \n",
    "        plot_metrics(accuracies, #recall, precision, f1, \n",
    "                     timegrid, window_for_averaging, rnn_window, \n",
    "                     pre_sleep_window, awake_window, ifsaveplots, \n",
    "                     output_path, ifplot = ifplot)\n",
    "        \n",
    "        \n",
    "        return accuracies, recall, precision, f1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_regression(predictions, y_test, ifsave = False, savepath = None, ifplot = False, iftest = False):\n",
    "\n",
    "    # check if the predictions are in the correct format\n",
    "   \n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # check if the y_test is in the correct format\n",
    "    \n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    errors = abs(y_test - predictions)\n",
    "    mape = np.where(y_test != 0, 100 * (errors / y_test), 0)\n",
    "    accuracy = 100 - np.mean(mape)\n",
    "    mse = sklearn.metrics.mean_squared_error(y_test, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = sklearn.metrics.r2_score(y_test, predictions)\n",
    "    squared_errors = np.square(y_test - predictions)\n",
    "    weights = 1.0 / (abs(y_test) + 0.1)\n",
    "    custom_mse = np.mean(weights * squared_errors)\n",
    "\n",
    "    if ifplot:\n",
    "\n",
    "        print('Mean Absolute Error:', round(np.mean(errors), 2))\n",
    "        print('Accuracy:', round(accuracy, 2), '%.')\n",
    "        print('Mean Squared Error:', round(mse, 2))\n",
    "        print('Root Mean Squared Error:', round(rmse, 2))\n",
    "        print('R2:', round(r2, 2))\n",
    "        print('Custom MSE:', round(custom_mse, 2))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=1)\n",
    "    plt.scatter(y_test, predictions, alpha=0.2)\n",
    "    plt.xlabel('Actual time to sleep onset')\n",
    "    plt.ylabel('Predicted time to sleep onset')\n",
    "    plt.title('Actual vs Predicted time to sleep onset')\n",
    "    if ifsave:\n",
    "        if iftest:\n",
    "            plt.savefig(os.path.join(savepath, 'test_actual_vs_predicted.png'))\n",
    "        else:\n",
    "            plt.savefig(os.path.join(savepath, 'crossval_actual_vs_predicted.png'))\n",
    "    if ifplot:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    residuals = y_test - predictions\n",
    "    plt.scatter(y_test, residuals, alpha=0.2)\n",
    "    plt.plot([y_test.min(), y_test.max()], [0, 0], 'k--', lw=1)\n",
    "    plt.xlabel('Actual time to sleep onset')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Actual vs Residuals for predicted time to sleep onset')\n",
    "    if ifsave:\n",
    "        if iftest:\n",
    "            plt.savefig(os.path.join(savepath, 'test_actual_vs_residuals.png'))\n",
    "        else:\n",
    "            plt.savefig(os.path.join(savepath, 'crossval_actual_vs_residuals.png'))\n",
    "    if ifplot:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    plt.hist(residuals)\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Residuals distribution')\n",
    "    if ifsave:\n",
    "        if iftest:\n",
    "            plt.savefig(os.path.join(savepath, 'test_residuals_distribution.png'))\n",
    "        else:\n",
    "            plt.savefig(os.path.join(savepath, 'crossval_residuals_distribution.png'))\n",
    "    if ifplot:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # for each actual, plot the mean residual \n",
    "    # round y_test to 0.1 decimal place\n",
    "    y_test = np.round(y_test, 1)\n",
    "\n",
    "    predictions_residuals_dict = {}\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i] not in predictions_residuals_dict:\n",
    "            predictions_residuals_dict[y_test[i]] = []\n",
    "        predictions_residuals_dict[y_test[i]].append(residuals[i])\n",
    "    \n",
    "    # sort it in ascending order of keys\n",
    "    predictions_residuals_dict = dict(sorted(predictions_residuals_dict.items()))\n",
    "\n",
    "\n",
    "    actual_predictions_dict = {}\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i] not in actual_predictions_dict:\n",
    "            actual_predictions_dict[y_test[i]] = []\n",
    "        actual_predictions_dict[y_test[i]].append(predictions[i])\n",
    "    \n",
    "    # sort it in ascending order of keys\n",
    "    actual_predictions_dict = dict(sorted(actual_predictions_dict.items()))\n",
    "\n",
    "    \n",
    "    predictions_residuals_mean_dict = {}\n",
    "    for key in predictions_residuals_dict:\n",
    "        predictions_residuals_mean_dict[key] = np.mean(predictions_residuals_dict[key])\n",
    "    \n",
    "    # sort it in ascending order of keys\n",
    "    predictions_residuals_mean_dict = dict(sorted(predictions_residuals_mean_dict.items()))\n",
    "\n",
    "\n",
    "    residuals_std_dict = {}\n",
    "    for key in predictions_residuals_dict:\n",
    "        residuals_std_dict[key] = np.std(predictions_residuals_dict[key])\n",
    "    # sort it in ascending order of keys\n",
    "    residuals_std_dict = dict(sorted(residuals_std_dict.items()))\n",
    "\n",
    "    \n",
    "    actuals_predictions_mean_dict = {}\n",
    "    for key in actual_predictions_dict:\n",
    "        actuals_predictions_mean_dict[key] = np.mean(actual_predictions_dict[key])\n",
    "    \n",
    "    # sort it in ascending order of keys\n",
    "    actuals_predictions_mean_dict = dict(sorted(actuals_predictions_mean_dict.items()))\n",
    "\n",
    "    predictions_std_dict = {}\n",
    "    for key in actual_predictions_dict:\n",
    "        predictions_std_dict[key] = np.std(actual_predictions_dict[key])\n",
    "    # sort it in ascending order of keys\n",
    "    predictions_std_dict = dict(sorted(predictions_std_dict.items()))\n",
    "\n",
    "    plt.plot(list(predictions_residuals_mean_dict.keys()), list(predictions_residuals_mean_dict.values()))\n",
    "    # create std shading\n",
    "    plt.fill_between(list(predictions_residuals_mean_dict.keys()),\n",
    "                        np.array(list(predictions_residuals_mean_dict.values())) - np.array(list(residuals_std_dict.values())),\n",
    "                        np.array(list(predictions_residuals_mean_dict.values())) + np.array(list(residuals_std_dict.values())),\n",
    "                        alpha=0.2)\n",
    "    # plot ideal line (x = 0)\n",
    "    plt.plot([y_test.min(), y_test.max()], [0, 0], 'k--', lw=1, color='red')\n",
    "    plt.legend(['Mean of residuals', 'STD of residuals', 'Ideal residuals line'])\n",
    "    plt.xlabel('Actual time to sleep onset')\n",
    "    plt.ylabel('Residuals for this actual')\n",
    "    plt.title('Mean residual of predictions for each actual time to sleep onset')\n",
    "    if ifsave:\n",
    "        if iftest:\n",
    "            plt.savefig(os.path.join(savepath, 'test_mean_residuals_across_time.png'))\n",
    "        else:\n",
    "            plt.savefig(os.path.join(savepath, 'crossval_mean_residuals_across_time.png'))\n",
    "    if ifplot:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(list(actuals_predictions_mean_dict.keys()), list(actuals_predictions_mean_dict.values()))\n",
    "    # create std shading\n",
    "    plt.fill_between(list(actuals_predictions_mean_dict.keys()),\n",
    "                        np.array(list(actuals_predictions_mean_dict.values())) - np.array(list(predictions_std_dict.values())),\n",
    "                        np.array(list(actuals_predictions_mean_dict.values())) + np.array(list(predictions_std_dict.values())),\n",
    "                        alpha=0.2)\n",
    "    # plot ideal line (x = y)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=1, color='red')\n",
    "    plt.legend(['Mean of predictions', 'STD of predictions', 'Ideal actual = prediction line (x = y)'])\n",
    "    plt.xlabel('Actual time to sleep onset')\n",
    "    plt.ylabel('Mean prediction for this actual')\n",
    "    plt.title('Mean prediction for each actual time to sleep onset')\n",
    "    if ifsave:\n",
    "        if iftest:\n",
    "            plt.savefig(os.path.join(savepath, 'test_mean_predictions_across_time.png'))\n",
    "        else:\n",
    "             plt.savefig(os.path.join(savepath, 'crossval_mean_predictions_across_time.png'))\n",
    "    if ifplot:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    test_metrics = {'MAE': round(np.mean(errors), 2), 'Accuracy': round(accuracy, 2), \n",
    "    'MSE': round(mse, 2), 'RMSE': round(rmse, 2), 'R2': round(r2, 2), 'custom_MSE': round(custom_mse, 2)}\n",
    "    return test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_participants(df, n_participants):\n",
    "    \"\"\"Selects a random sample of participants from the dataset.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The dataset.\n",
    "        n_participants (int): The number of participants to select.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A random sample of participants.\n",
    "    \"\"\"\n",
    "    # Select a random sample of participants\n",
    "    participants = df['Sbj_ID'].unique()\n",
    "    np.random.seed(42)\n",
    "    selected_participants = np.random.choice(participants, n_participants, replace=False)\n",
    "\n",
    "    for participant in selected_participants:\n",
    "        # check how many nan values there are in the dataset for this participant\n",
    "        n_nans = df[df['Sbj_ID'] == participant].isna().sum().sum()\n",
    "        # if there are more than 20 nan values, select another participant\n",
    "        while n_nans > 50:\n",
    "            selected_participants = np.delete(selected_participants, np.where(selected_participants == participant))\n",
    "            new_participant = np.random.choice(participants, 1, replace=False)\n",
    "            # check if the new participant is already in the list\n",
    "            if new_participant not in selected_participants:\n",
    "                selected_participants = np.append(selected_participants, new_participant)\n",
    "            else:\n",
    "                continue\n",
    "            participant = selected_participants[-1]\n",
    "            n_nans = df[df['Sbj_ID'] == participant].isna().sum().sum()\n",
    "\n",
    "    # Filter the dataset\n",
    "    df = df[df['Sbj_ID'].isin(selected_participants)].copy()\n",
    "\n",
    "    return df, selected_participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_random_participants(data, model, random_seed, ifplot, output_path_new, index, label, brfc_classes, awake_window, pre_sleep_window, ifsaveplots = True): \n",
    "    \n",
    "    \"\"\"\n",
    "    Test the model on a random subset of participants. \n",
    "    \"\"\"\n",
    "    # Set random seed\n",
    "\n",
    "    # create a new folder for the results\n",
    "    output_path_new = os.path.join(output_path_new, 'random_participants')\n",
    "    \n",
    "    if not os.path.exists(output_path_new):\n",
    "        os.makedirs(output_path_new)\n",
    "    \n",
    "    # intialise empty dataframe to store results\n",
    "    df_all = pd.DataFrame()\n",
    "\n",
    "    \n",
    "    participant_probas = {}\n",
    "    participant_timeline = {}\n",
    "\n",
    "    for sbj in data['Sbj_ID'].unique():\n",
    "        \n",
    "        # create a folder for each participant\n",
    "        output_path_participant = os.path.join(output_path_new, str(sbj))\n",
    "        if ifsaveplots:\n",
    "            if not os.path.exists(output_path_participant):\n",
    "                os.makedirs(output_path_participant)\n",
    "\n",
    "\n",
    "        participant_data = data[data['Sbj_ID'] == sbj].copy()\n",
    "        y = participant_data['Label']\n",
    "        timeline = participant_data['Old_label']\n",
    "        sleep_stages = participant_data['SleepStage']\n",
    "        X = participant_data.drop(['Label', 'SleepStage', 'Sbj_ID', 'ifCleanOnset', 'Time2Sleep', 'Old_label'], axis=1)\n",
    "\n",
    "        predictions = model.predict(X)\n",
    "        prediction_probabilities = model.predict_proba(X)\n",
    "        \n",
    "\n",
    "        class_idx = np.where(brfc_classes != 'Awake')[0][0]\n",
    "        prediction_probas_sleep =prediction_probabilities[:,class_idx]\n",
    "        participant_probas[sbj] = prediction_probas_sleep\n",
    "        participant_timeline[sbj] = timeline\n",
    "\n",
    "\n",
    "\n",
    "        # get the accuracy of each timepoint\n",
    "        if ifplot:\n",
    "            print('Predict for participant: ', sbj)\n",
    "\n",
    "            print('--------------------------------------Evaluate classificaiton ----------------------------------------')\n",
    "        \n",
    "        test_metrics = evaluate_classification(y, predictions, prediction_probabilities, index = index, label = label, iftest=True,\n",
    "                                ifplot=ifplot, output_path=output_path_participant, ifsaveplots= ifsaveplots, classes = brfc_classes, ifvocal = False)\n",
    "        test_metrics['Sbj_ID'] = sbj\n",
    "            \n",
    "        \n",
    "        \n",
    "        if ifplot:\n",
    "            print('------------------------------------------- No averaging ----------------------------------------')\n",
    "            print('Accuracy')\n",
    "            \n",
    "        _, _, _, _ = check_accuracy_timeline(y_pred = predictions, y_true = y,\n",
    "                                                                    timeline = timeline, sleep_stage = sleep_stages, \n",
    "                                                                    window_for_averaging=1, awake_window=awake_window, pre_sleep_window=pre_sleep_window,\n",
    "                                                                    if_proba=False, classes=None, rnn_window = 0, \n",
    "                                                                    ifsaveplots=ifsaveplots, output_path=output_path_participant, ifplot = ifplot)\n",
    "        if ifplot:\n",
    "            print('Probability of sleep as predicted by the model')\n",
    "        \n",
    "        \n",
    "        num_crossings, time_of_prediction = check_accuracy_timeline(y_pred = prediction_probabilities, y_true = y,\n",
    "                                                timeline = timeline, sleep_stage = sleep_stages, \n",
    "                                                window_for_averaging=1, awake_window=awake_window, pre_sleep_window=pre_sleep_window,\n",
    "                                                if_proba=True, classes=brfc_classes, rnn_window = 0, \n",
    "                                                ifsaveplots=ifsaveplots, output_path=output_path_participant, ifplot = ifplot, ifsubjecttest = True,\n",
    "                                                ifcrossings = True)\n",
    "        test_metrics['num_crossings'] = num_crossings\n",
    "        test_metrics['time_of_prediction'] = time_of_prediction\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        if ifplot:\n",
    "            print('-------------------------------Averaging of accuracy over 30 seconds---------------------------------')\n",
    "            print('Accuracy')\n",
    "        _, _, _, _ = check_accuracy_timeline(y_pred = predictions, y_true = y,\n",
    "                                                                    timeline = timeline, sleep_stage = sleep_stages, \n",
    "                                                                    window_for_averaging=5, awake_window=awake_window, pre_sleep_window=pre_sleep_window,\n",
    "                                                                    if_proba=False, classes=None, rnn_window = 0, \n",
    "                                                                    ifsaveplots=ifsaveplots, output_path=output_path_participant, ifplot = ifplot)\n",
    "        if ifplot:\n",
    "            print('Probability of sleep as predicted by the model')\n",
    "        \n",
    "        \n",
    "        num_crossings_smoothed, time_of_prediction_smoothed = check_accuracy_timeline(y_pred = prediction_probabilities, y_true = y,\n",
    "                                                                timeline = timeline, sleep_stage = sleep_stages, \n",
    "                                                                window_for_averaging=5, awake_window=awake_window, pre_sleep_window=pre_sleep_window,\n",
    "                                                                if_proba=True, classes=brfc_classes, rnn_window = 0, \n",
    "                                                                ifsaveplots=ifsaveplots, output_path=output_path_participant, ifplot = ifplot, ifsubjecttest = True, \n",
    "                                                                ifcrossings = True)\n",
    "        test_metrics['num_crossings_smoothed'] = num_crossings_smoothed\n",
    "        test_metrics['time_of_prediction_smoothed'] = time_of_prediction_smoothed \n",
    "        next_index = len(df_all)\n",
    "        df = pd.DataFrame(test_metrics, index=[next_index])\n",
    "        df_all = pd.concat([df_all, df])\n",
    "\n",
    "    df_all.to_csv(os.path.join(output_path_new, 'classification_results_random_participants.csv'))\n",
    "\n",
    "    # calculate how many participants have only one crossing in df_all['num_crossings'] out of all the participants\n",
    "    num_participants_one_crossing = len(df_all[df_all['num_crossings'] == 1])\n",
    "    num_participants = len(df_all)\n",
    "    one_crossing_percentage = num_participants_one_crossing/num_participants * 100\n",
    "    mean_time_of_prediction = np.mean(df_all['time_of_prediction'][df_all['num_crossings'] == 1])\n",
    "    std_time_of_prediction = np.std(df_all['time_of_prediction'][df_all['num_crossings'] == 1])\n",
    "\n",
    "\n",
    "    num_participants_one_crossing_smoothed = len(df_all[df_all['num_crossings_smoothed'] == 1])\n",
    "    one_crossing_percentage_smoothed = num_participants_one_crossing_smoothed/num_participants * 100\n",
    "    mean_time_of_prediction_smoothed = np.mean(df_all['time_of_prediction_smoothed'][df_all['num_crossings_smoothed'] == 1])\n",
    "    std_time_of_prediction_smoothed = np.std(df_all['time_of_prediction_smoothed'][df_all['num_crossings_smoothed'] == 1])\n",
    "\n",
    "    print(f'Percentage of participants with only one crossing: {one_crossing_percentage}%')\n",
    "    print(f'Mean time of prediction for participants with only one crossing: {mean_time_of_prediction} minutes')\n",
    "    print(f'STD of time of prediction for participants with only one crossing: {std_time_of_prediction} minutes')\n",
    "\n",
    "\n",
    "    print(f'Percentage of participants with only one crossing (smoothed): {one_crossing_percentage_smoothed}%')\n",
    "    print(f'Mean time of prediction for participants with only one crossing (smoothed): {mean_time_of_prediction_smoothed} minutes')\n",
    "    print(f'STD of time of prediction for participants with only one crossing (smoothed): {std_time_of_prediction_smoothed} minutes')\n",
    "    \n",
    "    output_dict = {'one_crossing_percentage': one_crossing_percentage, 'mean_time_of_prediction': mean_time_of_prediction,\n",
    "                    'std_time_of_prediction': std_time_of_prediction, 'one_crossing_percentage_smoothed': one_crossing_percentage_smoothed,\n",
    "                    'mean_time_of_prediction_smoothed': mean_time_of_prediction_smoothed, 'std_time_of_prediction_smoothed': std_time_of_prediction_smoothed}\n",
    "    \n",
    "    \n",
    "    return participant_probas, participant_timeline, output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reg_on_random_participants(participant_probas, participant_timeline, model, random_seed, ifplot, output_path_new, index, brfc_classes, awake_window, pre_sleep_window): \n",
    "    \n",
    "    \"\"\"\n",
    "    Test the model on a random subset of participants. \n",
    "    \"\"\"\n",
    "    # Set random seed\n",
    "\n",
    "    # create a new folder for the results\n",
    "    output_path_new = os.path.join(output_path_new, 'random_participants')\n",
    "    if not os.path.exists(output_path_new):\n",
    "        os.makedirs(output_path_new)\n",
    "    \n",
    "    # intialise empty dataframe to store results\n",
    "    df_all = pd.DataFrame()\n",
    "\n",
    "    \n",
    "    for sbj in list(participant_probas.keys()):\n",
    "        # create a folder for each participant\n",
    "        \n",
    "        output_path_participant = os.path.join(output_path_new, str(sbj))\n",
    "        if not os.path.exists(output_path_participant):\n",
    "            os.makedirs(output_path_participant)\n",
    "\n",
    "        y_test_timeline = participant_timeline[sbj]\n",
    "        probas = participant_probas[sbj]\n",
    "\n",
    "        # check if y_test_timeline and probas are the same length\n",
    "        if len(y_test_timeline) != len(probas):\n",
    "            error = 'y_test_timeline and probas are not the same length'\n",
    "\n",
    "        if np.isnan(probas).any():\n",
    "                    for i in range(len(probas)):\n",
    "                        if np.isnan(probas[i]):\n",
    "                            # omit the corresponding y_times2sleep\n",
    "                            y_test_timeline.pop(i)\n",
    "                            probas.pop(i)\n",
    "\n",
    "\n",
    "        probas = np.array(probas).reshape(-1, 1)\n",
    "        \n",
    "\n",
    "        timeline_predicted = model.predict(probas)\n",
    "       \n",
    "\n",
    "        # get the accuracy of each timepoint\n",
    "        if ifplot:\n",
    "            print('Predict for participant: ', sbj)\n",
    "\n",
    "            print('--------------------------------------Evaluate regression ----------------------------------------')\n",
    "        \n",
    "        test_metrics = evaluate_regression(timeline_predicted, y_test_timeline, iftest=True,\n",
    "                                ifplot=ifplot, savepath =output_path_participant, ifsave= True)\n",
    "        \n",
    "        df = pd.DataFrame(test_metrics, index = [sbj])\n",
    "        df_all = pd.concat([df_all, df])\n",
    "\n",
    "    df_all.to_csv(os.path.join(output_path_new, 'regression_results_random_participants.csv'))\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Create a function for plotting the sorted feature importances in a long horizontal bar chart and increased font size\n",
    "\n",
    "def plot_feature_importances(model, X, output_path, ifsaveplots = False, save_name = 'feature_importances.png', ifplot = True):\n",
    "    save_path = os.path.join(output_path, save_name)\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    names = [X.columns[i] for i in indices]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, 24), dpi=80)\n",
    "    ax.barh(range(X.shape[1]), importances[indices], align='center')\n",
    "    ax.set_yticks(range(X.shape[1]))\n",
    "    ax.set_yticklabels(names, fontsize=16)\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_ylabel('Feature')\n",
    "    ax.set_title('Feature importances')\n",
    "    if ifplot:\n",
    "        plt.show()\n",
    "    if ifsaveplots:\n",
    "        fig.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "\n",
    "\n",
    "    return importances, indices, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_test_split_sbjs(mydata_no_nan, test_size = 0.2, random_seed = 42):\n",
    " \n",
    "    # assert that the data has no NaNs\n",
    "    assert mydata_no_nan.isnull().sum().sum() == 0, 'There are NaNs in the data'\n",
    "    \n",
    "    # get the unique subjects\n",
    "    unique_subjects = mydata_no_nan['Sbj_ID'].unique()\n",
    "\n",
    "    # define the train size\n",
    "    train_size = 1 - test_size\n",
    "\n",
    "    # define the number of subjects in the test set\n",
    "\n",
    "    test_size_subjects = int(test_size * len(unique_subjects))\n",
    "\n",
    "    # Select the unique subjects for the test set\n",
    "\n",
    "    test_subjects = random.sample(list(unique_subjects), test_size_subjects)\n",
    "\n",
    "    # Select the unique subjects for the training set\n",
    "    train_subjects = [x for x in unique_subjects if x not in test_subjects]\n",
    "\n",
    "    # Create the test and training sets\n",
    "    test_set = mydata_no_nan[mydata_no_nan['Sbj_ID'].isin(test_subjects)]\n",
    "    train_set = mydata_no_nan[mydata_no_nan['Sbj_ID'].isin(train_subjects)]\n",
    "\n",
    "    # Create X and y from dataframes to use in scikit-learn (drop Label, SleepStage, Sbj_ID and ifCleanOnset columns)\n",
    "\n",
    "    X_train = train_set.drop(['Label', 'SleepStage', 'Sbj_ID', 'ifCleanOnset', 'Time2Sleep'], axis=1)\n",
    "    y_train = train_set['Label']\n",
    "    X_test = test_set.drop(['Label', 'SleepStage', 'Sbj_ID', 'ifCleanOnset', 'Time2Sleep'], axis=1)\n",
    "    y_test = test_set['Label']\n",
    "\n",
    "\n",
    "    # Create list of sbj_IDs for each sample in the training and test sets\n",
    "    sbj_train_set = train_set['Sbj_ID'].copy()\n",
    "    sbj_test_set = test_set['Sbj_ID'].copy()\n",
    "\n",
    "    # Turn the sbj_IDs into a list\n",
    "    sbj_train_set = sbj_train_set.tolist()\n",
    "    sbj_test_set = sbj_test_set.tolist()\n",
    "    \n",
    "\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, sbj_train_set, sbj_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming feature_importances is a 2D array of shape (len(pre_sleep_times), len(awake_times))\n",
    "# And pre_sleep_times and awake_times are lists of the unique values you've trained with\n",
    "\n",
    "def plot_feature_heatmap(feature_importances, pre_sleep_times, awake_times, feature_name):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(feature_importances, annot=True, cmap=\"YlGnBu\", \n",
    "                xticklabels=awake_times, yticklabels=pre_sleep_times)\n",
    "    plt.title(f'Feature Importance for {feature_name}')\n",
    "    plt.xlabel('Awake Time')\n",
    "    plt.ylabel('Pre Sleep Time')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rid_of_nans(probabilities, y_times2sleep = None):\n",
    "    if np.isnan(probabilities).any():\n",
    "        for i in range(len(probabilities)):\n",
    "            if np.isnan(probabilities[i]):\n",
    "                # omit the corresponding y_times2sleep\n",
    "                if y_times2sleep is not None:\n",
    "                    y_times2sleep.pop(i)\n",
    "                probabilities.pop(i)\n",
    "    return probabilities, y_times2sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_thresholds(epoch, initial_sleep_onset_threshold, initial_awake_threshold, awake_delta, sleep_delta, schedule_period=1):\n",
    "    \"\"\"\n",
    "    Updates the thresholds based on a target convergence point.\n",
    "\n",
    "    Parameters:\n",
    "    - epoch: current epoch number\n",
    "    - initial_awake_threshold: initial awake threshold \n",
    "    - initial_sleep_onset_threshold: initial sleep onset threshold\n",
    "    - convergence_point: point at which the thresholds should converge\n",
    "    - total_epochs: total epochs for which the training is scheduled\n",
    "    - schedule_period: epochs after which thresholds are adjusted. Default is 10 epochs.\n",
    "\n",
    "    Returns:\n",
    "    - New awake and sleep_onset thresholds\n",
    "    \"\"\"\n",
    "    \n",
    "    # Adjusting both thresholds based on the epoch \n",
    "    if epoch % schedule_period == 0 :\n",
    "        initial_awake_threshold -= awake_delta\n",
    "        initial_sleep_onset_threshold += sleep_delta\n",
    "\n",
    "    return round(initial_sleep_onset_threshold, 1), round(initial_awake_threshold,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import balanced random forest classifier\n",
    "import warnings\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVR\n",
    "# import random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "def explore_classifier_sleep_probabilities_schedule(classifier_type, initial_pre_sleep, initial_awake, convergence_points, \n",
    "                                                    total_epochs, mydata_normalised_preprocessed_one_hot, \n",
    "                                                    selected_participant_data = None, selected_participants = None,\n",
    "                                                    output_path = 'Balanced_Random_Forest_Schedule', ifsaveplots = True, \n",
    "                                                    ifplot = True, random_seed = 42, convergence_sensitivity = 0.1, ifresetresults = False):\n",
    "\n",
    "    \"\"\"\n",
    "    This function is used to explore the probabilities and accuracies outputted by classifier in predicting sleep onset.\n",
    "    \"\"\"\n",
    "    \n",
    "    warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "    warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "    #mydata_normalised_preprocessed_one_hot.dropna(inplace=True)\n",
    "\n",
    "\n",
    "    # Initialize a DataFrame to store results\n",
    "    columns = ['convergence_point', 'ifconverged', 'pre_sleep_window', 'awake_window', 'pos_class_weight',\n",
    "           'crossval_accuracy', 'crossval_precision', \n",
    "           'crossval_recall', 'crossval_f1_weighted',\n",
    "           'crossval_f1_macro', 'crossval_f1_pre_sleep', 'crossval_f1_awake'\n",
    "           'crossval_auc', 'crossval_mae', 'crossval_mse', 'crossval_rmse', 'crossval_r2',\n",
    "            'crossval_custom_mse', 'test_accuracy', 'test_precision',\n",
    "           'test_recall', 'test_f1_weighted', 'test_f1_macro', \n",
    "           'test_f1_pre_sleep', 'test_f1_awake',  'test_auc', 'test_one_crossing_percentage', \n",
    "           'test_mean_predicition_time', 'test_std_predicition_time', \n",
    "           'test_one_crossing_percentage_smoothed', 'test_mean_predicition_time_smoothed',\n",
    "            'test_std_predicition_time_smoothed',\n",
    "           'test_mae', 'test_mse', 'test_rmse', 'test_r2', 'test_custom_mse']\n",
    "            \n",
    "    results_df = pd.DataFrame(columns=columns)\n",
    "    results_dir = output_path\n",
    "\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "\n",
    "    if ifresetresults:\n",
    "        results_df.to_csv(os.path.join(results_dir, 'results.csv'))\n",
    "    \n",
    "    # check if results.csv already exists\n",
    "    if os.path.exists(os.path.join(results_dir, 'results.csv')):\n",
    "        results_df = pd.read_csv(os.path.join(results_dir, 'results.csv'), index_col=0)\n",
    "    else:\n",
    "        # save the empty dataframe\n",
    "        results_df.to_csv(os.path.join(results_dir, 'results.csv'))\n",
    "\n",
    "\n",
    "    for convergence_point in convergence_points:\n",
    "        print('Convergence point: ', convergence_point)\n",
    "        folder_name = f'{convergence_point}_convergence_point'\n",
    "        output_path_conv = os.path.join(results_dir, folder_name)\n",
    "        if not os.path.exists(output_path_conv):\n",
    "            os.makedirs(output_path_conv)\n",
    "\n",
    "        distance_to_convergence_sleep_onset = convergence_point - initial_pre_sleep\n",
    "        # round to 1 decimal place\n",
    "        sleep_onset_delta = round(distance_to_convergence_sleep_onset / total_epochs, 1)\n",
    "\n",
    "        distance_to_convergence_awake = initial_awake - convergence_point\n",
    "        # round to 1 decimal place\n",
    "        awake_delta = round(distance_to_convergence_awake / total_epochs, 1)\n",
    "        \n",
    "        brfc = BalancedRandomForestClassifier(n_estimators=50, random_state=42, sampling_strategy = 'all', replacement = True, warm_start = True)\n",
    "        \n",
    "        pre_sleep_window = initial_pre_sleep\n",
    "        awake_window = initial_awake\n",
    "        for epoch in range(total_epochs):\n",
    "            if epoch != 0:\n",
    "                n_estimators = brfc.n_estimators + 10\n",
    "                brfc.set_params(n_estimators=n_estimators)\n",
    "\n",
    "            if abs(pre_sleep_window - convergence_point) < convergence_sensitivity:\n",
    "                print('Sleep onset threshold has converged')\n",
    "                break\n",
    "\n",
    "            if abs(awake_window - convergence_point) < convergence_sensitivity:\n",
    "                print('Awake threshold has converged')\n",
    "                break\n",
    "\n",
    "            pre_sleep_window, awake_window = update_thresholds(epoch,  pre_sleep_window, awake_window, awake_delta, sleep_onset_delta)\n",
    "            \n",
    "            folder_name = f'{epoch}_epoch_{pre_sleep_window}_presleep_{awake_window}_awake'\n",
    "            if not os.path.exists(os.path.join(output_path_conv, folder_name)):\n",
    "                os.makedirs(os.path.join(output_path_conv, folder_name))\n",
    "\n",
    "            output_path_new = os.path.join(output_path_conv, folder_name)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #print('New sleep onset threshold: ', sleep_onset_threshold) \n",
    "            #print('New awake threshold: ', awake_threshold)\n",
    "\n",
    "            # Create a new dataframe to store the probabilities and accuracies\n",
    "            mydata_all_labels = mydata_normalised_preprocessed_one_hot.copy()\n",
    "            mydata_all_labels['Old_label'] = mydata_all_labels['Label'].copy()\n",
    "\n",
    "            if selected_participants is not None:\n",
    "                selected_participant_data2process = selected_participant_data.copy()\n",
    "                selected_participant_data2process['Old_label'] = selected_participant_data2process['Label'].copy()\n",
    "                selected_participant_data2process['Label'] = np.where(selected_participant_data2process['Label'] <= pre_sleep_window, f'{pre_sleep_window} minutes before sleep', 'Awake')\n",
    "\n",
    "            \n",
    "            # Swap Label columns into categorical '5 minutes before sleep'  (if Label <= 5) and '15+ minutes before sleep' in the other case\n",
    "            index = [f'{pre_sleep_window} minutes before sleep', 'Awake']\n",
    "            label = [f'{pre_sleep_window} minutes before sleep', 'Awake']\n",
    "            mydata_all_labels['Label'] = np.where(mydata_all_labels['Label'] <= pre_sleep_window, f'{pre_sleep_window} minutes before sleep', 'Awake')\n",
    "            \n",
    "\n",
    "            print('\\n')\n",
    "            print('_'*110)\n",
    "            print(f'                                    PRE-SLEEP WINDOW: {pre_sleep_window} minutes                                   ')\n",
    "            print('_'*110)\n",
    "            print('\\n')\n",
    "\n",
    "            print(f'--------------------------------------- AWAKE-WINDOW: {awake_window}+ minutes ------------------------------------------------')\n",
    "            print('\\n')\n",
    "\n",
    "\n",
    "            # Try classification on 5 minutes before sleep and 15+ minutes before sleep\n",
    "            mydata_for_training = mydata_normalised_preprocessed_one_hot[(mydata_normalised_preprocessed_one_hot['Label'] <= pre_sleep_window) | (mydata_normalised_preprocessed_one_hot['Label'] > awake_window)].copy()\n",
    "\n",
    "            # Swap Label columns into categorical '5 minutes before sleep'  (if Label <= 5) and '15+ minutes before sleep' in the other case\n",
    "            mydata_for_training['Label'] = np.where(mydata_for_training['Label'] <= pre_sleep_window, f'{pre_sleep_window} minutes before sleep', 'Awake')\n",
    "\n",
    "            # Split test, and training sets for training the model\n",
    "            X_train, y_train, X_test, y_test, sbj_train_set, sbj_test_set = train_test_split_sbjs(mydata_for_training, test_size = 0.2, random_seed = 42)\n",
    "\n",
    "            # Get the timeline for checking the model performance\n",
    "            X_all, y_all, y_old_all, y_sleepstage_all = get_timeline(mydata_all_labels)\n",
    "\n",
    "            # get only the subjects from sbj_test_set_5_5 in mydata_all_labels_5\n",
    "            mydata_all_labels_test = mydata_all_labels[mydata_all_labels['Sbj_ID'].isin(sbj_test_set)]\n",
    "\n",
    "            # Get the timeline for checking the model performance\n",
    "            X_all_test, y_all_test, y_old_all_test, y_sleepstage_all_test = get_timeline(mydata_all_labels_test)\n",
    "            \n",
    "            # get the positive class weight for y_all\n",
    "            pos_class_weight = len(y_all[y_all == 'Awake'])/len(y_all[y_all == f'{pre_sleep_window} minutes before sleep'])\n",
    "\n",
    "            if classifier_type == 'brfc':\n",
    "                # Create a random forest classifier\n",
    "            \n",
    "                brfc.fit(X_train, y_train)\n",
    "                    \n",
    "                # get the sorted feature importances and their names\n",
    "                _, _, _ = plot_feature_importances(brfc, X_train, ifsaveplots = ifsaveplots, output_path=output_path_new, ifplot = ifplot)\n",
    "             \n",
    "\n",
    "                brfc_classes = brfc.classes_\n",
    "\n",
    "                brfc_predictions = brfc.predict(X_all) \n",
    "                print('Predictions:', brfc_predictions)\n",
    "                brfc_predictions_proba = brfc.predict_proba(X_all)\n",
    "                brfc_predictions_test_proba = brfc.predict_proba(X_all_test)\n",
    "                brfc_predictions_test = brfc.predict(X_all_test)\n",
    "\n",
    "                # transform predicitons and labels for evalution \n",
    "                \n",
    "\n",
    "                crossval_metrics = evaluate_classification(y_all, brfc_predictions, brfc_predictions_proba, index= index, label = label,\n",
    "                                                        ifplot=ifplot, output_path=output_path_new, ifsaveplots= True, classes = brfc_classes)\n",
    "                crossval_metrics['pos_class_weight'] = pos_class_weight\n",
    "                \n",
    "                test_metrics = evaluate_classification(y_all_test, brfc_predictions_test, brfc_predictions_test_proba, index= index, label = label, iftest=True,\n",
    "                                                        ifplot=ifplot, output_path=output_path_new, ifsaveplots= True, classes = brfc_classes)\n",
    "                \n",
    "                _, _, participant_dict = test_on_random_participants(mydata_all_labels_test, brfc, random_seed = 42, \n",
    "                                                                                            ifplot = False, output_path_new = output_path_new, index = index, label = label,\n",
    "                                                                                            ifsaveplots = False, brfc_classes = brfc_classes, awake_window = awake_window, \n",
    "                                                                                         pre_sleep_window = pre_sleep_window)\n",
    "                \n",
    "                if selected_participants is not None:\n",
    "                    participant_probas, participant_timeline, _= test_on_random_participants(selected_participant_data2process, brfc, random_seed = 42, \n",
    "                                                                                            ifplot = False, output_path_new = output_path_new, index = index, label = label, \n",
    "                                                                                            brfc_classes = brfc_classes, awake_window = awake_window, \n",
    "                                                                                            pre_sleep_window = pre_sleep_window)\n",
    "\n",
    "                # get the accuracy of each timepoint\n",
    "                print('Now trying only on the unseen test data')\n",
    "                print('------------------------------------------- No averaging ----------------------------------------')\n",
    "                print('Accuracy')\n",
    "                \n",
    "                _, _, _, _ = check_accuracy_timeline(y_pred = brfc_predictions_test, y_true = y_all_test,\n",
    "                                                                            timeline = y_old_all_test, sleep_stage = y_sleepstage_all_test, \n",
    "                                                                            window_for_averaging=1, awake_window=awake_window, pre_sleep_window=pre_sleep_window,\n",
    "                                                                            if_proba=False, classes=None, rnn_window = 0, \n",
    "                                                                            ifsaveplots=True, output_path=output_path_new, ifplot = ifplot)\n",
    "                print('Probability of sleep as predicted by the model')\n",
    "            \n",
    "                \n",
    "                _ = check_accuracy_timeline(y_pred = brfc_predictions_test_proba, y_true = y_all_test,\n",
    "                                                        timeline = y_old_all_test, sleep_stage = y_sleepstage_all_test, \n",
    "                                                        window_for_averaging=1, awake_window=awake_window, pre_sleep_window=pre_sleep_window,\n",
    "                                                        if_proba=True, classes=brfc_classes, rnn_window = 0, \n",
    "                                                        ifsaveplots=True, output_path=output_path_new, ifplot = ifplot)\n",
    "\n",
    "\n",
    "\n",
    "                print('-------------------------------Averaging of accuracy over 30 seconds---------------------------------')\n",
    "                print('Accuracy')\n",
    "                _, _, _, _ = check_accuracy_timeline(y_pred = brfc_predictions_test, y_true = y_all_test,\n",
    "                                                                            timeline = y_old_all_test, sleep_stage = y_sleepstage_all_test, \n",
    "                                                                            window_for_averaging=5, awake_window=awake_window, pre_sleep_window=pre_sleep_window,\n",
    "                                                                            if_proba=False, classes=None, rnn_window = 0, \n",
    "                                                                            ifsaveplots=True, output_path=output_path_new, ifplot = ifplot)\n",
    "                print('Probability of sleep as predicted by the model')\n",
    "                \n",
    "                y_pred_sorted = check_accuracy_timeline(y_pred = brfc_predictions_test_proba, y_true = y_all_test,\n",
    "                                                        timeline = y_old_all_test, sleep_stage = y_sleepstage_all_test, \n",
    "                                                        window_for_averaging=5, awake_window=awake_window, pre_sleep_window=pre_sleep_window,\n",
    "                                                        if_proba=True, classes=brfc_classes, rnn_window = 0, \n",
    "                                                        ifsaveplots=True, output_path=output_path_new, ifplot = ifplot)\n",
    "\n",
    "                \n",
    "                # Perform regression to predict the time to sleep onset\n",
    "\n",
    "                y_pred_sorted = check_accuracy_timeline(y_pred = brfc_predictions_proba, y_true = y_all,\n",
    "                                                        timeline = y_old_all, sleep_stage = y_sleepstage_all, \n",
    "                                                        window_for_averaging=5, awake_window=awake_window, pre_sleep_window=pre_sleep_window,\n",
    "                                                        if_proba=True, classes=brfc_classes, rnn_window = 0, \n",
    "                                                        ifsaveplots=False, output_path=output_path_new, ifplot = ifplot)\n",
    "\n",
    "                \n",
    "                # From averqaged probabilities, try to do regression to predict the time to sleep onset\n",
    "                y_times2sleep = list(y_pred_sorted.keys())\n",
    "                probabilities = list(y_pred_sorted.values())\n",
    "                probabilities, y_times2sleep = get_rid_of_nans(probabilities, y_times2sleep)\n",
    "\n",
    "                # build a regressor to predict the time to sleep onset using svm\n",
    "                #svr = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\n",
    "                rf = RandomForestRegressor(n_estimators=150, random_state=42)\n",
    "                    \n",
    "                probabilities = np.array(probabilities).reshape(-1, 1)\n",
    "\n",
    "                rf.fit(probabilities, y_times2sleep)\n",
    "                rf_predictions = rf.predict(probabilities)\n",
    "                crossval_regression_metrics = evaluate_regression(rf_predictions, y_times2sleep, ifsave = ifsaveplots, \n",
    "                                                                ifplot=ifplot, savepath=output_path_new, iftest = False)\n",
    "\n",
    "                # get the predictions for the test data \n",
    "                class_idx = np.where(brfc_classes != 'Awake')[0][0]\n",
    "                probs_test = brfc_predictions_test_proba[:,class_idx]\n",
    "                probs_test, y_old_all_test = get_rid_of_nans(probs_test, y_old_all_test)\n",
    "                y_pred_times2sleep = rf.predict(probs_test.reshape(-1, 1))\n",
    "                test_regression_metrics = evaluate_regression(y_pred_times2sleep, y_old_all_test, ifsave = ifsaveplots, \n",
    "                                                                ifplot=ifplot, savepath=output_path_new, iftest = True)\n",
    "                if selected_participants is not None:\n",
    "                    test_reg_on_random_participants(participant_probas, participant_timeline, rf, random_seed, ifplot, \n",
    "                                                    output_path_new, index, brfc_classes, awake_window, pre_sleep_window)\n",
    "\n",
    "                # Add results to dataframe\n",
    "                ifconverged = 0\n",
    "                if epoch + 1 == total_epochs:\n",
    "                    ifconverged = 1\n",
    "\n",
    "                new_row = pd.DataFrame({\n",
    "                        'convergence_point': [convergence_point],\n",
    "                        'ifconverged': [ifconverged],\n",
    "                        'pre_sleep_window': [pre_sleep_window],\n",
    "                        'awake_window': [awake_window],\n",
    "                        'pos_class_weight': [crossval_metrics['pos_class_weight']],\n",
    "                        'crossval_accuracy': [crossval_metrics['accuracy']],\n",
    "                        'crossval_precision': [crossval_metrics['precision']],\n",
    "                        'crossval_recall': [crossval_metrics['recall']],\n",
    "                        'crossval_f1_weighted': [crossval_metrics['f1_weighted']],\n",
    "                        'crossval_f1_macro': [crossval_metrics['f1_macro']],\n",
    "                        'crossval_f1_pre_sleep': [crossval_metrics['f1_pre_sleep']],\n",
    "                        'crossval_f1_awake': [crossval_metrics['f1_awake']],\n",
    "                        'crossval_auc': [crossval_metrics['auc']],\n",
    "                        'cross_val_mae': [crossval_regression_metrics['MAE']],\n",
    "                        'cross_val_mse': [crossval_regression_metrics['MSE']],\n",
    "                        'cross_val_rmse': [crossval_regression_metrics['RMSE']],\n",
    "                        'cross_val_r2': [crossval_regression_metrics['R2']],\n",
    "                        'cross_val_custom_mse': [crossval_regression_metrics['custom_MSE']],\n",
    "                        'test_accuracy': [test_metrics['accuracy']],\n",
    "                        'test_precision': [test_metrics['precision']],\n",
    "                        'test_recall': [test_metrics['recall']],\n",
    "                        'test_f1_weighted': [test_metrics['f1_weighted']],\n",
    "                        'test_f1_macro': [test_metrics['f1_macro']],\n",
    "                        'test_f1_pre_sleep': [test_metrics['f1_pre_sleep']],\n",
    "                        'test_f1_awake': [test_metrics['f1_awake']],\n",
    "                        'test_auc': [test_metrics['auc']],\n",
    "                        'test_one_crossing_percentage': [participant_dict['one_crossing_percentage']],\n",
    "                        'test_mean_predicition_time': [participant_dict['mean_time_of_prediction']],\n",
    "                        'test_std_predicition_time': [participant_dict['std_time_of_prediction']],\n",
    "                        'test_one_crossing_percentage_smoothed': [participant_dict['one_crossing_percentage_smoothed']],\n",
    "                        'test_mean_predicition_time_smoothed': [participant_dict['mean_time_of_prediction_smoothed']],\n",
    "                        'test_std_predicition_time_smoothed': [participant_dict['std_time_of_prediction_smoothed']],\n",
    "                        'test_mae': [test_regression_metrics['MAE']],\n",
    "                        'test_mse': [test_regression_metrics['MSE']],\n",
    "                        'test_rmse': [test_regression_metrics['RMSE']],\n",
    "                        'test_r2': [test_regression_metrics['R2']],\n",
    "                        'test_custom_mse':[test_regression_metrics['custom_MSE']]\n",
    "                    })\n",
    "                \n",
    "                # Open the results dataframe and add the new results\n",
    "                results_df_all = pd.read_csv(os.path.join(results_dir, 'results.csv'))\n",
    "                results_df_all = pd.concat([results_df_all, new_row], ignore_index=True)\n",
    "                results_df_all.to_csv(os.path.join(results_dir, 'results.csv'), index=False) \n",
    "                del results_df_all\n",
    "                del new_row\n",
    "                \n",
    "                \n",
    "\n",
    "            del y_old_all\n",
    "            del y_sleepstage_all\n",
    "            del y_old_all_test\n",
    "            del y_sleepstage_all_test\n",
    "            del mydata_all_labels\n",
    "            del mydata_all_labels_test\n",
    "            del X_train\n",
    "            del y_train\n",
    "            del X_test\n",
    "            del y_test\n",
    "            del sbj_train_set\n",
    "            del sbj_test_set\n",
    "            del X_all\n",
    "            del y_all\n",
    "            del X_all_test\n",
    "            del y_all_test\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Create a function for plotting the sorted feature importances in a long horizontal bar chart and increased font size\n",
    "\n",
    "def plot_feature_importances(model, X, output_path, ifsaveplots = False, save_name = 'feature_importances.png', ifplot = True):\n",
    "    save_path = os.path.join(output_path, save_name)\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    names = [X.columns[i] for i in indices]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, 24), dpi=80)\n",
    "    ax.barh(range(X.shape[1]), importances[indices], align='center')\n",
    "    ax.set_yticks(range(X.shape[1]))\n",
    "    ax.set_yticklabels(names, fontsize=16)\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_ylabel('Feature')\n",
    "    ax.set_title('Feature importances')\n",
    "    if ifplot:\n",
    "        plt.show()\n",
    "    if ifsaveplots:\n",
    "        fig.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "\n",
    "\n",
    "    return importances, indices, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_test_split_sbjs(mydata_no_nan, test_size = 0.2, random_seed = 42):\n",
    " \n",
    "    # assert that the data has no NaNs\n",
    "    assert mydata_no_nan.isnull().sum().sum() == 0, 'There are NaNs in the data'\n",
    "    \n",
    "    # get the unique subjects\n",
    "    unique_subjects = mydata_no_nan['Sbj_ID'].unique()\n",
    "\n",
    "    # define the train size\n",
    "    train_size = 1 - test_size\n",
    "\n",
    "    # define the number of subjects in the test set\n",
    "\n",
    "    test_size_subjects = int(test_size * len(unique_subjects))\n",
    "\n",
    "    # Select the unique subjects for the test set\n",
    "\n",
    "    test_subjects = random.sample(list(unique_subjects), test_size_subjects)\n",
    "\n",
    "    # Select the unique subjects for the training set\n",
    "    train_subjects = [x for x in unique_subjects if x not in test_subjects]\n",
    "\n",
    "    # Create the test and training sets\n",
    "    test_set = mydata_no_nan[mydata_no_nan['Sbj_ID'].isin(test_subjects)]\n",
    "    train_set = mydata_no_nan[mydata_no_nan['Sbj_ID'].isin(train_subjects)]\n",
    "\n",
    "    # Create X and y from dataframes to use in scikit-learn (drop Label, SleepStage, Sbj_ID and ifCleanOnset columns)\n",
    "\n",
    "    X_train = train_set.drop(['Label', 'SleepStage', 'Sbj_ID', 'ifCleanOnset', 'Time2Sleep'], axis=1)\n",
    "    y_train = train_set['Label']\n",
    "    X_test = test_set.drop(['Label', 'SleepStage', 'Sbj_ID', 'ifCleanOnset', 'Time2Sleep'], axis=1)\n",
    "    y_test = test_set['Label']\n",
    "\n",
    "\n",
    "    # Create list of sbj_IDs for each sample in the training and test sets\n",
    "    sbj_train_set = train_set['Sbj_ID'].copy()\n",
    "    sbj_test_set = test_set['Sbj_ID'].copy()\n",
    "\n",
    "    # Turn the sbj_IDs into a list\n",
    "    sbj_train_set = sbj_train_set.tolist()\n",
    "    sbj_test_set = sbj_test_set.tolist()\n",
    "    \n",
    "\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, sbj_train_set, sbj_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming feature_importances is a 2D array of shape (len(pre_sleep_times), len(awake_times))\n",
    "# And pre_sleep_times and awake_times are lists of the unique values you've trained with\n",
    "\n",
    "def plot_feature_heatmap(feature_importances, pre_sleep_times, awake_times, feature_name):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(feature_importances, annot=True, cmap=\"YlGnBu\", \n",
    "                xticklabels=awake_times, yticklabels=pre_sleep_times)\n",
    "    plt.title(f'Feature Importance for {feature_name}')\n",
    "    plt.xlabel('Awake Time')\n",
    "    plt.ylabel('Pre Sleep Time')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_participants = np.array(selected_participants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_participants_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try and see if the loop works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "classifier_type = 'brfc'\n",
    "\n",
    "explore_classifier_sleep_probabilities_schedule(classifier_type, initial_pre_sleep = 2, initial_awake = 30, convergence_points = [10], \n",
    "                                                    total_epochs = 5, mydata_normalised_preprocessed_one_hot = data_train,\n",
    "                                                    selected_participant_data = selected_participants_data, selected_participants = selected_participants,\n",
    "                                                    output_path = 'Balanced_Random_Forest_Schedule_clean_onset', ifsaveplots = True, \n",
    "                                                    ifplot = False, random_seed = 42, ifresetresults=True, convergence_sensitivity=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypereparameter tune for window selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence_points = [3, 4, 5]\n",
    "classifier_type = 'brfc'\n",
    "\n",
    "explore_classifier_sleep_probabilities_schedule(classifier_type, initial_pre_sleep = 2, initial_awake = 30, convergence_points = convergence_points, \n",
    "                                                    total_epochs = 5, mydata_normalised_preprocessed_one_hot = data_train,\n",
    "                                                    selected_participant_data = selected_participants_data, \n",
    "                                                    selected_participants = selected_participants,\n",
    "                                                    output_path = 'Balanced_Random_Forest_Schedule_clean_onset', ifsaveplots = True, \n",
    "                                                    ifplot = False, random_seed = 42, convergence_sensitivity = 0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence_points = [6, 7, 8] \n",
    "classifier_type = 'brfc'\n",
    "\n",
    "explore_classifier_sleep_probabilities_schedule(classifier_type, initial_pre_sleep = 2, initial_awake = 30, convergence_points = convergence_points, \n",
    "                                                    total_epochs = 5, mydata_normalised_preprocessed_one_hot = data_train,\n",
    "                                                    selected_participant_data = selected_participants_data, selected_participants = selected_participants,\n",
    "                                                    output_path = 'Balanced_Random_Forest_Schedule_clean_onset', ifsaveplots = True, \n",
    "                                                    ifplot = False, random_seed = 42, convergence_sensitivity = 0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence_points = [9, 11, 12]\n",
    "classifier_type = 'brfc'\n",
    "\n",
    "explore_classifier_sleep_probabilities_schedule(classifier_type, initial_pre_sleep = 2, initial_awake = 30, convergence_points = convergence_points, \n",
    "                                                    total_epochs = 5, mydata_normalised_preprocessed_one_hot = data_train,\n",
    "                                                    selected_participant_data = selected_participants_data, selected_participants = selected_participants,\n",
    "                                                    output_path = 'Balanced_Random_Forest_Schedule_clean_onset', ifsaveplots = True, \n",
    "                                                    ifplot = False, random_seed = 42, convergence_sensitivity = 0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence_points = [13, 14, 15]\n",
    "classifier_type = 'brfc'\n",
    "\n",
    "explore_classifier_sleep_probabilities_schedule(classifier_type, initial_pre_sleep = 2, initial_awake = 30, convergence_points = convergence_points, \n",
    "                                                    total_epochs = 5, mydata_normalised_preprocessed_one_hot = data_train,\n",
    "                                                    selected_participant_data = selected_participants_data, selected_participants = selected_participants,\n",
    "                                                    output_path = 'Balanced_Random_Forest_Schedule_clean_onset', ifsaveplots = True, \n",
    "                                                    ifplot = False, random_seed = 42, convergence_sensitivity = 0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the best window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'Balanced_Random_Forest_Schedule_clean_onset'\n",
    "results_df_all = pd.read_csv(f'{output_path}/results.csv')\n",
    "results_df_all = results_df_all.drop(columns=['Unnamed: 0'])\n",
    "results_df_all.columns\n",
    "# show how many "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'Balanced_Random_Forest_Schedule_clean_onset'\n",
    "results_df_all = pd.read_csv(f'{output_path}/results.csv')\n",
    "results_df_all = results_df_all.drop(columns=['Unnamed: 0'])\n",
    "# only keep results where ifconverged = 1\n",
    "results_df_all_selected = results_df_all[results_df_all['ifconverged'] == 1]\n",
    "\n",
    "# show how many results we have\n",
    "print(f'We have {len(results_df_all_selected)} results')\n",
    "\n",
    "#Only keep the colums: \n",
    "columns = ['convergence_point', 'test_f1_weighted', 'test_f1_pre_sleep', \n",
    "       'test_f1_awake', 'test_precision',\n",
    "       'test_recall', 'test_auc', 'test_one_crossing_percentage',\n",
    "       'test_mean_predicition_time', 'test_std_predicition_time']\n",
    "results_df_all_selected = results_df_all_selected[columns]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sorted_results = results_df_all_selected.sort_values(by=['test_f1_weighted'], ascending=False)\n",
    "# make the values in the table only have 2 decimal places after saving\n",
    "sorted_results = sorted_results.round(2)\n",
    "\n",
    "headers = ['t_{PS}', 'F1_W', 'F1_{PS}','F1_{AW}', 'P', 'S', 'AUC', 'R_ps', 'Mean(t_{pred})', 'STD(t_{pred})']\n",
    "sorted_results.columns = headers\n",
    "# Save only the top 3 results\n",
    "sorted_results = sorted_results.head(5)\n",
    "\n",
    "sorted_results.to_latex(f'{output_path}/results_top5_f1.tex', index=False, float_format=\"%.2f\")\n",
    "\n",
    "print(sorted_results.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'Balanced_Random_Forest_Schedule_clean_onset'\n",
    "results_df_all = pd.read_csv(f'{output_path}/results.csv')\n",
    "results_df_all = results_df_all.drop(columns=['Unnamed: 0'])\n",
    "# only keep results where ifconverged = 1\n",
    "#results_df_all_selected = results_df_all[results_df_all['ifconverged'] == 1]\n",
    "#results_df_all_selected = results_df_all[results_df_all['ifconverged'] == 1]\n",
    "# show how many results we have\n",
    "print(f'We have {len(results_df_all_selected)} results')\n",
    "\n",
    "\n",
    "#Only keep the colums: \n",
    "columns = ['convergence_point', 'test_one_crossing_percentage',\n",
    "       'test_mean_predicition_time', 'test_std_predicition_time', 'test_f1_weighted', 'test_f1_pre_sleep', \n",
    "       'test_f1_awake', 'test_precision',\n",
    "       'test_recall', 'test_auc']\n",
    "results_df_all_selected = results_df_all_selected[columns]\n",
    "\n",
    "\n",
    "\n",
    "sorted_results = results_df_all_selected.sort_values(by=['test_one_crossing_percentage'], ascending=False)\n",
    "# make the values in the table only have 2 decimal places after saving\n",
    "sorted_results = sorted_results.round(2)\n",
    "\n",
    "headers = ['t_{PS}', 'R_ps', 'Mean(t_{pred})', 'STD(t_{pred})', 'F1_W', 'F1_{PS}','F1_{AW}', 'P', 'S', 'AUC']\n",
    "sorted_results.columns = headers\n",
    "# Save only the top 3 results\n",
    "sorted_results = sorted_results.head(5)\n",
    "\n",
    "sorted_results.to_latex(f'{output_path}/results_top5_crossing.tex', index=False, float_format=\"%.2f\")\n",
    "\n",
    "print(sorted_results.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the best models on the test set and save the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import balanced random forest classifier\n",
    "import warnings\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVR\n",
    "# import random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "def test_sleep_probabilities_schedule(classifier_type, initial_pre_sleep, initial_awake, convergence_points, \n",
    "                                                    total_epochs, mydata_normalised_preprocessed_one_hot, mydata_test,\n",
    "                                                    output_path = 'Balanced_Random_Forest_Schedule', ifsaveplots = True, \n",
    "                                                    ifplot = True, random_seed = 42, convergence_sensitivity = 0.1, ifresetresults = False):\n",
    "\n",
    "    \"\"\"\n",
    "    This function is used to explore the probabilities and accuracies outputted by classifier in predicting sleep onset.\n",
    "    \"\"\"\n",
    "    \n",
    "    warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "    warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "\n",
    "    # Initialize a DataFrame to store results\n",
    "    columns = ['convergence_point',  'ifconverged', 'test_f1_weighted', 'test_accuracy', 'test_precision',\n",
    "           'test_recall',  'test_f1_macro', \n",
    "           'test_f1_pre_sleep', 'test_f1_awake',  'test_auc', 'test_one_crossing_percentage', \n",
    "           'test_mean_predicition_time', 'test_std_predicition_time', \n",
    "           'test_one_crossing_percentage_smoothed', 'test_mean_predicition_time_smoothed',\n",
    "            'test_std_predicition_time_smoothed']\n",
    "            \n",
    "    results_df = pd.DataFrame(columns=columns)\n",
    "    results_dir = output_path\n",
    "\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "\n",
    "    if ifresetresults:\n",
    "        results_df.to_csv(os.path.join(results_dir, 'results.csv'))\n",
    "    \n",
    "    # check if results.csv already exists\n",
    "    if os.path.exists(os.path.join(results_dir, 'results.csv')):\n",
    "        results_df = pd.read_csv(os.path.join(results_dir, 'results.csv'), index_col=0)\n",
    "    else:\n",
    "        # save the empty dataframe\n",
    "        results_df.to_csv(os.path.join(results_dir, 'results.csv'))\n",
    "\n",
    "\n",
    "    for convergence_point in convergence_points:\n",
    "        print('Convergence point: ', convergence_point)\n",
    "        folder_name = f'{convergence_point}_convergence_point'\n",
    "        output_path_conv = os.path.join(results_dir, folder_name)\n",
    "        if not os.path.exists(output_path_conv):\n",
    "            os.makedirs(output_path_conv)\n",
    "\n",
    "        distance_to_convergence_sleep_onset = convergence_point - initial_pre_sleep\n",
    "        # round to 1 decimal place\n",
    "        sleep_onset_delta = round(distance_to_convergence_sleep_onset / total_epochs, 1)\n",
    "\n",
    "        distance_to_convergence_awake = initial_awake - convergence_point\n",
    "        # round to 1 decimal place\n",
    "        awake_delta = round(distance_to_convergence_awake / total_epochs, 1)\n",
    "        \n",
    "        brfc = BalancedRandomForestClassifier(n_estimators=50, random_state=42, sampling_strategy = 'all', replacement = True, warm_start = True)\n",
    "        \n",
    "        pre_sleep_window = initial_pre_sleep\n",
    "        awake_window = initial_awake\n",
    "        for epoch in range(total_epochs):\n",
    "            if epoch != 0:\n",
    "                n_estimators = brfc.n_estimators + 10\n",
    "                brfc.set_params(n_estimators=n_estimators)\n",
    "\n",
    "            if abs(pre_sleep_window - convergence_point) < convergence_sensitivity:\n",
    "                print('Sleep onset threshold has converged')\n",
    "                break\n",
    "\n",
    "            if abs(awake_window - convergence_point) < convergence_sensitivity:\n",
    "                print('Awake threshold has converged')\n",
    "                break\n",
    "\n",
    "            pre_sleep_window, awake_window = update_thresholds(epoch,  pre_sleep_window, awake_window, awake_delta, sleep_onset_delta)\n",
    "            \n",
    "            folder_name = f'{epoch}_epoch_{pre_sleep_window}_presleep_{awake_window}_awake'\n",
    "            if not os.path.exists(os.path.join(output_path_conv, folder_name)):\n",
    "                os.makedirs(os.path.join(output_path_conv, folder_name))\n",
    "\n",
    "            output_path_new = os.path.join(output_path_conv, folder_name)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #print('New sleep onset threshold: ', sleep_onset_threshold) \n",
    "            #print('New awake threshold: ', awake_threshold)\n",
    "\n",
    "            # Create a new dataframe to store the probabilities and accuracies\n",
    "            mydata_all_labels = mydata_normalised_preprocessed_one_hot.copy()\n",
    "            mydata_all_labels['Old_label'] = mydata_all_labels['Label'].copy()\n",
    "\n",
    "\n",
    "            \n",
    "            # Swap Label columns into categorical '5 minutes before sleep'  (if Label <= 5) and '15+ minutes before sleep' in the other case\n",
    "            index = [f'{pre_sleep_window} minutes before sleep', 'Awake']\n",
    "            label = [f'{pre_sleep_window} minutes before sleep', 'Awake']\n",
    "            mydata_all_labels['Label'] = np.where(mydata_all_labels['Label'] <= pre_sleep_window, f'{pre_sleep_window} minutes before sleep', 'Awake')\n",
    "\n",
    "            \n",
    "            mydata_testbothlabels = mydata_test.copy()\n",
    "            mydata_testbothlabels['Old_label'] = mydata_testbothlabels['Label'].copy()\n",
    "            mydata_testbothlabels['Label'] = np.where(mydata_testbothlabels['Label'] <= pre_sleep_window, f'{pre_sleep_window} minutes before sleep', 'Awake')\n",
    "        \n",
    "\n",
    "            print('\\n')\n",
    "            print('_'*110)\n",
    "            print(f'                                    PRE-SLEEP WINDOW: {pre_sleep_window} minutes                                   ')\n",
    "            print('_'*110)\n",
    "            print('\\n')\n",
    "\n",
    "            print(f'--------------------------------------- AWAKE-WINDOW: {awake_window}+ minutes ------------------------------------------------')\n",
    "            print('\\n')\n",
    "\n",
    "\n",
    "            # Try classification on 5 minutes before sleep and 15+ minutes before sleep\n",
    "            mydata_for_training = mydata_normalised_preprocessed_one_hot[(mydata_normalised_preprocessed_one_hot['Label'] <= pre_sleep_window) | (mydata_normalised_preprocessed_one_hot['Label'] > awake_window)].copy()\n",
    "\n",
    "            # Swap Label columns into categorical '5 minutes before sleep'  (if Label <= 5) and '15+ minutes before sleep' in the other case\n",
    "            mydata_for_training['Label'] = np.where(mydata_for_training['Label'] <= pre_sleep_window, f'{pre_sleep_window} minutes before sleep', 'Awake')\n",
    "\n",
    "            # Split test, and training sets for training the model\n",
    "            X_train, y_train, _, _, sbj_train_set, _ = train_test_split_sbjs(mydata_for_training, test_size = 0, random_seed = 42)\n",
    "        \n",
    "\n",
    "            # Get the timeline for checking the model performance\n",
    "            X_all, y_all, y_old_all, y_sleepstage_all = get_timeline(mydata_all_labels)\n",
    "\n",
    "            \n",
    "            X_test, y_test, _, _, sbj_test_set, _ = train_test_split_sbjs(mydata_testbothlabels, test_size = 0, random_seed = 42)\n",
    "            # get only the subjects from sbj_test_set_5_5 in mydata_all_labels_5\n",
    "\n",
    "            # Get the timeline for checking the model performance\n",
    "            X_all_test, y_all_test, y_old_all_test, y_sleepstage_all_test = get_timeline(mydata_testbothlabels)\n",
    "        \n",
    "            # get the positive class weight for y_all\n",
    "            pos_class_weight = len(y_all[y_all == 'Awake'])/len(y_all[y_all == f'{pre_sleep_window} minutes before sleep'])\n",
    "\n",
    "            if classifier_type == 'brfc':\n",
    "                # Create a random forest classifier\n",
    "            \n",
    "                brfc.fit(X_train, y_train)\n",
    "                    \n",
    "                # get the sorted feature importances and their names\n",
    "                _, _, _ = plot_feature_importances(brfc, X_train, ifsaveplots = ifsaveplots, output_path=output_path_new, ifplot = ifplot)\n",
    "             \n",
    "\n",
    "                brfc_classes = brfc.classes_\n",
    "\n",
    "                brfc_predictions = brfc.predict(X_all) \n",
    "                print('Predictions:', brfc_predictions)\n",
    "                brfc_predictions_proba = brfc.predict_proba(X_all)\n",
    "               \n",
    "                brfc_predictions_test_proba = brfc.predict_proba(X_all_test)\n",
    "                brfc_predictions_test = brfc.predict(X_all_test)\n",
    "\n",
    "                # transform predicitons and labels for evalution \n",
    "                \n",
    "\n",
    "                crossval_metrics = evaluate_classification(y_all, brfc_predictions, brfc_predictions_proba, index= index, label = label,\n",
    "                                                        ifplot=ifplot, output_path=output_path_new, ifsaveplots= True, classes = brfc_classes)\n",
    "                crossval_metrics['pos_class_weight'] = pos_class_weight\n",
    "                \n",
    "                test_metrics = evaluate_classification(y_all_test, brfc_predictions_test, brfc_predictions_test_proba, index= index, label = label, iftest=True,\n",
    "                                                        ifplot=ifplot, output_path=output_path_new, ifsaveplots= True, classes = brfc_classes)\n",
    "                \n",
    "                participant_probas, participant_timeline, participant_dict = test_on_random_participants(mydata_testbothlabels, brfc, random_seed = 42, \n",
    "                                                                                            ifplot = False, output_path_new = output_path_new, index = index, label = label,\n",
    "                                                                                            ifsaveplots = ifsaveplots, brfc_classes = brfc_classes, awake_window = awake_window, \n",
    "                                                                                         pre_sleep_window = pre_sleep_window)\n",
    "                \n",
    "                # get the accuracy of each timepoint\n",
    "                print('Now trying only on the unseen test data')\n",
    "                print('------------------------------------------- No averaging ----------------------------------------')\n",
    "                print('Accuracy')\n",
    "                \n",
    "                _, _, _, _ = check_accuracy_timeline(y_pred = brfc_predictions_test, y_true = y_all_test,\n",
    "                                                                            timeline = y_old_all_test, sleep_stage = y_sleepstage_all_test, \n",
    "                                                                            window_for_averaging=1, awake_window=awake_window, pre_sleep_window=pre_sleep_window,\n",
    "                                                                            if_proba=False, classes=None, rnn_window = 0, \n",
    "                                                                            ifsaveplots=True, output_path=output_path_new, ifplot = ifplot)\n",
    "                print('Probability of sleep as predicted by the model')\n",
    "            \n",
    "                \n",
    "                _ = check_accuracy_timeline(y_pred = brfc_predictions_test_proba, y_true = y_all_test,\n",
    "                                                        timeline = y_old_all_test, sleep_stage = y_sleepstage_all_test, \n",
    "                                                        window_for_averaging=1, awake_window=awake_window, pre_sleep_window=pre_sleep_window,\n",
    "                                                        if_proba=True, classes=brfc_classes, rnn_window = 0, \n",
    "                                                        ifsaveplots=True, output_path=output_path_new, ifplot = ifplot)\n",
    "\n",
    "\n",
    "\n",
    "                print('-------------------------------Averaging of accuracy over 30 seconds---------------------------------')\n",
    "                print('Accuracy')\n",
    "                _, _, _, _ = check_accuracy_timeline(y_pred = brfc_predictions_test, y_true = y_all_test,\n",
    "                                                                            timeline = y_old_all_test, sleep_stage = y_sleepstage_all_test, \n",
    "                                                                            window_for_averaging=5, awake_window=awake_window, pre_sleep_window=pre_sleep_window,\n",
    "                                                                            if_proba=False, classes=None, rnn_window = 0, \n",
    "                                                                            ifsaveplots=True, output_path=output_path_new, ifplot = ifplot)\n",
    "                print('Probability of sleep as predicted by the model')\n",
    "                \n",
    "                y_pred_sorted = check_accuracy_timeline(y_pred = brfc_predictions_test_proba, y_true = y_all_test,\n",
    "                                                        timeline = y_old_all_test, sleep_stage = y_sleepstage_all_test, \n",
    "                                                        window_for_averaging=5, awake_window=awake_window, pre_sleep_window=pre_sleep_window,\n",
    "                                                        if_proba=True, classes=brfc_classes, rnn_window = 0, \n",
    "                                                        ifsaveplots=True, output_path=output_path_new, ifplot = ifplot)\n",
    "\n",
    "                \n",
    "                # Perform regression to predict the time to sleep onset\n",
    "\n",
    "                y_pred_sorted = check_accuracy_timeline(y_pred = brfc_predictions_proba, y_true = y_all,\n",
    "                                                        timeline = y_old_all, sleep_stage = y_sleepstage_all, \n",
    "                                                        window_for_averaging=5, awake_window=awake_window, pre_sleep_window=pre_sleep_window,\n",
    "                                                        if_proba=True, classes=brfc_classes, rnn_window = 0, \n",
    "                                                        ifsaveplots=False, output_path=output_path_new, ifplot = ifplot)\n",
    "\n",
    "                \n",
    "                # From averqaged probabilities, try to do regression to predict the time to sleep onset\n",
    "                y_times2sleep = list(y_pred_sorted.keys())\n",
    "                probabilities = list(y_pred_sorted.values())\n",
    "                probabilities, y_times2sleep = get_rid_of_nans(probabilities, y_times2sleep)\n",
    "\n",
    "                # build a regressor to predict the time to sleep onset using svm\n",
    "                #svr = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\n",
    "                rf = RandomForestRegressor(n_estimators=150, random_state=42)\n",
    "                    \n",
    "                probabilities = np.array(probabilities).reshape(-1, 1)\n",
    "\n",
    "                rf.fit(probabilities, y_times2sleep)\n",
    "                rf_predictions = rf.predict(probabilities)\n",
    "                crossval_regression_metrics = evaluate_regression(rf_predictions, y_times2sleep, ifsave = ifsaveplots, \n",
    "                                                                ifplot=ifplot, savepath=output_path_new, iftest = False)\n",
    "\n",
    "                # get the predictions for the test data \n",
    "                class_idx = np.where(brfc_classes != 'Awake')[0][0]\n",
    "                probs_test = brfc_predictions_test_proba[:,class_idx]\n",
    "                probs_test, y_old_all_test = get_rid_of_nans(probs_test, y_old_all_test)\n",
    "                y_pred_times2sleep = rf.predict(probs_test.reshape(-1, 1))\n",
    "                test_regression_metrics = evaluate_regression(y_pred_times2sleep, y_old_all_test, ifsave = ifsaveplots, \n",
    "                                                                ifplot=ifplot, savepath=output_path_new, iftest = True)\n",
    "                \n",
    "                test_reg_on_random_participants(participant_probas, participant_timeline, rf, random_seed, ifplot, \n",
    "                                                output_path_new, index, brfc_classes, awake_window, pre_sleep_window)\n",
    "\n",
    "                # Add results to dataframe\n",
    "                ifconverged = 0\n",
    "                if epoch + 1 == total_epochs:\n",
    "                    ifconverged = 1\n",
    "\n",
    "                new_row = pd.DataFrame({\n",
    "                        'convergence_point': [convergence_point],\n",
    "                        'ifconverged': [ifconverged],\n",
    "                        'pre_sleep_window': [pre_sleep_window],\n",
    "                        'awake_window': [awake_window],\n",
    "                        'pos_class_weight': [crossval_metrics['pos_class_weight']],\n",
    "                        'crossval_accuracy': [crossval_metrics['accuracy']],\n",
    "                        'crossval_precision': [crossval_metrics['precision']],\n",
    "                        'crossval_recall': [crossval_metrics['recall']],\n",
    "                        'crossval_f1_weighted': [crossval_metrics['f1_weighted']],\n",
    "                        'crossval_f1_macro': [crossval_metrics['f1_macro']],\n",
    "                        'crossval_f1_pre_sleep': [crossval_metrics['f1_pre_sleep']],\n",
    "                        'crossval_f1_awake': [crossval_metrics['f1_awake']],\n",
    "                        'crossval_auc': [crossval_metrics['auc']],\n",
    "                        'cross_val_mae': [crossval_regression_metrics['MAE']],\n",
    "                        'cross_val_mse': [crossval_regression_metrics['MSE']],\n",
    "                        'cross_val_rmse': [crossval_regression_metrics['RMSE']],\n",
    "                        'cross_val_r2': [crossval_regression_metrics['R2']],\n",
    "                        'cross_val_custom_mse': [crossval_regression_metrics['custom_MSE']],\n",
    "                        'test_accuracy': [test_metrics['accuracy']],\n",
    "                        'test_precision': [test_metrics['precision']],\n",
    "                        'test_recall': [test_metrics['recall']],\n",
    "                        'test_f1_weighted': [test_metrics['f1_weighted']],\n",
    "                        'test_f1_macro': [test_metrics['f1_macro']],\n",
    "                        'test_f1_pre_sleep': [test_metrics['f1_pre_sleep']],\n",
    "                        'test_f1_awake': [test_metrics['f1_awake']],\n",
    "                        'test_auc': [test_metrics['auc']],\n",
    "                        'test_one_crossing_percentage': [participant_dict['one_crossing_percentage']],\n",
    "                        'test_mean_predicition_time': [participant_dict['mean_time_of_prediction']],\n",
    "                        'test_std_predicition_time': [participant_dict['std_time_of_prediction']],\n",
    "                        'test_one_crossing_percentage_smoothed': [participant_dict['one_crossing_percentage_smoothed']],\n",
    "                        'test_mean_predicition_time_smoothed': [participant_dict['mean_time_of_prediction_smoothed']],\n",
    "                        'test_std_predicition_time_smoothed': [participant_dict['std_time_of_prediction_smoothed']],\n",
    "                        'test_mae': [test_regression_metrics['MAE']],\n",
    "                        'test_mse': [test_regression_metrics['MSE']],\n",
    "                        'test_rmse': [test_regression_metrics['RMSE']],\n",
    "                        'test_r2': [test_regression_metrics['R2']],\n",
    "                        'test_custom_mse':[test_regression_metrics['custom_MSE']]\n",
    "                    })\n",
    "                \n",
    "                # Open the results dataframe and add the new results\n",
    "                results_df_all = pd.read_csv(os.path.join(results_dir, 'results.csv'))\n",
    "                results_df_all = pd.concat([results_df_all, new_row], ignore_index=True)\n",
    "                results_df_all.to_csv(os.path.join(results_dir, 'results.csv'), index=False) \n",
    "                del results_df_all\n",
    "                del new_row\n",
    "                \n",
    "                \n",
    "\n",
    "            del y_old_all\n",
    "            del y_sleepstage_all\n",
    "            del y_old_all_test\n",
    "            del y_sleepstage_all_test\n",
    "            del mydata_all_labels\n",
    "            del X_train\n",
    "            del y_train\n",
    "            del X_test\n",
    "            del y_test\n",
    "            del sbj_train_set\n",
    "            del sbj_test_set\n",
    "            del X_all\n",
    "            del y_all\n",
    "            del X_all_test\n",
    "            del y_all_test\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_no_nan = data_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence_points = [10]\n",
    "test_sleep_probabilities_schedule(classifier_type = 'brfc', initial_pre_sleep = 2, initial_awake = 30, convergence_points = convergence_points, \n",
    "                                                    total_epochs = 5, mydata_normalised_preprocessed_one_hot = data_train, mydata_test = data_test_no_nan,\n",
    "                                                    output_path = 'Balanced_Random_Forest_Schedule_clean_onset/test_best_models', ifsaveplots = False, \n",
    "                                                    ifplot = False, random_seed = 42, convergence_sensitivity = 0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of participants with only one crossing: 70.12987012987013%\n",
      "Mean time of prediction for participants with only one crossing: 10.46851851851852 minutes\n",
      "STD of time of prediction for participants with only one crossing: 7.071194149088364 minutes\n",
      "Percentage of participants with only one crossing (smoothed): 64.93506493506493%\n",
      "Mean time of prediction for participants with only one crossing (smoothed): 10.660000000000002 minutes\n",
      "STD of time of prediction for participants with only one crossing (smoothed): 7.399891891102194 minutes\n",
      "Now trying only on the unseen test data\n",
      "------------------------------------------- No averaging ----------------------------------------\n",
      "Accuracy\n",
      "Probability of sleep as predicted by the model\n",
      "-------------------------------Averaging of accuracy over 30 seconds---------------------------------\n",
      "Accuracy\n",
      "Probability of sleep as predicted by the model\n",
      "\n",
      "\n",
      "______________________________________________________________________________________________________________\n",
      "                                    PRE-SLEEP WINDOW: 3.6 minutes                                   \n",
      "______________________________________________________________________________________________________________\n",
      "\n",
      "\n",
      "--------------------------------------- AWAKE-WINDOW: 20.4+ minutes ------------------------------------------------\n",
      "\n",
      "\n",
      "Predictions: ['Awake' 'Awake' 'Awake' ... '3.6 minutes before sleep'\n",
      " '3.6 minutes before sleep' '3.6 minutes before sleep']\n",
      "Accuracy: 0.6602261553588987\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "3.6 minutes before sleep       0.26      0.84      0.40      2736\n",
      "                   Awake       0.96      0.63      0.76     17604\n",
      "\n",
      "                accuracy                           0.66     20340\n",
      "               macro avg       0.61      0.74      0.58     20340\n",
      "            weighted avg       0.87      0.66      0.71     20340\n",
      "\n",
      "F1 pre-sleep:  0.7631353463344416\n",
      "F1 awake:  0.3992002086412241\n",
      "Percentage of participants with only one crossing: 67.53246753246754%\n",
      "Mean time of prediction for participants with only one crossing: 10.982692307692307 minutes\n",
      "STD of time of prediction for participants with only one crossing: 7.229738827973667 minutes\n",
      "Percentage of participants with only one crossing (smoothed): 66.23376623376623%\n",
      "Mean time of prediction for participants with only one crossing (smoothed): 10.739215686274513 minutes\n",
      "STD of time of prediction for participants with only one crossing (smoothed): 7.34302599460202 minutes\n",
      "Now trying only on the unseen test data\n",
      "------------------------------------------- No averaging ----------------------------------------\n",
      "Accuracy\n",
      "Probability of sleep as predicted by the model\n",
      "-------------------------------Averaging of accuracy over 30 seconds---------------------------------\n",
      "Accuracy\n",
      "Probability of sleep as predicted by the model\n",
      "\n",
      "\n",
      "______________________________________________________________________________________________________________\n",
      "                                    PRE-SLEEP WINDOW: 4.4 minutes                                   \n",
      "______________________________________________________________________________________________________________\n",
      "\n",
      "\n",
      "--------------------------------------- AWAKE-WINDOW: 15.6+ minutes ------------------------------------------------\n",
      "\n",
      "\n",
      "Predictions: ['Awake' 'Awake' 'Awake' ... '4.4 minutes before sleep'\n",
      " '4.4 minutes before sleep' '4.4 minutes before sleep']\n",
      "Accuracy: 0.6710422812192723\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "4.4 minutes before sleep       0.30      0.81      0.44      3278\n",
      "                   Awake       0.95      0.64      0.77     17062\n",
      "\n",
      "                accuracy                           0.67     20340\n",
      "               macro avg       0.63      0.73      0.60     20340\n",
      "            weighted avg       0.84      0.67      0.71     20340\n",
      "\n",
      "F1 pre-sleep:  0.7667259352229543\n",
      "F1 awake:  0.44227723597566054\n",
      "Percentage of participants with only one crossing: 68.83116883116884%\n",
      "Mean time of prediction for participants with only one crossing: 10.741509433962264 minutes\n",
      "STD of time of prediction for participants with only one crossing: 7.345501356234256 minutes\n",
      "Percentage of participants with only one crossing (smoothed): 63.63636363636363%\n",
      "Mean time of prediction for participants with only one crossing (smoothed): 10.424489795918367 minutes\n",
      "STD of time of prediction for participants with only one crossing (smoothed): 7.151467113994787 minutes\n",
      "Now trying only on the unseen test data\n",
      "------------------------------------------- No averaging ----------------------------------------\n",
      "Accuracy\n",
      "Probability of sleep as predicted by the model\n",
      "-------------------------------Averaging of accuracy over 30 seconds---------------------------------\n",
      "Accuracy\n",
      "Probability of sleep as predicted by the model\n",
      "\n",
      "\n",
      "______________________________________________________________________________________________________________\n",
      "                                    PRE-SLEEP WINDOW: 5.2 minutes                                   \n",
      "______________________________________________________________________________________________________________\n",
      "\n",
      "\n",
      "--------------------------------------- AWAKE-WINDOW: 10.8+ minutes ------------------------------------------------\n",
      "\n",
      "\n",
      "Predictions: ['Awake' 'Awake' 'Awake' ... '5.2 minutes before sleep'\n",
      " '5.2 minutes before sleep' '5.2 minutes before sleep']\n",
      "Accuracy: 0.6836283185840708\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "5.2 minutes before sleep       0.35      0.79      0.48      3818\n",
      "                   Awake       0.93      0.66      0.77     16522\n",
      "\n",
      "                accuracy                           0.68     20340\n",
      "               macro avg       0.64      0.73      0.63     20340\n",
      "            weighted avg       0.82      0.68      0.72     20340\n",
      "\n",
      "F1 pre-sleep:  0.771768043979429\n",
      "F1 awake:  0.4845814977973568\n",
      "Percentage of participants with only one crossing: 71.42857142857143%\n",
      "Mean time of prediction for participants with only one crossing: 10.336363636363636 minutes\n",
      "STD of time of prediction for participants with only one crossing: 7.427207076598693 minutes\n",
      "Percentage of participants with only one crossing (smoothed): 63.63636363636363%\n",
      "Mean time of prediction for participants with only one crossing (smoothed): 10.404081632653062 minutes\n",
      "STD of time of prediction for participants with only one crossing (smoothed): 7.139226482529205 minutes\n",
      "Now trying only on the unseen test data\n",
      "------------------------------------------- No averaging ----------------------------------------\n",
      "Accuracy\n",
      "Probability of sleep as predicted by the model\n",
      "-------------------------------Averaging of accuracy over 30 seconds---------------------------------\n",
      "Accuracy\n",
      "Probability of sleep as predicted by the model\n",
      "\n",
      "\n",
      "______________________________________________________________________________________________________________\n",
      "                                    PRE-SLEEP WINDOW: 6.0 minutes                                   \n",
      "______________________________________________________________________________________________________________\n",
      "\n",
      "\n",
      "--------------------------------------- AWAKE-WINDOW: 6.0+ minutes ------------------------------------------------\n",
      "\n",
      "\n",
      "Predictions: ['Awake' 'Awake' 'Awake' ... '6.0 minutes before sleep'\n",
      " '6.0 minutes before sleep' '6.0 minutes before sleep']\n",
      "Accuracy: 0.6918879056047198\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "6.0 minutes before sleep       0.39      0.77      0.51      4330\n",
      "                   Awake       0.91      0.67      0.77     16010\n",
      "\n",
      "                accuracy                           0.69     20340\n",
      "               macro avg       0.65      0.72      0.64     20340\n",
      "            weighted avg       0.80      0.69      0.72     20340\n",
      "\n",
      "F1 pre-sleep:  0.7742516479953891\n",
      "F1 awake:  0.5149005340970664\n",
      "Percentage of participants with only one crossing: 70.12987012987013%\n",
      "Mean time of prediction for participants with only one crossing: 10.407407407407407 minutes\n",
      "STD of time of prediction for participants with only one crossing: 7.421686237762748 minutes\n",
      "Percentage of participants with only one crossing (smoothed): 63.63636363636363%\n",
      "Mean time of prediction for participants with only one crossing (smoothed): 10.455102040816328 minutes\n",
      "STD of time of prediction for participants with only one crossing (smoothed): 7.15151079298918 minutes\n",
      "Now trying only on the unseen test data\n",
      "------------------------------------------- No averaging ----------------------------------------\n",
      "Accuracy\n",
      "Probability of sleep as predicted by the model\n",
      "-------------------------------Averaging of accuracy over 30 seconds---------------------------------\n",
      "Accuracy\n",
      "Probability of sleep as predicted by the model\n"
     ]
    }
   ],
   "source": [
    "convergence_points = [6]\n",
    "test_sleep_probabilities_schedule(classifier_type = 'brfc', initial_pre_sleep = 2, initial_awake = 30, convergence_points = convergence_points, \n",
    "                                                    total_epochs = 5, mydata_normalised_preprocessed_one_hot = data_train, mydata_test = data_test_no_nan,\n",
    "                                                    output_path = 'Balanced_Random_Forest_Schedule_clean_onset/test_best_models', ifsaveplots = True, \n",
    "                                                    ifplot = False, random_seed = 42, convergence_sensitivity = 0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   t_{PS}   R_ps  Mean(t_{pred})  STD(t_{pred})  F1_W  F1_{PS}  F1_{AW}     P  \\\n",
      "4      10  74.03           11.29           7.80  0.70     0.74     0.61  0.54   \n",
      "9       6  70.13           10.41           7.42  0.72     0.77     0.51  0.39   \n",
      "\n",
      "      S   AUC  \n",
      "4  0.71  0.75  \n",
      "9  0.77  0.78  \n"
     ]
    }
   ],
   "source": [
    "folder_name = 'Balanced_Random_Forest_Schedule_clean_onset'\n",
    "results_saving_dir = 'Balanced_Random_Forest_Schedule_clean_onset/test_best_models'\n",
    "results_final_test = pd.read_csv(f'{results_saving_dir}/results.csv')\n",
    "# only keep results where ifconverged = 1\n",
    "results_final_test = results_final_test[results_final_test['ifconverged'] == 1]\n",
    "columns = ['convergence_point', 'test_one_crossing_percentage',\n",
    "       'test_mean_predicition_time', 'test_std_predicition_time', 'test_f1_weighted', 'test_f1_pre_sleep', \n",
    "       'test_f1_awake', 'test_precision',\n",
    "       'test_recall', 'test_auc']\n",
    "results_final_test_selected = results_final_test[columns]\n",
    "\n",
    "sorted_results = results_final_test_selected.sort_values(by=['test_one_crossing_percentage'], ascending=False)\n",
    "sorted_results = sorted_results.round(2)\n",
    "headers = ['t_{PS}', 'R_ps', 'Mean(t_{pred})', 'STD(t_{pred})', 'F1_W', 'F1_{PS}','F1_{AW}', 'P', 'S', 'AUC']\n",
    "sorted_results.columns = headers\n",
    "\n",
    "sorted_results.to_latex(f'{folder_name}/results_test_best.tex', index=False, float_format=\"%.2f\")\n",
    "print(sorted_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
