{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18329,"status":"ok","timestamp":1693954083089,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"},"user_tz":-60},"id":"mlxNtzmEYQCn","outputId":"cd95fb1b-3aef-4187-fb0a-1fd0f5f7497e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"YkCnikvOVK8A"},"source":["## Attach google drive\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10103,"status":"ok","timestamp":1693954093190,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"},"user_tz":-60},"id":"mE-11idpVXaH","outputId":"4c73dabb-cd22-4619-a5a5-2d60152c8f5a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.12.2)\n","Collecting imblearn\n","  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.42.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n","Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (from imblearn) (0.10.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n","Installing collected packages: imblearn\n","Successfully installed imblearn-0.0\n"]}],"source":["!pip install scikit-learn pandas numpy matplotlib seaborn imblearn scipy"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1693954093190,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"},"user_tz":-60},"id":"V0dTG0LwOctB"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ElvUVfxvVJo4"},"source":["## Import all the necessary libraries and packages\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":7934,"status":"ok","timestamp":1693954101123,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"},"user_tz":-60},"id":"TH40xgi5VJo8"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import sklearn\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import cross_val_predict\n","from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import cross_val_predict\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import r2_score\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","from sklearn.preprocessing import StandardScaler\n","import seaborn as sns\n","import scipy\n","import random"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4257,"status":"ok","timestamp":1693954105377,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"},"user_tz":-60},"id":"61iz5piwVJo9"},"outputs":[],"source":["data_train = pd.read_csv('/content/drive/MyDrive/sleep onset datasets/data_perfectly_clean_train.csv')\n","data_test = pd.read_csv('/content/drive/MyDrive/sleep onset datasets/data_perfectly_clean_test.csv')\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1693954105377,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"},"user_tz":-60},"id":"UlTmXkEgVJo-","outputId":"197b7f66-1d7c-4577-ff01-65c2bfb34bf1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["       Label  Sbj_ID  Age  Gender  Race1  Race2  Race3  Race4  ifCleanOnset  \\\n","0       29.9     323   79       1    1.0    0.0    0.0    0.0             1   \n","1       29.8     323   79       1    1.0    0.0    0.0    0.0             1   \n","2       29.7     323   79       1    1.0    0.0    0.0    0.0             1   \n","3       29.6     323   79       1    1.0    0.0    0.0    0.0             1   \n","4       29.5     323   79       1    1.0    0.0    0.0    0.0             1   \n","...      ...     ...  ...     ...    ...    ...    ...    ...           ...   \n","16195    0.4    6629   78       0    0.0    0.0    0.0    1.0             1   \n","16196    0.3    6629   78       0    0.0    0.0    0.0    1.0             1   \n","16197    0.2    6629   78       0    0.0    0.0    0.0    1.0             1   \n","16198    0.1    6629   78       0    0.0    0.0    0.0    1.0             1   \n","16199    0.0    6629   78       0    0.0    0.0    0.0    1.0             1   \n","\n","       Time2Sleep  ...  wrseltr5  slpapnea5  cpap5  dntaldv5  uvula5  \\\n","0            68.5  ...      -1.0        0.0    0.0       0.0     0.0   \n","1            68.5  ...      -1.0        0.0    0.0       0.0     0.0   \n","2            68.5  ...      -1.0        0.0    0.0       0.0     0.0   \n","3            68.5  ...      -1.0        0.0    0.0       0.0     0.0   \n","4            68.5  ...      -1.0        0.0    0.0       0.0     0.0   \n","...           ...  ...       ...        ...    ...       ...     ...   \n","16195        43.5  ...       1.0        0.0    0.0       0.0     0.0   \n","16196        43.5  ...       1.0        0.0    0.0       0.0     0.0   \n","16197        43.5  ...       1.0        0.0    0.0       0.0     0.0   \n","16198        43.5  ...       1.0        0.0    0.0       0.0     0.0   \n","16199        43.5  ...       1.0        0.0    0.0       0.0     0.0   \n","\n","       insmnia5  rstlesslgs5  whiirs5c  epslpscl5c  hoostmeq5c  \n","0           0.0          0.0       8.0        11.0        15.0  \n","1           0.0          0.0       8.0        11.0        15.0  \n","2           0.0          0.0       8.0        11.0        15.0  \n","3           0.0          0.0       8.0        11.0        15.0  \n","4           0.0          0.0       8.0        11.0        15.0  \n","...         ...          ...       ...         ...         ...  \n","16195       0.0          0.0      19.0        13.0        11.0  \n","16196       0.0          0.0      19.0        13.0        11.0  \n","16197       0.0          0.0      19.0        13.0        11.0  \n","16198       0.0          0.0      19.0        13.0        11.0  \n","16199       0.0          0.0      19.0        13.0        11.0  \n","\n","[16200 rows x 90 columns]"],"text/html":["\n","  <div id=\"df-936b60ec-2a93-4286-9785-dd6efccb356b\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Label</th>\n","      <th>Sbj_ID</th>\n","      <th>Age</th>\n","      <th>Gender</th>\n","      <th>Race1</th>\n","      <th>Race2</th>\n","      <th>Race3</th>\n","      <th>Race4</th>\n","      <th>ifCleanOnset</th>\n","      <th>Time2Sleep</th>\n","      <th>...</th>\n","      <th>wrseltr5</th>\n","      <th>slpapnea5</th>\n","      <th>cpap5</th>\n","      <th>dntaldv5</th>\n","      <th>uvula5</th>\n","      <th>insmnia5</th>\n","      <th>rstlesslgs5</th>\n","      <th>whiirs5c</th>\n","      <th>epslpscl5c</th>\n","      <th>hoostmeq5c</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>29.9</td>\n","      <td>323</td>\n","      <td>79</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>68.5</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8.0</td>\n","      <td>11.0</td>\n","      <td>15.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>29.8</td>\n","      <td>323</td>\n","      <td>79</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>68.5</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8.0</td>\n","      <td>11.0</td>\n","      <td>15.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>29.7</td>\n","      <td>323</td>\n","      <td>79</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>68.5</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8.0</td>\n","      <td>11.0</td>\n","      <td>15.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>29.6</td>\n","      <td>323</td>\n","      <td>79</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>68.5</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8.0</td>\n","      <td>11.0</td>\n","      <td>15.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>29.5</td>\n","      <td>323</td>\n","      <td>79</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>68.5</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8.0</td>\n","      <td>11.0</td>\n","      <td>15.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>16195</th>\n","      <td>0.4</td>\n","      <td>6629</td>\n","      <td>78</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>43.5</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>19.0</td>\n","      <td>13.0</td>\n","      <td>11.0</td>\n","    </tr>\n","    <tr>\n","      <th>16196</th>\n","      <td>0.3</td>\n","      <td>6629</td>\n","      <td>78</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>43.5</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>19.0</td>\n","      <td>13.0</td>\n","      <td>11.0</td>\n","    </tr>\n","    <tr>\n","      <th>16197</th>\n","      <td>0.2</td>\n","      <td>6629</td>\n","      <td>78</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>43.5</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>19.0</td>\n","      <td>13.0</td>\n","      <td>11.0</td>\n","    </tr>\n","    <tr>\n","      <th>16198</th>\n","      <td>0.1</td>\n","      <td>6629</td>\n","      <td>78</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>43.5</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>19.0</td>\n","      <td>13.0</td>\n","      <td>11.0</td>\n","    </tr>\n","    <tr>\n","      <th>16199</th>\n","      <td>0.0</td>\n","      <td>6629</td>\n","      <td>78</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>43.5</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>19.0</td>\n","      <td>13.0</td>\n","      <td>11.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>16200 rows × 90 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-936b60ec-2a93-4286-9785-dd6efccb356b')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-936b60ec-2a93-4286-9785-dd6efccb356b button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-936b60ec-2a93-4286-9785-dd6efccb356b');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-a26eea22-27f4-437b-87b5-aa9ca649bfdf\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a26eea22-27f4-437b-87b5-aa9ca649bfdf')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-a26eea22-27f4-437b-87b5-aa9ca649bfdf button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":5}],"source":["data_test"]},{"cell_type":"code","source":["data_test.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4_a6jZ-F_Yxb","executionInfo":{"status":"ok","timestamp":1693954105377,"user_tz":-60,"elapsed":3,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}},"outputId":"2da93e9e-640e-488c-a3ad-9b44590b186a"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['Label', 'Sbj_ID', 'Age', 'Gender', 'Race1', 'Race2', 'Race3', 'Race4',\n","       'ifCleanOnset', 'Time2Sleep', 'SleepStage', 'delta', 'theta', 'alpha',\n","       'beta', 'DTratio', 'DAratio', 'DBratio', 'TAratio', 'TBratio',\n","       'ABratio', 'Transratio', 'deltapeak', 'thetapeak', 'alphapeak',\n","       'betapeak', 'TCdelta', 'TCtheta', 'TCalpha', 'TCbeta',\n","       'DN_HistogramMode_5', 'DN_HistogramMode_10', 'CO_f1ecac',\n","       'CO_FirstMin_ac', 'CO_HistogramAMI_even_2_5', 'CO_trev_1_num',\n","       'MD_hrv_classic_pnn40', 'SB_BinaryStats_mean_longstretch1',\n","       'SB_TransitionMatrix_3ac_sumdiagcov', 'PD_PeriodicityWang_th0_01',\n","       'CO_Embed2_Dist_tau_d_expfit_meandiff',\n","       'IN_AutoMutualInfoStats_40_gaussian_fmmi',\n","       'FC_LocalSimple_mean1_tauresrat', 'DN_OutlierInclude_p_001_mdrmd',\n","       'DN_OutlierInclude_n_001_mdrmd', 'SP_Summaries_welch_rect_area_5_1',\n","       'SB_BinaryStats_diff_longstretch0', 'SB_MotifThree_quantile_hh',\n","       'SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1',\n","       'SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1',\n","       'SP_Summaries_welch_rect_centroid', 'FC_LocalSimple_mean3_stderr',\n","       'gamma', 'GAration', 'GBratio', 'GTratio', 'GDratio', 'gammapeak',\n","       'TCgamma', 'DTcouple', 'DAcouple', 'DBcouple', 'TAcouple', 'TBcouple',\n","       'ABcouple', 'DGcouple', 'TGcouple', 'AGcouple', 'BGcouple',\n","       'Entropy Rate LZ', 'Aperiodic component (spectral exponent)',\n","       'O-information', 'S-information', 'nap5', 'trbleslpng5', 'slpngpills5',\n","       'sleepy5', 'legsdscmfrt5', 'rubbnglgs5', 'wrserest5', 'wrseltr5',\n","       'slpapnea5', 'cpap5', 'dntaldv5', 'uvula5', 'insmnia5', 'rstlesslgs5',\n","       'whiirs5c', 'epslpscl5c', 'hoostmeq5c'],\n","      dtype='object')"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1709,"status":"ok","timestamp":1693954107084,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"},"user_tz":-60},"id":"y4l3h3K2VJo_","outputId":"fd8f408d-a56f-4a4c-c50c-bb55eb648c75"},"outputs":[{"output_type":"stream","name":"stdout","text":["{5749: 1, 5881: 2, 4190: 9, 323: 10, 5876: 10, 694: 11, 2040: 12, 2780: 12, 10: 12, 2651: 13, 4437: 13, 4014: 14, 5782: 14, 6454: 14, 1476: 15, 5838: 19, 5489: 20, 3920: 21, 2574: 21, 912: 22, 3106: 22, 1405: 24, 6316: 24, 6072: 26, 4555: 27, 3211: 28, 6261: 29, 823: 30, 2681: 31, 4157: 33, 4806: 35, 384: 35, 1501: 40, 5784: 42, 2750: 42, 6629: 43, 2375: 46, 5939: 49, 614: 54, 3855: 54, 5476: 54, 5240: 56, 3980: 63, 460: 65, 1033: 69, 893: 70, 2413: 70, 2377: 74, 658: 75, 3641: 79, 3793: 86, 5438: 92, 5722: 99, 3894: 105}\n"]}],"source":["# plot how many nans there are for each participant in data_test\n","\n","test_unique_ids = data_test['Sbj_ID'].unique()\n","test_unique_ids = test_unique_ids.tolist()\n","len(test_unique_ids)\n","\n","# calculate how many nans there are for each participant in data_test\n","nans_per_participant = {}\n","for i in test_unique_ids:\n","    nans_per_participant[i] = data_test[data_test['Sbj_ID'] == i]['delta'].isnull().sum()\n","\n","# get the ids of the pariticpants with the least nans\n","nans_per_participant = {k: v for k, v in sorted(nans_per_participant.items(), key=lambda item: item[1])}\n","print(nans_per_participant)\n","\n","# get the top 20 participants with the least nans\n","top_20 = list(nans_per_participant.keys())[:20]\n","\n","# get data_test with only the top 20 participants with the least nans\n","selected_participants_data = data_test[data_test['Sbj_ID'].isin(top_20)]\n","\n","# save the data_test with only the top 20 participants with the least nans\n","selected_participants_data.to_csv('data_test_perfectly_clean_top_20.csv', index=False)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1693954107085,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"},"user_tz":-60},"id":"trKQkdGYVJo_","outputId":"4959f3cc-a921-42e3-b79b-8daa4e06bf16"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[323,\n"," 694,\n"," 912,\n"," 1476,\n"," 2040,\n"," 2651,\n"," 2780,\n"," 3920,\n"," 4014,\n"," 4190,\n"," 4437,\n"," 5749,\n"," 5782,\n"," 5838,\n"," 5876,\n"," 6454,\n"," 10,\n"," 2574,\n"," 5489,\n"," 5881]"]},"metadata":{},"execution_count":8}],"source":["selected_participants_data = pd.read_csv('/content/drive/MyDrive/sleep onset datasets/data_test_perfectly_clean_top_20.csv')\n","selected_participants = selected_participants_data['Sbj_ID'].unique().tolist()\n","selected_participants"]},{"cell_type":"markdown","metadata":{"id":"D-Bj-76pVJpA"},"source":["## Define the helper functions"]},{"cell_type":"markdown","metadata":{"id":"HVIDKf7IVJpA"},"source":["### Define fuctions to create custom Dataset and DataLoader\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"NNvTzx_lVJpA","executionInfo":{"status":"ok","timestamp":1693954107793,"user_tz":-60,"elapsed":711,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["# Define the device:\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","#Create a PyTorch dataset\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","class SleepDataset(Dataset):\n","    def __init__(self, sequences, labels):\n","        self.sequences = sequences\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.sequences)\n","\n","    def __getitem__(self, idx):\n","        return torch.from_numpy(self.sequences[idx]).float(), torch.from_numpy(np.array(self.labels[idx])).float()"]},{"cell_type":"markdown","metadata":{"id":"B2QNmhZ7VJpA"},"source":["### Define functions to preprocess the data -> break up into sequences, resample, stratify, etc."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"o_L_STiNVJpB","executionInfo":{"status":"ok","timestamp":1693954107793,"user_tz":-60,"elapsed":8,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["# function to turn dataset from regression to classification\n","def regression_to_classification(dataframe, sleep_onset_threshold, awake_threshold):\n","    ''' This function takes a dataframe with regression labels and turns it into a classification problem '''\n","\n","    # Try classification on 5 minutes before sleep and 20+ minutes before sleep\n","    dataframe_transformed = dataframe[(dataframe['Label'] <= sleep_onset_threshold) | (dataframe['Label'] > awake_threshold)].copy()\n","\n","    # Swap Label columns into 1  (if Label <= 5) and 0\n","    dataframe_transformed['Label'] = dataframe_transformed['Label'].apply(lambda x: 1 if x <= sleep_onset_threshold else 0)\n","\n","    index = [1, 0]\n","    label = [f'{sleep_onset_threshold} minutes', f'{awake_threshold}-30 minutes']\n","    return dataframe_transformed, index, label\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"7EwSJqMPVJpB","executionInfo":{"status":"ok","timestamp":1693954107793,"user_tz":-60,"elapsed":8,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["from sklearn.model_selection import StratifiedShuffleSplit\n","from imblearn.over_sampling import SMOTE\n","from imblearn.under_sampling import RandomUnderSampler\n","\n","def handle_missing_data(data, ifmissing):\n","    if not ifmissing:\n","        assert data.isnull().sum().sum() == 0, \"There are NaNs in the data\"\n","\n","def ss_split_remaining_subjects(subjects, labels, random_seed):\n","        sss_val_test = StratifiedShuffleSplit(n_splits=1, train_size=0.5,\n","                                              test_size=0.5, random_state=random_seed)\n","        for val_idx, test_idx in sss_val_test.split(subjects, labels):\n","            val_subjects = subjects[val_idx]\n","            test_subjects = subjects[test_idx]\n","        return val_subjects, test_subjects\n","\n","\n","def split_data_into_sets(data, train_proportion, if_stratified_sampling, iftest, random_seed):\n","     # Create a list of all unique subjects\n","    subjects = data['Sbj_ID'].unique()\n","\n","    # Randomly select some of the 80% of subjects to be in the training set, 20% to the validation set and 20% in the test set\n","    np.random.seed(random_seed)\n","\n","    val_subjects = []\n","    test_subjects = []\n","    train_subjects = []\n","    if if_stratified_sampling == 1:\n","        # Get representative labels for each subject for stratification\n","        subject_labels = data.groupby('Sbj_ID')['Label'].apply(lambda x: x.value_counts().idxmax())\n","\n","        sss = StratifiedShuffleSplit(n_splits=1, train_size=train_proportion, test_size=1-train_proportion, random_state=random_seed)\n","\n","        for train_idx, temp_idx in sss.split(subjects, subject_labels):\n","            train_subjects = subjects[train_idx]\n","            temp_subjects = subjects[temp_idx]\n","\n","        if iftest == 1:\n","            # Now split the remaining subjects into validation and test sets\n","            sss_val_test = StratifiedShuffleSplit(n_splits=1, train_size=0.5, test_size=0.5, random_state=random_seed)\n","\n","            for val_idx, test_idx in sss_val_test.split(temp_subjects, subject_labels.iloc[temp_idx]):\n","                val_subjects = temp_subjects[val_idx]\n","                test_subjects = temp_subjects[test_idx]\n","        else:\n","            val_subjects = temp_subjects\n","    else:\n","        train_subjects = np.random.choice(subjects, size=int(train_proportion*len(subjects)), replace=False)\n","\n","\n","        if iftest == 1:\n","            val_proportions = (1 - train_proportion)/2\n","            val_subjects = np.random.choice(np.setdiff1d(subjects, train_subjects), size=int(val_proportions*len(subjects)), replace=False)\n","            test_subjects = np.setdiff1d(subjects, np.concatenate((train_subjects, val_subjects)))\n","        else:\n","            val_proportions = 1 - train_proportion\n","            val_subjects = np.setdiff1d(subjects, train_subjects)\n","            test_subjects = []\n","\n","    # Print the number of subjects in each set\n","    print(f'There are {len(train_subjects)} subjects in the training set, {len(val_subjects)} subjects in the validation set and {len(test_subjects)} subjects in the test set')\n","\n","    return train_subjects, val_subjects, test_subjects\n","\n","def assign_data_set(data, train_subjects, val_subjects, test_subjects):\n","    data['Set'] = 'train'\n","    data.loc[data['Sbj_ID'].isin(val_subjects), 'Set'] = 'val'\n","    data.loc[data['Sbj_ID'].isin(test_subjects), 'Set'] = 'test'\n","    return data.sort_values('Sbj_ID')\n","\n","from imblearn.over_sampling import SMOTE\n","from imblearn.under_sampling import RandomUnderSampler\n","import numpy as np\n","\n","def generate_sequences_and_labels(group, window_size, ifoutput_end_points=0):\n","    sequences = []\n","    labels = []\n","\n","    group_features = group.drop(['Sbj_ID', 'Label', 'Set'], axis=1).to_numpy()\n","    ifmissing_column = group['ifmissing_after_imputation'].to_numpy()\n","\n","    # If window size is longer than the group, skip\n","    if window_size > len(group_features):\n","        return sequences, labels\n","\n","    for i in range(len(group_features) - window_size):\n","        if ifmissing_column[i : i + window_size].sum() > 0:\n","            continue\n","\n","        if ifoutput_end_points == 0 and group['Label'].iloc[i : i + window_size].nunique() > 1:\n","            continue\n","\n","        sequences.append(group_features[i : i + window_size])\n","        labels.append(group['Label'].iloc[i])\n","\n","    return sequences, labels\n","\n","def resample_data(sequences, labels, method, window_size, random_seed=42):\n","    if method == 'oversampling':\n","        print('Performing SMOTE oversampling')\n","        smote = SMOTE(random_state=random_seed)\n","        sequences_resampled, labels_resampled = smote.fit_resample(\n","            sequences.reshape(sequences.shape[0], -1), labels)\n","        sequences = sequences_resampled.reshape(-1, window_size, sequences.shape[-1])\n","        labels = labels_resampled\n","    elif method == 'undersampling':\n","        print('Performing undersampling')\n","        rus = RandomUnderSampler(random_state=random_seed)\n","        sequences_resampled, labels_resampled = rus.fit_resample(\n","            sequences.reshape(sequences.shape[0], -1), labels)\n","        sequences = sequences_resampled.reshape(-1, window_size, sequences.shape[-1])\n","        labels = labels_resampled\n","\n","    return sequences, labels\n","\n","def determine_output_end_points(train_proportion, iftest, train_end_points, test_end_points, val_end_points):\n","    if int(train_proportion) == 1:\n","        return train_end_points\n","    else:\n","        if iftest == 1:\n","            return test_end_points\n","        else:\n","            return val_end_points\n","\n","def format_output(train_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels, ifoutputsubjects, train_subjects, val_subjects, test_subjects, ifoutput_end_points, end_points=None, sleep_stages=None):\n","\n","    if ifoutputsubjects:\n","\n","        if ifoutput_end_points == 1:\n","            return train_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels, train_subjects, val_subjects, test_subjects, end_points, sleep_stages\n","        else:\n","            return train_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels, train_subjects, val_subjects, test_subjects\n","\n","    else:\n","        if ifoutput_end_points == 1:\n","            return train_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels, end_points, sleep_stages\n","        else:\n","            return train_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"zcqKZE3pVJpC","executionInfo":{"status":"ok","timestamp":1693954107793,"user_tz":-60,"elapsed":7,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["def create_sliding_windows_new(data, window_size_minutes=0.5, ifmissing=True, random_seed=42, ifoutputsubjects=0,\n","                           train_proportion=0.8, ifoutput_end_points=0, if_stratified_sampling=1,\n","                           resampling_method=None, iftest=1):\n","\n","    handle_missing_data(data, ifmissing)\n","\n","    train_subjects, val_subjects, test_subjects = split_data_into_sets(data, train_proportion, if_stratified_sampling, iftest, random_seed)\n","\n","    data = assign_data_set(data, train_subjects, val_subjects, test_subjects)\n","\n","    # Convert the window size from minutes to 6-second epochs\n","    window_size = int((window_size_minutes * 60) / 6)\n","\n","    train_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels, train_end_points, train_sleep_stages, val_end_points, val_sleep_stages, test_end_points, test_sleep_stages = generate_sequences_and_labels(data, window_size, ifoutput_end_points)\n","\n","    train_sequences, train_labels = resample_data(train_sequences, train_labels, resampling_method, random_seed, window_size)\n","\n","    if ifoutput_end_points:\n","        end_points, sleep_stages = determine_output_end_points(train_proportion, iftest, train_end_points, val_end_points, test_end_points, train_sleep_stages, val_sleep_stages, test_sleep_stages)\n","\n","    return format_output(train_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels, ifoutputsubjects, ifoutput_end_points, train_subjects, val_subjects, test_subjects, end_points, sleep_stages)"]},{"cell_type":"code","execution_count":65,"metadata":{"id":"_yuh7kX4VJpC","executionInfo":{"status":"ok","timestamp":1693955633180,"user_tz":-60,"elapsed":214,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["\n","\n","# Create a function for creating sliding windows of selected length\n","def create_sliding_windows(data,  window_size_minutes=0.5,\n","                           ifmissing = True, random_seed = 42, ifoutputsubjects = 0,\n","                           train_proportion = 0.8, ifoutput_end_points = 0,\n","                           if_stratified_sampling = 1,\n","                           resampling_method = None, iftest = 0, ifvocal = False):\n","\n","    # Assert whether therea are any NaNs in the data\n","    if not ifmissing:\n","        assert data.isnull().sum().sum() == 0, \"There are NaNs in the data\"\n","\n","    # extract 'Old_label' column\n","    labels = data['Old_label'].to_numpy()\n","\n","    # Create a list of all unique subjects\n","    subjects = data['Sbj_ID'].unique()\n","\n","    # Randomly select some of the 80% of subjects to be in the training set, 20% to the validation set and 20% in the test set\n","    np.random.seed(random_seed)\n","\n","    val_subjects = []\n","    test_subjects = []\n","    train_subjects = []\n","    if if_stratified_sampling == 1:\n","        # Get representative labels for each subject for stratification\n","        subject_labels = data.groupby('Sbj_ID')['Label'].apply(lambda x: x.value_counts().idxmax())\n","\n","        sss = StratifiedShuffleSplit(n_splits=1, train_size=train_proportion, test_size=1-train_proportion, random_state=random_seed)\n","\n","        for train_idx, temp_idx in sss.split(subjects, subject_labels):\n","            train_subjects = subjects[train_idx]\n","            temp_subjects = subjects[temp_idx]\n","\n","        if iftest == 1:\n","            # Now split the remaining subjects into validation and test sets\n","            sss_val_test = StratifiedShuffleSplit(n_splits=1, train_size=0.5, test_size=0.5, random_state=random_seed)\n","\n","            for val_idx, test_idx in sss_val_test.split(temp_subjects, subject_labels.iloc[temp_idx]):\n","                val_subjects = temp_subjects[val_idx]\n","                test_subjects = temp_subjects[test_idx]\n","        else:\n","            val_subjects = temp_subjects\n","    else:\n","        train_subjects = np.random.choice(subjects, size=int(train_proportion*len(subjects)), replace=False)\n","\n","\n","        if iftest == 1:\n","            val_proportions = (1 - train_proportion)/2\n","            val_subjects = np.random.choice(np.setdiff1d(subjects, train_subjects), size=int(val_proportions*len(subjects)), replace=False)\n","            test_subjects = np.setdiff1d(subjects, np.concatenate((train_subjects, val_subjects)))\n","        else:\n","            val_proportions = 1 - train_proportion\n","            val_subjects = np.setdiff1d(subjects, train_subjects)\n","            test_subjects = []\n","\n","    # Print the number of subjects in each set\n","    if ifvocal:\n","        print(f'There are {len(train_subjects)} subjects in the training set, {len(val_subjects)} subjects in the validation set and {len(test_subjects)} subjects in the test set')\n","\n","\n","    # Create a new column in the dataframe that indicates whether the subject is in the training set, val set or the test set\n","    data['Set'] = 'train'\n","    data.loc[data['Sbj_ID'].isin(val_subjects), 'Set'] = 'val'\n","    data.loc[data['Sbj_ID'].isin(test_subjects), 'Set'] = 'test'\n","\n","    # Sort your dataframe by Sbj_ID if not already sorted\n","    data = data.sort_values('Sbj_ID')\n","\n","    # Convert the window size from minutes to 6-second epochs\n","    window_size = int((window_size_minutes*60)/6)\n","\n","    # Create empty lists to store your sequences and labels\n","    train_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels = [], [], [], [], [], []\n","    if ifoutput_end_points == 1:\n","        test_end_points, test_sleep_stages, val_end_points, val_sleep_stages, train_end_points, train_sleep_stages = [], [], [], [], [], []\n","\n","    # Group the DataFrame by subject ID and iterate over each group, dividing into training and test sets\n","    for _, group in data.groupby('Sbj_ID'):\n","\n","        group = group.sort_values('Old_label', ascending=False)\n","\n","        # For each group, get the feature columns and convert them into numpy array\n","        if ifoutput_end_points == 1:\n","             group_features = group.drop(['Sbj_ID', 'Label', 'Set', 'ifmissing_after_imputation', 'Old_label', 'ifCleanOnset', 'Time2Sleep', 'SleepStage'], axis=1).to_numpy()\n","             group_old_labels = group['Old_label'].to_numpy()\n","             group_sleep_stages = group['SleepStage'].to_numpy()\n","             group_labels = group['Label'].to_numpy()\n","        else:\n","            group_features = group.drop(['Sbj_ID', 'Label', 'Set', 'ifmissing_after_imputation', 'ifCleanOnset', 'Time2Sleep', 'SleepStage'], axis=1).to_numpy()\n","\n","        ifmissing_column = group['ifmissing_after_imputation'].to_numpy()\n","\n","        train_test = group['Set'].iloc[0]\n","\n","        #if window_size == len(group_features):\n","        #    if train_test == 'train':\n","        #        print('got here')\n","        #        train_sequences.append(group_features)\n","        #        train_labels.append(group['Label'].iloc[0])\n","        #    elif train_test == 'val':\n","        #        val_sequences.append(group_features)\n","        #        val_labels.append(group['Label'].iloc[0])\n","        #    elif train_test == 'test':\n","        #        test_sequences.append(group_features)\n","        #        test_labels.append(group['Label'].iloc[0])\n","        #elif window_size > len(group_features):\n","        #    continue\n","\n","\n","        # Iterate over the group array with a sliding window\n","        for i in range(len(group_features) - window_size):\n","\n","            # If there are any NaNs in the window, skip it\n","\n","            if ifmissing_column[i : i + window_size].sum() > 0:\n","                continue\n","\n","            # Check if the labels are consistent in the window (only if we're not outputting probabilities for the whole duration)\n","            if ifoutput_end_points == 0:\n","                if group['Label'].iloc[i : i + window_size].nunique() > 1:\n","                    continue\n","\n","            if train_test == 'train':\n","                # Append the window data to your sequences\n","                train_sequences.append(group_features[i : i + window_size])\n","                # Append the label corresponding to the end of the window\n","                train_labels.append(group['Label'].iloc[i])\n","\n","                if ifoutput_end_points == 1:\n","                    # Append the starting point of the window to the list of starting points\n","                    train_end_points.append(group_old_labels[i+window_size])\n","                    train_sleep_stages.append(group_sleep_stages[i+window_size])\n","\n","            elif train_test == 'val':\n","                # Append the window data to your sequences\n","                val_sequences.append(group_features[i : i + window_size])\n","\n","                # Append the label corresponding to the end of the window\n","                val_labels.append(group['Label'].iloc[i])\n","\n","                if ifoutput_end_points == 1:\n","                    # Append the starting point of the window to the list of starting points\n","                    val_end_points.append(group_old_labels[i+window_size])\n","                    val_sleep_stages.append(group_sleep_stages[i+window_size])\n","\n","\n","            elif train_test == 'test':\n","\n","                # Append the window data to your sequences\n","                test_sequences.append(group_features[i : i + window_size])\n","\n","                # Append the label corresponding to the end of the window\n","                test_labels.append(group['Label'].iloc[i])\n","\n","                if ifoutput_end_points == 1:\n","                    # Append the starting point of the window to the list of starting points\n","                    test_end_points.append(group_old_labels[i+window_size])\n","                    test_sleep_stages.append(group_sleep_stages[i+window_size])\n","\n","    # Convert the sequences and labels into numpy arrays\n","    train_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels = np.array(train_sequences), np.array(train_labels), np.array(val_sequences), np.array(val_labels), np.array(test_sequences), np.array(test_labels)\n","    if ifoutput_end_points == 1:\n","        train_end_points, train_sleep_stages, val_end_points, val_sleep_stages, test_end_points, test_sleep_stages = np.array(train_end_points), np.array(train_sleep_stages), np.array(val_end_points), np.array(val_sleep_stages), np.array(test_end_points), np.array(test_sleep_stages)\n","\n","\n","    if resampling_method == 'oversampling':\n","        if ifvocal:\n","            print('Performing SMOTE oversampling')\n","        smote = SMOTE(random_state=random_seed)\n","        if ifoutput_end_points == 1:\n","            train_sequences_and_end_points = np.concatenate((train_sequences.reshape(train_sequences.shape[0], -1), train_end_points.reshape(-1, 1)), axis=1)\n","            train_sequences_and_end_points_resampled, train_labels_resampled = smote.fit_resample(train_sequences_and_end_points, train_labels)\n","            train_sequences_resampled = train_sequences_and_end_points_resampled[:, :-1]\n","            train_end_points_resampled = train_sequences_and_end_points_resampled[:, -1]\n","            train_sequences = train_sequences_resampled.reshape(-1, window_size, train_sequences.shape[-1])\n","            train_end_points = train_end_points_resampled\n","        else:\n","            train_sequences_resampled, train_labels_resampled = smote.fit_resample(train_sequences.reshape(train_sequences.shape[0], -1), train_labels)\n","        train_sequences = train_sequences_resampled.reshape(-1, window_size, train_sequences.shape[-1])\n","        train_labels = train_labels_resampled\n","    elif resampling_method == 'undersampling':\n","        if ifvocal:\n","            print('Performing undersampling')\n","        rus = RandomUnderSampler(random_state=random_seed)\n","        if ifoutput_end_points == 1:\n","            train_sequences_and_end_points = np.concatenate((train_sequences.reshape(train_sequences.shape[0], -1), train_end_points.reshape(-1, 1)), axis=1)\n","            train_sequences_and_end_points_resampled, train_labels_resampled = rus.fit_resample(train_sequences_and_end_points, train_labels)\n","            train_sequences_resampled = train_sequences_and_end_points_resampled[:, :-1]\n","            train_end_points_resampled = train_sequences_and_end_points_resampled[:, -1]\n","            train_sequences = train_sequences_resampled.reshape(-1, window_size, train_sequences.shape[-1])\n","            train_end_points = train_end_points_resampled\n","        else:\n","            train_sequences_resampled, train_labels_resampled = rus.fit_resample(train_sequences.reshape(train_sequences.shape[0], -1), train_labels)\n","        train_sequences = train_sequences_resampled.reshape(-1, window_size, train_sequences.shape[-1])\n","        train_labels = train_labels_resampled\n","    elif resampling_method is None:\n","        pass\n","\n","    if ifoutput_end_points == 1:\n","        if int(train_proportion) == 1:\n","\n","            end_points = train_end_points\n","            sleep_stages = train_sleep_stages\n","            if ifvocal:\n","                print('end_points shape', end_points.shape)\n","\n","        else:\n","            if iftest == 1:\n","                end_points = test_end_points\n","                sleep_stages = test_sleep_stages\n","            else:\n","                end_points = val_end_points\n","                sleep_stages = val_sleep_stages\n","\n","    del data\n","    gc.collect()\n","\n","\n","    if ifoutputsubjects:\n","        if ifoutput_end_points == 1:\n","            return train_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels, train_subjects, val_subjects, test_subjects, end_points, sleep_stages\n","        else:\n","            return train_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels, train_subjects, val_subjects, test_subjects\n","    else:\n","        if ifoutput_end_points == 1:\n","            return train_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels, end_points, sleep_stages\n","        else:\n","            return train_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels\n","\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"iebM535UVJpD","executionInfo":{"status":"ok","timestamp":1693954107794,"user_tz":-60,"elapsed":8,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["import numpy as np\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","from sklearn.ensemble import RandomForestRegressor\n","\n","# Create a function to impute the missing values in the dataset\n","def impute_missing_values (dataframe, method = 'LOCF', limit = 20):\n","\n","    # create a missing mask (column) fthat would indicate whether the values in any of the columns of mydata are missing\n","    # 1 indicates missing, 0 indicates not missing\n","    missing_mask = dataframe.isnull().sum(axis=1).astype(bool).astype(int)\n","    dataframe_imputed = dataframe.copy()\n","    del dataframe\n","    dataframe_imputed = pd.concat([dataframe_imputed, missing_mask.rename('ifmissing')], axis=1)\n","\n","    if method == 'LOCF':\n","\n","        dataframe_imputed.fillna(method='ffill', inplace=True, limit = limit)\n","\n","        # fill the rest with backward fill\n","        #dataframe_imputed.fillna(method='bfill', inplace=True, limit = limit)\n","\n","    elif method =='NOCB':\n","\n","        dataframe_imputed.fillna(method='bfill', inplace=True, limit = limit)\n","\n","        # fill the rest with forward fill\n","        #dataframe_imputed.fillna(method='ffill', inplace=True, limit = limit)\n","\n","    elif method == 'linear interpolation':\n","\n","        dataframe_imputed.interpolate(method='linear', inplace=True, limit = limit)\n","\n","    elif method == 'quadratic interpolation':\n","\n","        dataframe_imputed.interpolate(method='quadratic', inplace=True, limit = limit)\n","\n","    elif method == 'mean':\n","\n","        dataframe_imputed.fillna(dataframe.mean(), inplace=True, limit = limit)\n","\n","    elif method == 'median':\n","\n","        dataframe_imputed.fillna(dataframe.median(), inplace=True, limit = limit)\n","\n","    elif method == 'MICE':\n","\n","        # Define an imputer\n","        imp = IterativeImputer(estimator=RandomForestRegressor(n_estimators=10, random_state=0),\n","                            missing_values=np.nan,\n","                            sample_posterior=False,\n","                            max_iter=10,\n","                            random_state=0,\n","                            verbose=2)\n","\n","\n","        columns = dataframe_imputed.columns\n","\n","        # Apply the imputer\n","        df_imputed = imp.fit_transform(dataframe_imputed)\n","\n","        # Convert back to DataFrame\n","        dataframe_imputed = pd.DataFrame(df_imputed, columns=columns)\n","\n","    elif method == 'None':\n","        dataframe_imputed = dataframe_imputed\n","\n","    missing_mask_new = dataframe_imputed.isnull().sum(axis=1).astype(bool).astype(int)\n","\n","    dataframe_imputed = pd.concat([dataframe_imputed, missing_mask_new.rename('ifmissing_after_imputation')], axis=1)\n","\n","    return dataframe_imputed\n"]},{"cell_type":"markdown","metadata":{"id":"_lpQFlysVJpD"},"source":["### Define the model architectures"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"z-gUwVY8VJpD","executionInfo":{"status":"ok","timestamp":1693954107794,"user_tz":-60,"elapsed":8,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class SleepOnsetRNNClassifier(nn.Module):\n","\n","    def __init__(self, input_size, hidden_size, num_layers, output_size=1, dropout=0.0, l2=0.0):\n","        super(SleepOnsetRNNClassifier, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.l2 = l2\n","\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","\n","        out, _ = self.lstm(x, (h0, c0))\n","        out = self.fc(out[:, -1, :])\n","       #prob = self.sigmoid(out)\n","        return out\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"792nzIN_VJpD","executionInfo":{"status":"ok","timestamp":1693954107794,"user_tz":-60,"elapsed":7,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["class PretrainedModelForRegression(nn.Module):\n","    def __init__(self, original_model):\n","        super(PretrainedModelForRegression, self).__init__()\n","\n","        # Extract feature parts from the original model\n","        self.features = nn.Sequential(\n","            original_model.lstm,\n","            # You can add more layers from the original model here if needed\n","        )\n","\n","        # Regression head\n","        self.regression_head = nn.Linear(original_model.hidden_size, 1)\n","\n","    def forward(self, x):\n","        # Note: Here we're not initializing h0 and c0 as the original model does.\n","        # This means they will default to 0s.\n","        out, _ = self.features(x)\n","        out = self.regression_head(out[:, -1, :])\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"S6bXm4MTVJpD"},"source":["### Define the training and evaluation functions for classification"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"ZMxVzbhDVJpD","executionInfo":{"status":"ok","timestamp":1693954107794,"user_tz":-60,"elapsed":7,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["from sklearn.metrics import f1_score\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","# Train function\n","def train_classification(model, train_loader, val_loader, num_epochs,\n","                        criterion, optimizer, device, index,\n","                        threshold=0.5, ifprobabilities=False, ifplot = False):\n","\n","    train_losses = []\n","    val_losses = []\n","    sigmoid_function = torch.nn.Sigmoid()  # define a sigmoid function\n","\n","    for epoch in range(num_epochs):\n","\n","        # Train the model\n","        model.train()\n","        train_predictions, train_actuals = [], []\n","        for i, (inputs, labels_and_endpoints) in enumerate(train_loader):\n","            inputs = inputs.to(device)\n","            labels = labels_and_endpoints[:, 0].to(device)\n","\n","\n","            # Forward pass\n","            outputs = model(inputs)\n","            #print(outputs)\n","            #print(labels)\n","            outputs_detach = outputs.detach()\n","            #print(outputs_detach)\n","            probabilities = sigmoid_function(outputs_detach).cpu().numpy() # convert logits to probabilities\n","            #print(probabilities)\n","\n","            loss = criterion(outputs, labels.view(-1, 1))\n","\n","            # Backward and optimize\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            train_actuals.extend(labels.cpu().numpy())\n","            train_predictions.extend((probabilities > threshold).astype(int).flatten())\n","\n","\n","        report = classification_report(train_actuals, train_predictions)\n","        if ifplot:\n","            print('-'*100)\n","            print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item()}')\n","            print('-'*100)\n","            print('Training Report:')\n","            print(report)\n","            print('-'*100)\n","\n","        # Validate the model\n","        model.eval()\n","        with torch.no_grad():\n","            val_loss = 0\n","            predictions, actuals, prediction_probabilities = [], [], []\n","            for inputs, labels_and_endpoints in val_loader:\n","                inputs = inputs.to(device)\n","                labels = labels_and_endpoints[:, 0].to(device)\n","\n","                logits = model(inputs)  # these are logits now\n","                val_loss += criterion(logits, labels.view(-1, 1)).item()\n","\n","                probabilities = sigmoid_function(logits).cpu().numpy()  # convert logits to probabilities\n","\n","                predictions.extend((probabilities > threshold).astype(int).flatten())\n","\n","\n","                if ifprobabilities:\n","                    prediction_probabilities.extend(probabilities)\n","\n","                actuals.extend(labels.cpu().numpy())\n","\n","\n","            # Calculate metrics\n","            f1_scores = f1_score(actuals, predictions, average='weighted', labels = index)\n","            if ifplot:\n","                print('-'*100)\n","                print(f'Validation Loss: {val_loss/len(val_loader)}')\n","                print('F1-Scores for each class:')\n","                print(f1_scores)\n","\n","                # Calculate metrics\n","                report = classification_report(actuals, predictions)\n","\n","                print('Validation Report:')\n","                print(report)\n","\n","\n","        # Save the model training and validation losses for plotting\n","        train_losses.append(loss.item())\n","        val_losses.append(val_loss/len(val_loader))\n","\n","\n","    del train_predictions, train_actuals\n","\n","    if ifprobabilities:\n","        return train_losses, val_losses, model, optimizer, predictions, actuals, prediction_probabilities\n","    else:\n","        return train_losses, val_losses, model, optimizer, predictions, actuals\n","\n","\n","# Test function\n","def test_classification(model, test_loader, device, criterion, index, threshold=0.5, ifprobabilities=False, ifplot = False):\n","    model.eval()\n","    predictions, actuals, prediction_probabilities = [], [], []\n","\n","    sigmoid_function = torch.nn.Sigmoid()  # define a sigmoid function\n","\n","    with torch.no_grad():\n","        test_loss = 0\n","        for inputs, labels_and_endpoints in test_loader:\n","            inputs = inputs.to(device)\n","            labels = labels_and_endpoints[:, 0].to(device)\n","\n","            logits = model(inputs)  # these are logits now\n","            test_loss += criterion(logits, labels.view(-1, 1)).item()\n","\n","            probabilities = sigmoid_function(logits).cpu().numpy()  # convert logits to probabilities\n","\n","            predictions.extend((probabilities > threshold).astype(int).flatten())\n","            if ifprobabilities:\n","                prediction_probabilities.extend(probabilities)\n","            actuals.extend(labels.cpu().numpy())\n","        if ifplot:\n","            print(f'Test Loss: {test_loss/len(test_loader)}')\n","\n","    # Calculate metrics\n","    f1_scores = f1_score(actuals, predictions, average=None, labels=index)\n","\n","\n","    if ifplot:\n","        print('F1-Scores for each class:')\n","        print(f1_scores)\n","\n","    # Calculate metrics\n","    report = classification_report(actuals, predictions)\n","\n","    if ifprobabilities:\n","        return predictions, actuals, prediction_probabilities\n","    else:\n","        return predictions, actuals"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"nyQKkVpbVJpD","executionInfo":{"status":"ok","timestamp":1693954107795,"user_tz":-60,"elapsed":8,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["\n","def get_timeline(mydata_no_nan):\n","\n","    # assert that the data has no NaNs\n","    assert mydata_no_nan.isnull().sum().sum() == 0, 'There are NaNs in the data'\n","\n","\n","    # Create X and y from dataframes to use in scikit-learn (drop Label, SleepStage, Sbj_ID and ifCleanOnset columns)\n","    X_all = mydata_no_nan.drop(['Label', 'Old_label', 'SleepStage', 'Sbj_ID', 'ifCleanOnset', 'Time2Sleep'], axis=1)\n","    y_all = mydata_no_nan['Label']\n","    y_old_all = mydata_no_nan['Old_label']\n","    y_sleepstage_all = mydata_no_nan['SleepStage']\n","\n","    del mydata_no_nan\n","\n","\n","\n","    return X_all, y_all, y_old_all, y_sleepstage_all"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"qvur9v4NVJpD","executionInfo":{"status":"ok","timestamp":1693954107795,"user_tz":-60,"elapsed":8,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","import pandas as pd\n","import os\n","from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n","                             roc_curve, roc_auc_score, auc)\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.metrics import f1_score\n","\n","def evaluate_classification(true_labels, predictions, prediction_probabilities,\n","                            index=['10 minutes before sleep', 'Awake'],\n","                            columns=['10 minutes before sleep', 'Awake'],\n","                            save_path='confusion_matrix.png', ifsaveplots = False,\n","                            output_path=None, iftest = False, ifplot = True, classes = None, ifvocal = True):\n","\n","    # Evaluate the performance of the model\n","    accuracy = accuracy_score(true_labels, predictions)\n","    if iftest:\n","        if ifvocal:\n","            print(f'Accuracy: {accuracy}')\n","\n","    # Classification report\n","    report = classification_report(true_labels, predictions)\n","    if iftest:\n","        if ifvocal:\n","            print(report)\n","\n","    # Decide on the filename based on iftest\n","    report_filename = 'test_classification_report.txt' if iftest else 'crossval_classification_report.txt'\n","\n","     # Dictionary to store all metrics\n","    metrics_dict = {}\n","    metrics_dict['accuracy'] = accuracy\n","    # Classification report\n","    precision, recall, _,  _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n","    f1 = f1_score(true_labels, predictions, average='weighted')\n","    f1_macro = f1_score(true_labels, predictions, average='macro')\n","    f1_pos, f1_neg = f1_score(true_labels, predictions, average=None)\n","\n","\n","    metrics_dict['precision'] = precision\n","    metrics_dict['recall'] = recall\n","    metrics_dict['f1_weighted'] = f1\n","    metrics_dict['f1_macro'] = f1_macro\n","    metrics_dict['f1_pre_sleep'] = f1_pos\n","    metrics_dict['f1_awake'] = f1_neg\n","\n","\n","\n","    if iftest:\n","        if ifvocal:\n","            print('F1 pre-sleep: ', f1_pos)\n","            print('F1 awake: ', f1_neg)\n","\n","\n","\n","    # Save classification report to a .txt file\n","    if ifsaveplots:\n","        if output_path:\n","            report_filename = os.path.join(output_path, report_filename)\n","        with open(report_filename, 'w') as f:\n","            f.write(report)\n","    # Plot the confusion matrix\n","\n","    # Include the names of the classes in the confusion matrix\n","    cm = confusion_matrix(true_labels, predictions, normalize='true', labels=index)\n","    cm = pd.DataFrame(cm, index=index, columns=columns)\n","\n","    # Rename the columns and rows to the names of the classes\n","    cm.index = columns\n","    plt.figure(figsize=(5.5, 4))\n","    sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues')\n","\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.title('Confusion Matrix for times before sleep')\n","\n","    if iftest:\n","        saving_prefix = 'test'\n","    else:\n","        saving_prefix = 'crossval'\n","\n","    save_path = f'{saving_prefix}_{save_path}'\n","\n","    if ifsaveplots:\n","        if output_path:\n","            save_path = os.path.join(output_path, save_path)\n","        # Save the confusion matrix to a file\n","        plt.savefig(save_path)\n","\n","    # Display the confusion matrix\n","    if ifplot:\n","        plt.show()\n","    plt.close()\n","\n","    roc_path = f'{saving_prefix}_roc_curve.png'\n","\n","    # ROC and AUC\n","\n","    # if array is 2D\n","    if type(prediction_probabilities) == list:\n","        prediction_probabilities = np.array(prediction_probabilities)\n","        prediction_probabilities = prediction_probabilities.ravel()\n","\n","\n","    fpr, tpr, _ = roc_curve(true_labels, prediction_probabilities)\n","    roc_auc = auc(fpr, tpr)\n","    #metrics_dict['roc_curve'] = (fpr, tpr)\n","    metrics_dict['auc'] = roc_auc\n","\n","    plt.figure(figsize=(7, 7))\n","    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n","    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC)')\n","    plt.legend(loc=\"lower right\")\n","\n","\n","    if ifsaveplots:\n","        if output_path:\n","            roc_path = os.path.join(output_path, roc_path)\n","        plt.savefig(roc_path)\n","\n","    if ifplot:\n","        plt.show()\n","    plt.close()\n","\n","    del true_labels, predictions, cm, report, f1, f1_macro, f1_pos, f1_neg, roc_auc\n","\n","\n","\n","    return metrics_dict\n"]},{"cell_type":"markdown","metadata":{"id":"9mFIiM_vVJpE"},"source":["### Define the functions for probability analysis"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"CLYgxXP6VJpE","executionInfo":{"status":"ok","timestamp":1693954107795,"user_tz":-60,"elapsed":8,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","\n","def preprocess_data(y_pred, y_true, timeline, sleep_stage):\n","    y_pred = np.array(y_pred)\n","    y_true = np.array(y_true)\n","    timeline = np.array(timeline)\n","    sleep_stage = np.array(sleep_stage)\n","    timeline = np.round(timeline, 1)\n","    return y_pred, y_true, timeline, sleep_stage\n","\n","def get_indices_for_unique_values(timeline):\n","    unique_values = np.unique(timeline)\n","    unique_values_sorted = np.sort(unique_values)\n","    indices = {}\n","    for value in unique_values:\n","        indices[value] = np.where(timeline == value)[0]\n","    return dict(sorted(indices.items())), unique_values_sorted\n","\n","def update_indices_with_missing_values(indices, rnn_window):\n","    timegrid = np.arange(0, 30.1 - rnn_window, 0.1)\n","    timegrid = np.round(timegrid, 1)\n","    missing_values = np.setdiff1d(timegrid, list(indices.keys()))\n","    for value in missing_values:\n","        indices[value] = np.nan\n","    return dict(sorted(indices.items())), timegrid\n","\n","def sort_predictions_by_time(y_pred, indices, timegrid):\n","    y_pred_sorted = [np.mean(y_pred[indices[value]]) if not np.isnan(indices[value]).any() else np.nan for value in timegrid]\n","    y_pred_std = [np.std(y_pred[indices[value]]) if not np.isnan(indices[value]).any() else np.nan for value in timegrid]\n","    return y_pred_sorted, y_pred_std\n","\n","def downsample_data(data, window_for_averaging):\n","    return [np.nanmean(data[i:i+window_for_averaging]) for i in range(0, len(data), window_for_averaging)]\n","\n","def plot_data(y_pred_mean, y_pred_std, sleep_stage_mean, sleep_stage_std, timegrid, window_for_averaging, rnn_window, pre_sleep_window=None, awake_window=None, ifsaveplots=False, output_path=None, ifplot = False, ifsubjecttest = False, change_points_times = None):\n","\n","    # convert to numpy arrays to avoid errors\n","    y_pred_mean = np.array(y_pred_mean)\n","    y_pred_std = np.array(y_pred_std)\n","    sleep_stage_mean = np.array(sleep_stage_mean)\n","    sleep_stage_std = np.array(sleep_stage_std)\n","\n","    def plot_graph(include_std, include_crossings, filename_suffix):\n","        plt.figure(figsize=(14, 7))\n","        plt.xlabel('Time to sleep onset (min)', fontsize=14, fontweight='bold')\n","        plt.ylabel('Probability', fontsize=14, fontweight='bold')\n","        xticks = np.arange(0, len(y_pred_mean), 20 // window_for_averaging)\n","        plt.xticks(xticks, timegrid[xticks * window_for_averaging], fontsize=12, fontweight='bold')\n","        plt.yticks(fontsize=12, fontweight='bold')\n","\n","        plt.plot(y_pred_mean, label='Predicted Probability' if ifsubjecttest else 'Mean Predicted Probability', color='blue')\n","        if include_std:\n","            plt.fill_between(np.arange(len(y_pred_mean)), y_pred_mean - y_pred_std, y_pred_mean + y_pred_std, color='blue', alpha=0.1)\n","\n","        plt.plot(sleep_stage_mean, label='Sleep Stages' if ifsubjecttest else 'Mean Sleep Stages', color='purple', linestyle='--', linewidth=2.5)\n","        if include_std:\n","            plt.fill_between(np.arange(len(sleep_stage_mean)), sleep_stage_mean - sleep_stage_std, sleep_stage_mean + sleep_stage_std, color='purple', alpha=0.1)\n","\n","        plt.hlines(0.5, linestyle='--', color='r', xmin=0, xmax=len(y_pred_mean), label='Threshold for classifying as pre-sleep onset', linewidth=2.5)\n","        if include_crossings and change_points_times is not None:\n","            plt.vlines(change_points_times, ymin=-0.1, ymax=1.1, color='seagreen', label='Detected change points', linewidth=2.5, linestyle='--')\n","\n","        if awake_window:\n","            awake_window_probas = int((awake_window - rnn_window) * 10 // window_for_averaging)\n","            plt.axvspan(awake_window_probas, len(y_pred_mean), facecolor='g', alpha=0.2, label='Awake window')\n","        if pre_sleep_window:\n","            pre_sleep_window_probas = int((pre_sleep_window - rnn_window) * 10 // window_for_averaging)\n","            plt.axvspan(pre_sleep_window_probas, 0, facecolor='r', alpha=0.2, label='Pre-sleep window')\n","\n","        plt.ylim(-0.1, 1.1)\n","        plt.legend(loc='upper right', fontsize=14)\n","        filename = f\"sleep_probabilities_{filename_suffix}_averaged_over_{window_for_averaging}.jpg\"\n","        if ifsaveplots:\n","            if output_path:\n","                filename = os.path.join(output_path, filename)\n","            plt.savefig(filename, format='jpg', dpi=200)\n","        if ifplot:\n","            plt.show()\n","        plt.close()\n","\n","    plot_graph(False, False, \"without_std\")\n","    plot_graph(True, False, \"with_std\")\n","    plot_graph(False, True, \"without_std_with_crossings\")\n","    plot_graph(True, True, \"with_std_with_crossings\")\n","\n","\n","\n","def calculate_metrics(y_pred, y_true, timegrid, indices):\n","    correct = y_true == y_pred\n","    accuracies = [np.mean(correct[indices[value]]) if not np.isnan(indices[value]).any() else np.nan for value in timegrid]\n","\n","    prediction_dict_by_time = {value: y_pred[indices[value]] if indices[value] is not np.nan else np.nan for value in timegrid}\n","    true_values_dict_by_time = {value: y_true[indices[value]] if indices[value] is not np.nan else np.nan for value in timegrid}\n","\n","    tp = {value: np.sum(np.logical_and(prediction_dict_by_time[value] == 1, true_values_dict_by_time[value] == 1)) if indices[value] is not np.nan else np.nan for value in timegrid}\n","    tn = {value: np.sum(np.logical_and(prediction_dict_by_time[value] == 0, true_values_dict_by_time[value] == 0)) if indices[value] is not np.nan else np.nan for value in timegrid}\n","    fp = {value: np.sum(np.logical_and(prediction_dict_by_time[value] == 1, true_values_dict_by_time[value] == 0)) if indices[value] is not np.nan else np.nan for value in timegrid}\n","    fn = {value: np.sum(np.logical_and(prediction_dict_by_time[value] == 0, true_values_dict_by_time[value] == 1)) if indices[value] is not np.nan else np.nan for value in timegrid}\n","\n","    recall = [tp[value] / (tp[value] + fn[value]) for value in timegrid]\n","    precision = [tp[value] / (tp[value] + fp[value]) for value in timegrid]\n","    f1 = [2 * (precision[i] * recall[i]) / (precision[i] + recall[i]) if precision[i] + recall[i] != 0 else np.nan for i in range(len(precision))]\n","\n","    return accuracies, recall, precision, f1\n","\n","def plot_metric(metric_values, metric_name, color, timegrid, window_for_averaging, rnn_window,\n","                pre_sleep_window=None, awake_window=None, ifsaveplots=False, output_path=None,\n","                ifplot = False):\n","\n","    plt.figure(figsize=(20,10))\n","    plt.xlabel('Minutes', fontsize=14)\n","    xticks = np.arange(0, len(metric_values), 10//window_for_averaging)\n","    plt.xticks(xticks, timegrid[xticks*window_for_averaging], fontsize=12)\n","    plt.yticks(fontsize=12)\n","\n","    plt.plot(metric_values, label=metric_name, color=color)\n","    #plt.hlines(0.5, linestyle='--', color='r', label='Threshold for classifying as pre-sleep onset', xmin=0, xmax=len(metric_values))\n","\n","    if awake_window is not None:\n","        awake_window_probas = int((awake_window - rnn_window)* 10//window_for_averaging)\n","        plt.axvspan(awake_window_probas, len(metric_values), facecolor='g', alpha=0.2, label='Awake window')\n","\n","    if pre_sleep_window is not None:\n","        pre_sleep_window_probas = int((pre_sleep_window - rnn_window) * 10//window_for_averaging)\n","        plt.axvspan(pre_sleep_window_probas, 0, facecolor='r', alpha=0.2, label='Pre-sleep window')\n","\n","    plt.ylim(-0.1, 1.1)\n","    plt.legend(loc='upper right', fontsize=12)\n","    if ifsaveplots:\n","        filename = f\"{metric_name}_metric_averaged_over_{window_for_averaging}.jpg\"\n","        if output_path:\n","            filename = os.path.join(output_path, filename)\n","        plt.savefig(filename, format='jpg', dpi=300)\n","\n","    if ifplot:\n","        plt.show()\n","    plt.close()\n","\n","\n","def plot_metrics(accuracies, #recall, precision, f1,\n","                 timegrid, window_for_averaging, rnn_window,\n","                 pre_sleep_window=None, awake_window=None,\n","                 ifsaveplots=False, output_path=None, ifplot = False):\n","\n","    plot_metric(accuracies, 'Accuracy', 'blue', timegrid, window_for_averaging, rnn_window,\n","                pre_sleep_window, awake_window, ifsaveplots, output_path, ifplot = ifplot)\n","\n","    #plot_metric(recall, 'Recall', 'green', timegrid, window_for_averaging, rnn_window,\n","                #pre_sleep_window, awake_window, ifsaveplots, output_path, ifplot = ifplot)\n","\n","    #plot_metric(precision, 'Precision', 'purple', timegrid, window_for_averaging, rnn_window,\n","                #pre_sleep_window, awake_window, ifsaveplots, output_path, ifplot = ifplot)\n","\n","    #plot_metric(f1, 'F1', 'red', timegrid, window_for_averaging, rnn_window,\n","                #pre_sleep_window, awake_window, ifsaveplots, output_path, ifplot = ifplot)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"myH7suxKVJpE"},"source":["### Define functions for probability change point analysis"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"h-NwT5xaVJpE","executionInfo":{"status":"ok","timestamp":1693954107795,"user_tz":-60,"elapsed":7,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["def nan_convolve(array, window, mode='valid'):\n","    if mode != 'valid':\n","        raise ValueError(\"Only 'valid' mode is currently supported\")\n","\n","    output = np.zeros(len(array) - len(window) + 1)\n","    for i in range(len(output)):\n","        current_slice = array[i: i + len(window)]\n","        valid_data = current_slice[~np.isnan(current_slice)]\n","        valid_window = window[:len(valid_data)]\n","        output[i] = np.sum(valid_data * valid_window)\n","    return output"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"NqTTQDPyVJpE","executionInfo":{"status":"ok","timestamp":1693954107795,"user_tz":-60,"elapsed":7,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["def threshold_consistency_change_point(data, times, threshold = 0.5, consistency_length=None, window_size=20, proportion=0.7, stay_above_proportion=0.8):\n","    #assert len(data) == len(times), \"Data and times must have the same length\"\n","\n","    if type(data) == list:\n","        data = np.array(data)\n","    if type(times) == list:\n","        times = np.array(times)\n","\n","\n","    data = data[::-1]\n","\n","    above = data > threshold\n","\n","\n","    change_points_consistent = np.array([])  # Initialize here\n","    # reverse data\n","\n","    #print(data)\n","    #print(times)\n","\n","    # Check for consistent crossing\n","    if consistency_length:\n","        consistent_above = nan_convolve(above, np.ones(consistency_length), 'valid') == consistency_length\n","        change_points_consistent = np.where(consistent_above)[0] + consistency_length - 1\n","\n","    # Check for proportion in a running window\n","    if not consistency_length or (window_size and proportion):\n","        within_window = nan_convolve(above, np.ones(window_size), 'valid') / window_size\n","        change_points_window = np.where(within_window > proportion)[0] + window_size - 1\n","        change_points_indices = np.sort(np.unique(np.concatenate([change_points_consistent, change_points_window])))\n","    else:\n","        change_points_indices = change_points_consistent\n","\n","    old_indices = change_points_indices.copy()\n","\n","    final_change_points = np.array([])  # Initialize here\n","    last_index = None\n","\n","\n","    # change to integer\n","    change_points_indices = change_points_indices.astype(int)\n","    #print(\"change_points_indices: \", change_points_indices)\n","\n","    for index in change_points_indices:\n","        #print(\"index: \", index)\n","        preceding_data = data[:index]\n","        # calculate how many values are above the threshold (>threshold) while omitting nans\n","        percentage =  np.nansum(preceding_data > threshold)/ np.sum(~np.isnan(preceding_data))\n","       #print(\"percentage: \", percentage)\n","        if percentage > stay_above_proportion:\n","           #print('Delete index: ', index)\n","            # delete the last entry from the final change points\n","            if len(final_change_points) > 0:\n","                final_change_points = np.delete(final_change_points, -1)\n","\n","            change_points_indices = np.delete(change_points_indices, np.where(change_points_indices == index)[0])\n","            #print(\"change_points_indices after deletion: \", change_points_indices)\n","            # add index to final change points\n","\n","            final_change_points = np.append(final_change_points, index)\n","            #print(\"final_change_points after deletion: \", final_change_points)\n","        else:\n","            #print('index that broke consistency: ', index)\n","            # add the remaining change points (change_points_indices) to the final change points\n","            final_change_points = np.concatenate([final_change_points, change_points_indices])\n","            #print(\"final_change_points after concatenation: \", final_change_points)\n","            break\n","\n","\n","    if len(final_change_points) == 0 and len(old_indices) > 0:\n","        final_change_points = np.array([old_indices[-1]])\n","\n","    # Ensure all indices are integers\n","    final_change_points = final_change_points.astype(int)\n","\n","    # Convert indices to associated times\n","    change_points_times = times[final_change_points].tolist()\n","\n","    # Check if the last timepoint is the largest one in the original time grid\n","    if final_change_points is not None and len(final_change_points) > 0:\n","        # the last index of data that is not nan\n","        last_index = np.where(~np.isnan(data))[0][-1]\n","        if final_change_points[0] == last_index:\n","            return np.array([]), np.array([])\n","        elif change_points_times[0] > 28:\n","           return np.array([]), np.array([])\n","\n","    del change_points_indices, change_points_consistent, change_points_window\n","\n","    return final_change_points, change_points_times\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"D62QL14zVJpE"},"source":["### Define the loop for probability analysis"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"Gpjyjq1FVJpE","executionInfo":{"status":"ok","timestamp":1693954107796,"user_tz":-60,"elapsed":8,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["def check_accuracy_timeline(y_pred, y_true, timeline, sleep_stage, window_for_averaging=1,\n","                            if_proba=False, classes=None, pre_sleep_window=None,\n","                            awake_window=None, rnn_window=None, ifsaveplots=False, output_path=None,\n","                            ifplot = False, ifsubjecttest = False, ifcrossings = False):\n","    y_pred, y_true, timeline, sleep_stage = preprocess_data(y_pred, y_true, timeline, sleep_stage)\n","\n","    indices, unique_values_sorted = get_indices_for_unique_values(timeline)\n","    indices, timegrid = update_indices_with_missing_values(indices, rnn_window)\n","\n","    if if_proba:\n","        if classes is not None:\n","            class_idx = np.where(classes != 'Awake')[0][0]\n","\n","\n","            y_pred = y_pred[:,class_idx]\n","\n","        y_pred_mean, y_pred_std = sort_predictions_by_time(y_pred, indices, timegrid)\n","        sleep_stage_mean, sleep_stage_std = sort_predictions_by_time(sleep_stage, indices, timegrid)\n","\n","\n","        if window_for_averaging > 1:\n","            y_pred_mean = downsample_data(y_pred_mean, window_for_averaging)\n","            y_pred_std = downsample_data(y_pred_std, window_for_averaging)\n","            sleep_stage_mean= downsample_data(sleep_stage_mean, window_for_averaging)\n","            sleep_stage_std = downsample_data(sleep_stage_std, window_for_averaging)\n","            downsampled_timegrid = downsample_data(timegrid, window_for_averaging)\n","\n","\n","        change_points_times = np.array([])\n","        final_change_points = np.array([])\n","\n","        if ifcrossings:\n","            window_size = int(20/window_for_averaging)\n","            if window_for_averaging > 1:\n","                dict_pred_time = {downsampled_timegrid[i]: y_pred_mean[i] for i in range(len(y_pred_mean))}\n","                # sort in descending order according to keys\n","                sorted_dict_pred_time = dict(sorted(dict_pred_time.items(), reverse=True))\n","                # get the values of sorted dictionary\n","                y_probas_sorted = list(sorted_dict_pred_time.values())\n","\n","\n","                final_change_points, change_points_times = threshold_consistency_change_point(data = y_probas_sorted,\n","                                                                        times = downsampled_timegrid,\n","                                                                        threshold=0.5,\n","                                                                        window_size=window_size, proportion=0.8, stay_above_proportion=0.6)\n","            else:\n","                dict_pred_time = {timegrid[i]: y_pred_mean[i] for i in range(len(y_pred_mean))}\n","                # sort in descending order according to keys\n","                sorted_dict_pred_time = dict(sorted(dict_pred_time.items(), reverse=True))\n","                # get the values of sorted dictionary\n","                y_probas_sorted = list(sorted_dict_pred_time.values())\n","\n","                final_change_points, change_points_times = threshold_consistency_change_point(data = y_probas_sorted,\n","                                                                        times = timegrid,\n","                                                                        threshold=0.5,\n","                                                                        window_size=window_size, proportion=0.8, stay_above_proportion=0.6)\n","\n","            num_change_points = len(final_change_points)\n","            if num_change_points == 1:\n","                time_of_prediction = change_points_times[0]\n","            else:\n","                time_of_prediction = np.nan\n","\n","\n","        plot_data(y_pred_mean, y_pred_std, sleep_stage_mean, sleep_stage_std,\n","                  timegrid, window_for_averaging, rnn_window,\n","                  pre_sleep_window, awake_window, ifsaveplots,\n","                  output_path, ifplot = ifplot, ifsubjecttest = ifsubjecttest,\n","                   change_points_times = final_change_points)\n","\n","        if window_for_averaging > 1:\n","            if ifcrossings:\n","\n","                del  dict_pred_time, sorted_dict_pred_time, y_probas_sorted, final_change_points, change_points_times\n","\n","                return num_change_points, time_of_prediction\n","            else:\n","\n","                dict_pred_time = {downsampled_timegrid[i]: y_pred_mean[i] for i in range(len(y_pred_mean))}\n","\n","                del downsampled_timegrid\n","\n","                return dict_pred_time\n","        elif ifcrossings:\n","            del dict_pred_time, sorted_dict_pred_time, y_probas_sorted, final_change_points, change_points_times\n","            return num_change_points, time_of_prediction\n","        else:\n","            del timegrid\n","            return y_pred_mean\n","\n","    else:\n","        accuracies, recall, precision, f1 = calculate_metrics(y_pred, y_true, timegrid, indices)\n","        if window_for_averaging > 1:\n","            accuracies = downsample_data(accuracies, window_for_averaging)\n","            #recall = downsample_data(recall, window_for_averaging)\n","            #precision = downsample_data(precision, window_for_averaging)\n","            #f1 = downsample_data(f1, window_for_averaging)\n","\n","\n","        plot_metrics(accuracies, #recall, precision, f1,\n","                     timegrid, window_for_averaging, rnn_window,\n","                     pre_sleep_window, awake_window, ifsaveplots,\n","                     output_path, ifplot = ifplot)\n","\n","        return accuracies, recall, precision, f1\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Rt9SL2MVVJpF"},"source":["### Define the training and evaluation functions for regression"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"OKcKss-pVJpF","executionInfo":{"status":"ok","timestamp":1693954108034,"user_tz":-60,"elapsed":246,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["import torch\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","def train_regression(model, train_loader, num_epochs, criterion,\n","                     optimizer, device, ifplot=False, ifvalidation = True, val_loader = None):\n","\n","    train_losses = []\n","    val_losses = []\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n","\n","        # Train the model\n","        model.train()\n","        running_train_loss = 0.0\n","        train_predictions, train_actuals = [], []\n","        for i, (inputs, labels_and_endpoints) in enumerate(train_loader):\n","            inputs = inputs.to(device)\n","            labels = labels_and_endpoints[:, 1]\n","            labels= labels.to(device)\n","\n","            # Forward pass\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels.view(-1, 1))\n","\n","            # Backward and optimize\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_train_loss += loss.item()\n","            outputs_detached = outputs.detach()\n","            train_predictions.extend(outputs_detached.cpu().numpy().flatten())\n","            train_actuals.extend(labels.cpu().numpy())\n","\n","        avg_train_loss = running_train_loss / len(train_loader)\n","        train_losses.append(avg_train_loss)\n","\n","\n","\n","        if ifvalidation:\n","            # Validate the model\n","            model.eval()\n","            running_val_loss = 0.0\n","            val_predictions, val_actuals = [], []\n","            with torch.no_grad():\n","                for inputs, labels_and_endpoints in val_loader:\n","                    inputs = inputs.to(device)\n","                    labels = labels_and_endpoints[:, 1]\n","                    labels = labels.to(device)\n","\n","                    outputs = model(inputs)\n","                    running_val_loss += criterion(outputs, labels.view(-1, 1)).item()\n","                    val_predictions.extend(outputs.cpu().numpy().flatten())\n","                    val_actuals.extend(labels.cpu().numpy())\n","\n","\n","\n","                avg_val_loss = running_val_loss / len(val_loader)\n","                val_losses.append(avg_val_loss)\n","\n","        # Calculate metrics\n","        if ifvalidation:\n","            mse = mean_squared_error(val_actuals, val_predictions)\n","            r2 = r2_score(val_actuals, val_predictions)\n","            mae = mean_absolute_error(val_actuals, val_predictions)\n","        else:\n","            mse = mean_squared_error(train_actuals, train_predictions)\n","            r2 = r2_score(train_actuals, train_predictions)\n","            mae = mean_absolute_error(train_actuals, train_predictions)\n","\n","        if ifplot:\n","            print('-'*100)\n","            if ifvalidation:\n","                print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}')\n","            else:\n","                print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss}')\n","            print(f'Mean Squared Error: {mse}, R2 Score: {r2}')\n","            print(f'Mean Absolute Error: {mae}')\n","    if ifvalidation:\n","        del train_predictions, train_actuals\n","        return  val_predictions, val_actuals, train_losses, val_losses, model, optimizer\n","    else:\n","        return  train_predictions, train_actuals, train_losses, model, optimizer\n","\n","\n","# Test function\n","def test_regression(model, test_loader, device, criterion, ifplot=False):\n","    model.eval()\n","    predictions, actuals = [], []\n","\n","    with torch.no_grad():\n","        test_loss = 0\n","        for inputs, labels_and_endpoints in test_loader:\n","            inputs = inputs.to(device)\n","            labels = labels_and_endpoints[:, 1].to(device)\n","\n","            outputs = model(inputs)  # these are outputs now\n","            test_loss += criterion(outputs, labels.view(-1, 1)).item()\n","\n","            predictions.extend(outputs.cpu().numpy().flatten())\n","            actuals.extend(labels.cpu().numpy())\n","        if ifplot:\n","            print(f'Test Loss: {test_loss/len(test_loader)}')\n","\n","    # Calculate metrics\n","    mse = mean_squared_error(actuals, predictions)\n","    r2 = r2_score(actuals, predictions)\n","    mae = mean_absolute_error(actuals, predictions)\n","    if ifplot:\n","        print('Mean Squared Error:', mse)\n","        print('R2 Score:', r2)\n","        print('Mean Absolute Error:', mae)\n","    del mse, r2, mae\n","    return predictions, actuals"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"SwDmktEyVJpF","executionInfo":{"status":"ok","timestamp":1693954108034,"user_tz":-60,"elapsed":3,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["def evaluate_regression(predictions, y_test, ifsaveplots = False, savepath = None, ifplot = False, iftest = False):\n","\n","    # check if the predictions are in the correct format\n","\n","    predictions = np.array(predictions)\n","\n","    # check if the y_test is in the correct format\n","\n","    y_test = np.array(y_test)\n","\n","    errors = abs(y_test - predictions)\n","    mape = np.where(y_test != 0, 100 * (errors / y_test), 0)\n","    accuracy = 100 - np.mean(mape)\n","    mse = sklearn.metrics.mean_squared_error(y_test, predictions)\n","    rmse = np.sqrt(mse)\n","    r2 = sklearn.metrics.r2_score(y_test, predictions)\n","    squared_errors = np.square(y_test - predictions)\n","    weights = 1.0 / (abs(y_test) + 0.1)\n","    custom_mse = np.mean(weights * squared_errors)\n","\n","    if ifplot:\n","\n","        print('Mean Absolute Error:', round(np.mean(errors), 2))\n","        print('Accuracy:', round(accuracy, 2), '%.')\n","        print('Mean Squared Error:', round(mse, 2))\n","        print('Root Mean Squared Error:', round(rmse, 2))\n","        print('R2:', round(r2, 2))\n","        print('Custom MSE:', round(custom_mse, 2))\n","\n","\n","\n","\n","    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=1)\n","    plt.scatter(y_test, predictions, alpha=0.2)\n","    plt.xlabel('Actual time to sleep onset')\n","    plt.ylabel('Predicted time to sleep onset')\n","    plt.title('Actual vs Predicted time to sleep onset')\n","    if ifsaveplots:\n","        if iftest:\n","            plt.savefig(os.path.join(savepath, 'test_actual_vs_predicted.png'))\n","        else:\n","            plt.savefig(os.path.join(savepath, 'crossval_actual_vs_predicted.png'))\n","    if ifplot:\n","        plt.show()\n","    plt.close()\n","\n","    residuals = y_test - predictions\n","    plt.scatter(y_test, residuals, alpha=0.2)\n","    plt.plot([y_test.min(), y_test.max()], [0, 0], 'k--', lw=1)\n","    plt.xlabel('Actual time to sleep onset')\n","    plt.ylabel('Residuals')\n","    plt.title('Actual vs Residuals for predicted time to sleep onset')\n","    if ifsaveplots:\n","        if iftest:\n","            plt.savefig(os.path.join(savepath, 'test_actual_vs_residuals.png'))\n","        else:\n","            plt.savefig(os.path.join(savepath, 'crossval_actual_vs_residuals.png'))\n","    if ifplot:\n","        plt.show()\n","    plt.close()\n","\n","    plt.hist(residuals)\n","    plt.xlabel('Residuals')\n","    plt.ylabel('Frequency')\n","    plt.title('Residuals distribution')\n","    if ifsaveplots:\n","        if iftest:\n","            plt.savefig(os.path.join(savepath, 'test_residuals_distribution.png'))\n","        else:\n","            plt.savefig(os.path.join(savepath, 'crossval_residuals_distribution.png'))\n","    if ifplot:\n","        plt.show()\n","    plt.close()\n","\n","    # for each actual, plot the mean residual\n","    # round y_test to 0.1 decimal place\n","    y_test = np.round(y_test, 1)\n","\n","    predictions_residuals_dict = {}\n","    for i in range(len(y_test)):\n","        if y_test[i] not in predictions_residuals_dict:\n","            predictions_residuals_dict[y_test[i]] = []\n","        predictions_residuals_dict[y_test[i]].append(residuals[i])\n","\n","    # sort it in ascending order of keys\n","    predictions_residuals_dict = dict(sorted(predictions_residuals_dict.items()))\n","\n","\n","    actual_predictions_dict = {}\n","    for i in range(len(y_test)):\n","        if y_test[i] not in actual_predictions_dict:\n","            actual_predictions_dict[y_test[i]] = []\n","        actual_predictions_dict[y_test[i]].append(predictions[i])\n","\n","    # sort it in ascending order of keys\n","    actual_predictions_dict = dict(sorted(actual_predictions_dict.items()))\n","\n","\n","    predictions_residuals_mean_dict = {}\n","    for key in predictions_residuals_dict:\n","        predictions_residuals_mean_dict[key] = np.mean(predictions_residuals_dict[key])\n","\n","    # sort it in ascending order of keys\n","    predictions_residuals_mean_dict = dict(sorted(predictions_residuals_mean_dict.items()))\n","\n","\n","    residuals_std_dict = {}\n","    for key in predictions_residuals_dict:\n","        residuals_std_dict[key] = np.std(predictions_residuals_dict[key])\n","    # sort it in ascending order of keys\n","    residuals_std_dict = dict(sorted(residuals_std_dict.items()))\n","\n","\n","    actuals_predictions_mean_dict = {}\n","    for key in actual_predictions_dict:\n","        actuals_predictions_mean_dict[key] = np.mean(actual_predictions_dict[key])\n","\n","    # sort it in ascending order of keys\n","    actuals_predictions_mean_dict = dict(sorted(actuals_predictions_mean_dict.items()))\n","\n","    predictions_std_dict = {}\n","    for key in actual_predictions_dict:\n","        predictions_std_dict[key] = np.std(actual_predictions_dict[key])\n","    # sort it in ascending order of keys\n","    predictions_std_dict = dict(sorted(predictions_std_dict.items()))\n","\n","    plt.plot(list(predictions_residuals_mean_dict.keys()), list(predictions_residuals_mean_dict.values()))\n","    # create std shading\n","    plt.fill_between(list(predictions_residuals_mean_dict.keys()),\n","                        np.array(list(predictions_residuals_mean_dict.values())) - np.array(list(residuals_std_dict.values())),\n","                        np.array(list(predictions_residuals_mean_dict.values())) + np.array(list(residuals_std_dict.values())),\n","                        alpha=0.2)\n","    # plot ideal line (x = 0)\n","    plt.plot([y_test.min(), y_test.max()], [0, 0], 'k--', lw=1, color='red')\n","    plt.legend(['Mean of residuals', 'STD of residuals', 'Ideal residuals line'])\n","    plt.xlabel('Actual time to sleep onset')\n","    plt.ylabel('Residuals for this actual')\n","    plt.title('Mean residual of predictions for each actual time to sleep onset')\n","    if ifsaveplots:\n","        if iftest:\n","            plt.savefig(os.path.join(savepath, 'test_mean_residuals_across_time.png'))\n","        else:\n","            plt.savefig(os.path.join(savepath, 'crossval_mean_residuals_across_time.png'))\n","    if ifplot:\n","        plt.show()\n","    plt.close()\n","\n","    plt.plot(list(actuals_predictions_mean_dict.keys()), list(actuals_predictions_mean_dict.values()))\n","    # create std shading\n","    plt.fill_between(list(actuals_predictions_mean_dict.keys()),\n","                        np.array(list(actuals_predictions_mean_dict.values())) - np.array(list(predictions_std_dict.values())),\n","                        np.array(list(actuals_predictions_mean_dict.values())) + np.array(list(predictions_std_dict.values())),\n","                        alpha=0.2)\n","    # plot ideal line (x = y)\n","    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=1, color='red')\n","    plt.legend(['Mean of predictions', 'STD of predictions', 'Ideal actual = prediction line (x = y)'])\n","    plt.xlabel('Actual time to sleep onset')\n","    plt.ylabel('Mean prediction for this actual')\n","    plt.title('Mean prediction for each actual time to sleep onset')\n","    if ifsaveplots:\n","        if iftest:\n","            plt.savefig(os.path.join(savepath, 'test_mean_predictions_across_time.png'))\n","        else:\n","             plt.savefig(os.path.join(savepath, 'crossval_mean_predictions_across_time.png'))\n","    if ifplot:\n","        plt.show()\n","    plt.close()\n","\n","    test_metrics = {'MAE': round(np.mean(errors), 2), 'Accuracy': round(accuracy, 2),\n","    'MSE': round(mse, 2), 'RMSE': round(rmse, 2), 'R2': round(r2, 2), 'custom_MSE': round(custom_mse, 2)}\n","\n","\n","    del predictions, predictions_residuals_dict\n","    del actual_predictions_dict, predictions_residuals_mean_dict\n","    del actuals_predictions_mean_dict, predictions_std_dict, residuals, residuals_std_dict, y_test\n","    del mse, rmse, r2, custom_mse\n","\n","    return test_metrics"]},{"cell_type":"markdown","metadata":{"id":"HO7xWkvYVJpF"},"source":["### Define functions for assessing the model's performance on a single participant"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"N7p64dcGVJpF","executionInfo":{"status":"ok","timestamp":1693954108034,"user_tz":-60,"elapsed":2,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["def select_random_participants(df, n_participants):\n","    \"\"\"Selects a random sample of participants from the dataset.\n","\n","    Args:\n","        df (pandas.DataFrame): The dataset.\n","        n_participants (int): The number of participants to select.\n","\n","    Returns:\n","        pandas.DataFrame: A random sample of participants.\n","    \"\"\"\n","    # Select a random sample of participants\n","    participants = df['Sbj_ID'].unique()\n","    np.random.seed(42)\n","    selected_participants = np.random.choice(participants, n_participants, replace=False)\n","\n","    for participant in selected_participants:\n","        # check how many nan values there are in the dataset for this participant\n","        n_nans = df[df['Sbj_ID'] == participant].isna().sum().sum()\n","        # if there are more than 20 nan values, select another participant\n","        while n_nans > 50:\n","            selected_participants = np.delete(selected_participants, np.where(selected_participants == participant))\n","            new_participant = np.random.choice(participants, 1, replace=False)\n","            # check if the new participant is already in the list\n","            if new_participant not in selected_participants:\n","                selected_participants = np.append(selected_participants, new_participant)\n","            else:\n","                continue\n","            participant = selected_participants[-1]\n","            n_nans = df[df['Sbj_ID'] == participant].isna().sum().sum()\n","\n","    # Filter the dataset\n","    df = df[df['Sbj_ID'].isin(selected_participants)].copy()\n","\n","    return df, selected_participants"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"Wt0QI743VJpG","executionInfo":{"status":"ok","timestamp":1693954108034,"user_tz":-60,"elapsed":2,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["def test_on_random_participants(data, model, criterion, random_seed, ifplot,\n","                                device, output_path_new, index, convergence_point,\n","                                awake_window, pre_sleep_window, ifsaveplots = True,\n","                                window_size_minutes = 1, ifmissing = True,\n","                                method = None, resampling_method = None, filling_limit = 0):\n","\n","    \"\"\"\n","    Test the model on a random subset of participants.\n","    \"\"\"\n","    # create a new folder for the results\n","    output_path_new = os.path.join(output_path_new, 'random_participants')\n","\n","    if not os.path.exists(output_path_new):\n","        os.makedirs(output_path_new)\n","\n","    # intialise empty dataframe to store results\n","    df_all = pd.DataFrame()\n","\n","    participant_dataloaders = {}\n","\n","    for i, sbj in enumerate(data['Sbj_ID'].unique()):\n","\n","        print('Participant {} of {}'.format(i + 1, len(data['Sbj_ID'].unique())))\n","\n","        # create a folder for each participant\n","        output_path_participant = os.path.join(output_path_new, str(sbj))\n","        if ifsaveplots:\n","            if not os.path.exists(output_path_participant):\n","                os.makedirs(output_path_participant)\n","\n","        participant_data = data[data['Sbj_ID'] == sbj].copy()\n","\n","        test_sequences_sbj, test_labels_sbj, test_end_points_sbj, test_sleep_stages_sbj, index, label = prepare_classification_data(mydata_train = participant_data,\n","                                                                                                        sleep_onset_threshold = convergence_point,\n","                                                                                                        awake_threshold = convergence_point,\n","                                                                                                        method = method, filling_limit = filling_limit,\n","                                                                                                        window_size_minutes = window_size_minutes,\n","                                                                                                        ifmissing = ifmissing, random_seed = random_seed,\n","                                                                                                        resampling_method = None)\n","\n","        # Get the tuple of test labels and test end points\n","        test_labels_and_end_points_sbj = [(test_labels_sbj[i], test_end_points_sbj[i]) for i in range(len(test_labels_sbj))]\n","\n","        # Create the test dataset and dataloader with labels and end points (for the regression head)\n","        test_dataset_sbj = SleepDataset(test_sequences_sbj, test_labels_and_end_points_sbj)\n","        test_loader_sbj = DataLoader(test_dataset_sbj, batch_size=1, shuffle=False)\n","\n","        predictions, actuals, probas = test_classification(model, test_loader_sbj, device, criterion, index, threshold=0.5, ifprobabilities=True)\n","        timeline = test_end_points_sbj #participant_data['Old_label']\n","        participant_dataloaders[sbj] = test_loader_sbj\n","\n","        # get the accuracy of each timepoint\n","        if ifplot:\n","            print('Predict for participant: ', sbj)\n","\n","            print('--------------------------------------Evaluate classificaiton ----------------------------------------')\n","\n","        test_metrics = evaluate_classification(actuals, predictions, probas, index = index, iftest=True, columns = label,\n","                                ifplot=ifplot, output_path=output_path_participant, ifsaveplots = ifsaveplots, classes = None, ifvocal = False)\n","        test_metrics['Sbj_ID'] = sbj\n","\n","\n","        if ifplot:\n","                print('------------------------------------------- No averaging ----------------------------------------')\n","                print('Accuracy')\n","        _, _, _, _ = check_accuracy_timeline(predictions, test_labels_sbj, timeline,\n","                                            test_sleep_stages_sbj, window_for_averaging= 1, awake_window = awake_window,\n","                                            pre_sleep_window = pre_sleep_window, rnn_window = window_size_minutes,\n","                                            ifsaveplots = ifsaveplots, output_path= output_path_participant, ifplot = ifplot)\n","        if ifplot:\n","            print('Probability of sleep as predicted by the model')\n","        num_crossings, time_of_prediction = check_accuracy_timeline(probas, test_labels_sbj, timeline,\n","                                                test_sleep_stages_sbj, window_for_averaging= 1, if_proba = True,\n","                                                classes = index, awake_window = awake_window, pre_sleep_window = pre_sleep_window,\n","                                                rnn_window = window_size_minutes, ifsaveplots = ifsaveplots, output_path= output_path_participant,\n","                                                ifplot = ifplot, ifsubjecttest = True, ifcrossings = True)\n","        test_metrics['num_crossings'] = num_crossings\n","        test_metrics['time_of_prediction'] = time_of_prediction\n","\n","        if ifplot:\n","            print('-------------------------------Averaging of accuracy over 30 seconds---------------------------------')\n","            print('Accuracy')\n","        _, _, _, _ = check_accuracy_timeline(predictions, test_labels_sbj, timeline,\n","                                            test_sleep_stages_sbj, window_for_averaging= 5,\n","                                            awake_window = awake_window, pre_sleep_window = pre_sleep_window,\n","                                            rnn_window = window_size_minutes, ifsaveplots = ifsaveplots,\n","                                            output_path= output_path_participant, ifplot = ifplot)\n","        if ifplot:\n","            print('Probability of sleep as predicted by the model')\n","        num_crossings_smoothed, time_of_prediction_smoothed = check_accuracy_timeline(probas, test_labels_sbj, timeline,\n","                                                test_sleep_stages_sbj, window_for_averaging= 5, if_proba = True,\n","                                                classes = index, awake_window = awake_window, pre_sleep_window = pre_sleep_window,\n","                                                rnn_window = window_size_minutes, ifsaveplots = ifsaveplots, output_path= output_path_participant,\n","                                                ifplot = ifplot, ifsubjecttest = True, ifcrossings = True)\n","\n","        test_metrics['num_crossings_smoothed'] = num_crossings_smoothed\n","        test_metrics['time_of_prediction_smoothed'] = time_of_prediction_smoothed\n","\n","        del test_dataset_sbj, test_loader_sbj, test_labels_and_end_points_sbj, test_sequences_sbj, test_labels_sbj, test_end_points_sbj, test_sleep_stages_sbj\n","        del predictions, actuals, probas\n","\n","\n","        next_index = len(df_all)\n","        df = pd.DataFrame(test_metrics, index=[next_index])\n","        df_all = pd.concat([df_all, df])\n","\n","        del test_metrics\n","\n","    df_all.to_csv(os.path.join(output_path_new, 'classification_results_random_participants.csv'))\n","\n","    # calculate how many participants have only one crossing in df_all['num_crossings'] out of all the participants\n","    num_participants_one_crossing = len(df_all[df_all['num_crossings'] == 1])\n","    num_participants = len(df_all)\n","    one_crossing_percentage = num_participants_one_crossing/num_participants * 100\n","    mean_time_of_prediction = np.mean(df_all['time_of_prediction'][df_all['num_crossings'] == 1])\n","    std_time_of_prediction = np.std(df_all['time_of_prediction'][df_all['num_crossings'] == 1])\n","\n","\n","    num_participants_one_crossing_smoothed = len(df_all[df_all['num_crossings_smoothed'] == 1])\n","    one_crossing_percentage_smoothed = num_participants_one_crossing_smoothed/num_participants * 100\n","    mean_time_of_prediction_smoothed = np.mean(df_all['time_of_prediction_smoothed'][df_all['num_crossings_smoothed'] == 1])\n","    std_time_of_prediction_smoothed = np.std(df_all['time_of_prediction_smoothed'][df_all['num_crossings_smoothed'] == 1])\n","\n","\n","    print(f'Percentage of participants with only one crossing: {one_crossing_percentage}%')\n","    print(f'Mean time of prediction for participants with only one crossing: {mean_time_of_prediction} minutes')\n","    print(f'STD of time of prediction for participants with only one crossing: {std_time_of_prediction} minutes')\n","\n","\n","    print(f'Percentage of participants with only one crossing (smoothed): {one_crossing_percentage_smoothed}%')\n","    print(f'Mean time of prediction for participants with only one crossing (smoothed): {mean_time_of_prediction_smoothed} minutes')\n","    print(f'STD of time of prediction for participants with only one crossing (smoothed): {std_time_of_prediction_smoothed} minutes')\n","\n","    output_dict = {'one_crossing_percentage': one_crossing_percentage, 'mean_time_of_prediction': mean_time_of_prediction,\n","                    'std_time_of_prediction': std_time_of_prediction, 'one_crossing_percentage_smoothed': one_crossing_percentage_smoothed,\n","                    'mean_time_of_prediction_smoothed': mean_time_of_prediction_smoothed, 'std_time_of_prediction_smoothed': std_time_of_prediction_smoothed}\n","\n","    del one_crossing_percentage, mean_time_of_prediction, std_time_of_prediction, one_crossing_percentage_smoothed, mean_time_of_prediction_smoothed, std_time_of_prediction_smoothed\n","\n","    return participant_dataloaders, output_dict"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"ZmAJNSoSVJpG","executionInfo":{"status":"ok","timestamp":1693954108034,"user_tz":-60,"elapsed":2,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["def test_reg_on_random_participants(participant_dataloaders, model, criterion, device, random_seed, ifplot, output_path_new, index):\n","\n","    \"\"\"\n","    Test the model on a random subset of participants.\n","    \"\"\"\n","    # Set random seed\n","\n","    # create a new folder for the results\n","    output_path_new = os.path.join(output_path_new, 'random_participants')\n","    if not os.path.exists(output_path_new):\n","        os.makedirs(output_path_new)\n","\n","    # intialise empty dataframe to store results\n","    df_all = pd.DataFrame()\n","\n","\n","    for sbj in list(participant_dataloaders.keys()):\n","        # create a folder for each participant\n","\n","        output_path_participant = os.path.join(output_path_new, str(sbj))\n","        if not os.path.exists(output_path_participant):\n","            os.makedirs(output_path_participant)\n","\n","        test_dataloader = participant_dataloaders[sbj]\n","\n","        predictions, actuals = test_regression(model, test_dataloader, device, criterion)\n","\n","        # get the accuracy of each timepoint\n","        if ifplot:\n","            print('Predict for participant: ', sbj)\n","\n","            print('--------------------------------------Evaluate regression ----------------------------------------')\n","\n","        test_metrics = evaluate_regression(predictions, actuals, iftest=True,\n","                                ifplot=ifplot, savepath =output_path_participant, ifsaveplots= True)\n","\n","\n","        del test_dataloader, predictions, actuals\n","        df = pd.DataFrame(test_metrics, index = [sbj])\n","        df_all = pd.concat([df_all, df])\n","        del test_metrics\n","\n","    df_all.to_csv(os.path.join(output_path_new, 'regression_results_random_participants.csv'))\n","\n"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"z-ZZn62iVJpG","executionInfo":{"status":"ok","timestamp":1693954108034,"user_tz":-60,"elapsed":2,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["\n","def train_test_split_sbjs(mydata_no_nan, test_size = 0.2, random_seed = 42):\n","\n","    # assert that the data has no NaNs\n","    assert mydata_no_nan.isnull().sum().sum() == 0, 'There are NaNs in the data'\n","\n","    # get the unique subjects\n","    unique_subjects = mydata_no_nan['Sbj_ID'].unique()\n","\n","    # define the train size\n","    train_size = 1 - test_size\n","\n","    # define the number of subjects in the test set\n","\n","    test_size_subjects = int(test_size * len(unique_subjects))\n","\n","    # Select the unique subjects for the test set\n","\n","    test_subjects = random.sample(list(unique_subjects), test_size_subjects)\n","\n","    # Select the unique subjects for the training set\n","    train_subjects = [x for x in unique_subjects if x not in test_subjects]\n","\n","    # Create the test and training sets\n","    test_set = mydata_no_nan[mydata_no_nan['Sbj_ID'].isin(test_subjects)]\n","    train_set = mydata_no_nan[mydata_no_nan['Sbj_ID'].isin(train_subjects)]\n","\n","    # Create X and y from dataframes to use in scikit-learn (drop Label, SleepStage, Sbj_ID and ifCleanOnset columns)\n","\n","    X_train = train_set.drop(['Label', 'SleepStage', 'Sbj_ID', 'ifCleanOnset', 'Time2Sleep'], axis=1)\n","    y_train = train_set['Label']\n","    X_test = test_set.drop(['Label', 'SleepStage', 'Sbj_ID', 'ifCleanOnset', 'Time2Sleep'], axis=1)\n","    y_test = test_set['Label']\n","\n","\n","    # Create list of sbj_IDs for each sample in the training and test sets\n","    sbj_train_set = train_set['Sbj_ID'].copy()\n","    sbj_test_set = test_set['Sbj_ID'].copy()\n","\n","    # Turn the sbj_IDs into a list\n","    sbj_train_set = sbj_train_set.tolist()\n","    sbj_test_set = sbj_test_set.tolist()\n","\n","\n","\n","    return X_train, y_train, X_test, y_test, sbj_train_set, sbj_test_set"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"LVU3ERUdVJpG","executionInfo":{"status":"ok","timestamp":1693954108034,"user_tz":-60,"elapsed":2,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["def get_rid_of_nans(probabilities, y_times2sleep = None):\n","    if np.isnan(probabilities).any():\n","        for i in range(len(probabilities)):\n","            if np.isnan(probabilities[i]):\n","                # omit the corresponding y_times2sleep\n","                if y_times2sleep is not None:\n","                    y_times2sleep.pop(i)\n","                probabilities.pop(i)\n","    return probabilities, y_times2sleep"]},{"cell_type":"markdown","metadata":{"id":"rib1PnKaVJpH"},"source":["### Define the function for schedule learning"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"jw1pMV1_VJpH","executionInfo":{"status":"ok","timestamp":1693954108035,"user_tz":-60,"elapsed":3,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["def update_thresholds(epoch, initial_sleep_onset_threshold, initial_awake_threshold, awake_delta, sleep_delta, schedule_period=1):\n","    \"\"\"\n","    Updates the thresholds based on a target convergence point.\n","\n","    Parameters:\n","    - epoch: current epoch number\n","    - initial_awake_threshold: initial awake threshold\n","    - initial_sleep_onset_threshold: initial sleep onset threshold\n","    - convergence_point: point at which the thresholds should converge\n","    - total_epochs: total epochs for which the training is scheduled\n","    - schedule_period: epochs after which thresholds are adjusted. Default is 10 epochs.\n","\n","    Returns:\n","    - New awake and sleep_onset thresholds\n","    \"\"\"\n","\n","    # Adjusting both thresholds based on the epoch\n","    if epoch % schedule_period == 0 :\n","        initial_awake_threshold -= awake_delta\n","        initial_sleep_onset_threshold += sleep_delta\n","\n","    return round(initial_sleep_onset_threshold, 1), round(initial_awake_threshold,1)"]},{"cell_type":"markdown","metadata":{"id":"8qjH2rzuVJpH"},"source":["### Define helper functions for reducing the length of schedule learning loop"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"V7DWQO-CVJpH","executionInfo":{"status":"ok","timestamp":1693954108035,"user_tz":-60,"elapsed":3,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["def prepare_classification_data(mydata_train, sleep_onset_threshold,\n","                                awake_threshold, method, filling_limit, window_size_minutes,\n","                                ifmissing, random_seed, resampling_method,\n","                                ):\n","\n","    mydata_train_class, index, label = regression_to_classification(mydata_train, sleep_onset_threshold, awake_threshold)\n","\n","    mydata_imputed_class = impute_missing_values(mydata_train_class, method=method, limit=filling_limit)\n","\n","\n","    train_sequences, train_labels, _, _, _, _, train_end_points, train_sleep_stages = create_sliding_windows(data=mydata_imputed_class, window_size_minutes=window_size_minutes,\n","                                                                                                        ifmissing=ifmissing, random_seed=random_seed, train_proportion=1,\n","                                                                                                        if_stratified_sampling=0, resampling_method=resampling_method,\n","                                                                                                          iftest=0, ifoutput_end_points=1)\n","    del mydata_imputed_class\n","    del mydata_train\n","\n","    return train_sequences, train_labels, train_end_points, train_sleep_stages, index, label\n","\n","def define_loss_function(ifclassweights, train_labels, device, ifplot = False):\n","    if ifclassweights:\n","        class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n","        class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n","        pos_class_idx = np.where(np.unique(train_labels) == 1)[0][0]\n","        pos_weight = torch.tensor([class_weights[pos_class_idx]]).to(device)\n","        if ifplot:\n","            print('Positive class weights: ', pos_weight)\n","        return nn.BCEWithLogitsLoss(pos_weight=pos_weight), pos_weight\n","    else:\n","        return nn.BCEWithLogitsLoss(), None\n","\n","def train_and_evaluate_classification_model(model, train_loader, label,  test_labels, test_end_points,\n","                                            test_sleep_stages,\n","                                            test_loader, device, criterion,\n","                                            optimizer, num_epochs, index, ifprobabilities,\n","                                            ifplot, ifsaveplots, save_directory_epoch,\n","                                            awake_threshold, sleep_onset_threshold, window_size_minutes):\n","    if ifprobabilities:\n","        train_losses, test_losses, model, optimizer, test_predictions, test_actuals, test_prediction_probabilities = train_classification(model = model, train_loader=train_loader,\n","                                                                                                                                          val_loader=test_loader,\n","                                                                                                                                          device = device,\n","                                                                                                                                          criterion=criterion, optimizer=optimizer,\n","                                                                                                                                          num_epochs=num_epochs, index=index,\n","                                                                                                                                          ifprobabilities=ifprobabilities,\n","                                                                                                                                          ifplot = ifplot)\n","\n","        test_metrics = evaluate_classification(true_labels=test_actuals, predictions=test_predictions,\n","                                               prediction_probabilities=test_prediction_probabilities,\n","                                               index=index, columns = label, ifsaveplots=ifsaveplots,\n","                                               output_path=save_directory_epoch, ifplot=ifplot)\n","\n","        if ifplot:\n","                print('------------------------------------------- No averaging ----------------------------------------')\n","                print('Accuracy')\n","        accuracies, precision, recall, f1 = check_accuracy_timeline(test_predictions, test_labels, test_end_points,\n","                                                                    test_sleep_stages, window_for_averaging= 1, awake_window=awake_threshold,\n","                                                                    pre_sleep_window=sleep_onset_threshold, rnn_window = window_size_minutes,\n","                                                                    ifsaveplots = ifsaveplots, output_path= save_directory_epoch, ifplot = ifplot)\n","        if ifplot:\n","            print('Probability of sleep as predicted by the model')\n","        y_pred_sorted = check_accuracy_timeline(test_prediction_probabilities, test_labels, test_end_points,\n","                                                test_sleep_stages, window_for_averaging= 1, if_proba = True,\n","                                                classes = index, awake_window=awake_threshold, pre_sleep_window=sleep_onset_threshold,\n","                                                rnn_window = window_size_minutes, ifsaveplots = ifsaveplots, output_path= save_directory_epoch,\n","                                                ifplot = ifplot)\n","        if ifplot:\n","            print('-------------------------------Averaging of accuracy over 30 seconds---------------------------------')\n","            print('Accuracy')\n","        accuracies, precision, recall, f1 = check_accuracy_timeline(test_predictions, test_labels, test_end_points,\n","                                                                    test_sleep_stages, window_for_averaging= 5,\n","                                                                    awake_window=awake_threshold, pre_sleep_window=sleep_onset_threshold,\n","                                                                    rnn_window = window_size_minutes, ifsaveplots = ifsaveplots,\n","                                                                    output_path= save_directory_epoch, ifplot = ifplot)\n","        if ifplot:\n","            print('Probability of sleep as predicted by the model')\n","        y_pred_sorted = check_accuracy_timeline(test_prediction_probabilities, test_labels, test_end_points,\n","                                                test_sleep_stages, window_for_averaging= 5, if_proba = True,\n","                                                classes = index,  awake_window=awake_threshold,\n","                                                pre_sleep_window=sleep_onset_threshold, rnn_window = window_size_minutes,\n","                                                ifsaveplots = ifsaveplots, output_path= save_directory_epoch, ifplot = ifplot)\n","\n","        return train_losses, test_losses, model, optimizer, test_predictions, test_actuals, test_prediction_probabilities, test_metrics\n","\n","    else:\n","        train_losses, test_losses, model, optimizer, test_predictions, test_actuals, test_prediction_probabilities = train_classification(model = model, train_loader=train_loader,\n","                                                                                                                                          val_loader=test_loader,\n","                                                                                                                                          device = device,\n","                                                                                                                                          criterion=criterion, optimizer=optimizer,\n","                                                                                                                                          num_epochs=num_epochs, index=index,\n","                                                                                                                                          ifprobabilities=ifprobabilities,\n","                                                                                                                                          ifplot = ifplot)\n","\n","        test_metrics = evaluate_classification(true_labels=test_actuals, predictions=test_predictions,\n","                                               prediction_probabilities=None,\n","                                               index=index, columns = label, ifsaveplots=ifsaveplots,\n","                                               output_path=save_directory_epoch, ifplot=ifplot)\n","\n","        return train_losses, test_losses, model, optimizer, test_predictions, test_actuals, None, test_metrics\n","\n","def train_and_evaluate_regression_model(model, train_loader, test_loader, criterion, optimizer, device, num_epochs, ifplot, ifsaveplots, output_path):\n","    test_predictions, test_actuals, train_losses, test_losses, model, optimizer = train_regression(\n","        model = model, train_loader = train_loader, val_loader = test_loader, num_epochs = num_epochs, criterion = criterion, optimizer =optimizer, device = device, ifplot = ifplot, ifvalidation = True)\n","\n","    test_metrics = evaluate_regression(test_predictions, test_actuals, ifsaveplots=ifsaveplots, savepath=output_path, ifplot = ifplot)\n","\n","    return train_losses, test_losses, model, optimizer, test_predictions, test_actuals, test_metrics"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"dwm0rzq3VJpH","executionInfo":{"status":"ok","timestamp":1693954108035,"user_tz":-60,"elapsed":3,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["def save_models(model_classification, model_regression, save_directory_epoch, sleep_onset_threshold, awake_threshold, window_size_minutes):\n","    model_path_classification = os.path.join(save_directory_epoch, f\"model_classification_presleep_{sleep_onset_threshold}_awake_{awake_threshold}_window_{window_size_minutes}.pt\")\n","    model_regression_path = os.path.join(save_directory_epoch, f\"model_regression_presleep_{sleep_onset_threshold}_awake_{awake_threshold}_window_{window_size_minutes}.pt\")\n","    torch.save(model_classification.state_dict(), model_path_classification)\n","    torch.save(model_regression.state_dict(), model_regression_path)\n","\n","def plot_losses(train_losses, test_losses, title, ifplot, ifsaveplots, save_directory, filename):\n","    plt.plot(train_losses, label='Training loss')\n","    plt.plot(test_losses, label='Validation loss')\n","    plt.title(title)\n","    plt.legend()\n","    if ifplot:\n","        plt.show()\n","    if ifsaveplots:\n","        plt.savefig(os.path.join(save_directory, filename))\n","    plt.close()"]},{"cell_type":"markdown","metadata":{"id":"_99nmyw3VJpH"},"source":["### Define custom MSE loss function"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1693954108605,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"},"user_tz":-60},"id":"jYPyf4ZFVJpH","outputId":"9a8e6a42-ffba-4d01-ec52-ea60cad300b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.1099)\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","class CustomMSELoss(nn.Module):\n","    def __init__(self, epsilon=0.1):\n","        super(CustomMSELoss, self).__init__()\n","        self.epsilon = epsilon  # Small value to prevent division by zero\n","\n","    def forward(self, predictions, targets):\n","        # Compute error\n","        errors = (predictions - targets)**2\n","\n","        # Compute weights inversely proportional to target values\n","        weights = 1.0 / (torch.abs(targets) + self.epsilon)\n","\n","        # Weighted sum of errors\n","        loss = torch.sum(weights * errors) / torch.sum(weights)\n","        return loss\n","\n","# Test the custom loss\n","predictions = torch.tensor([0.5, 0.2, 0.7])\n","targets = torch.tensor([0.1, 0.4, 0.8])\n","criterion = CustomMSELoss()\n","loss = criterion(predictions, targets)\n","print(loss)\n"]},{"cell_type":"markdown","metadata":{"id":"cTy5nGqzVJpI"},"source":["### Definte the loop for schedule learning"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"WCAn4qU4VJpI","executionInfo":{"status":"ok","timestamp":1693954108606,"user_tz":-60,"elapsed":5,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["from sklearn.utils.class_weight import compute_class_weight\n","import warnings\n","def training_loop_RNN_regclass_schedule(mydata, convergence_point, total_epochs,  filling_limit, method, ifclassweights, input_size,\n","                                        num_epochs, hidden_size, num_layers, dropout, l2, lr, weight_decay, batch_size,\n","                                        device, random_seed = 42, window_size_minutes = 1, ifprobabilitiesanalysis = 0,\n","                                        if_stratified_sampling = 1, initial_awake_threshold = 30, initial_sleep_onset_threshold = 2,\n","                                        convergence_sensitivity = 0.1, resampling_method = None, ifplot = False, ifsaveplots = True,\n","                                        output_path=None, shuffle_train_loader = True, results_saving_dir = None, selected_participants_data = None,\n","                                        selected_participants = None, ifresetresults = False):\n","\n","    ################## INITIALISATION OF RESULTS STORAGE ##################\n","    warnings.filterwarnings('ignore', category=RuntimeWarning)\n","    warnings.filterwarnings('ignore', category=UserWarning)\n","\n","    if results_saving_dir is None:\n","        results_saving_dir = output_path\n","\n","    save_directory = output_path\n","\n","    if not os.path.exists(save_directory):\n","        os.makedirs(save_directory)\n","\n","    # Initialize a DataFrame to store results\n","    columns = ['convergence_point', 'initial_sleep_onset_threshold','ifconverged', 'pre_sleep_window', 'awake_window', 'pos_class_weight',\n","                'sliding_window_size', 'Accuracy', 'precision', 'recall', 'f1_weighted',\n","                'f1_macro', 'f1_pre_sleep', 'f1_awake', 'auc',\n","                'one_crossing_percentage', 'mean_time_of_prediction',\n","                'std_time_of_prediction', 'one_crossing_percentage_smoothed',\n","                'mean_time_of_prediction_smoothed', 'std_time_of_prediction_smoothed',\n","                'MAE', 'MSE', 'RMSE', 'R2', 'custom_MSE']\n","\n","    if ifresetresults:\n","        results_df = pd.DataFrame(columns=columns)\n","        results_df.to_csv(os.path.join(results_saving_dir, 'results.csv'))\n","\n","    ################## PREPARE DATA FOR TRAINING AND TESTING ##################\n","    # Firstly, get the datasets for a classification and regressions problem\n","    # Create a new column 'old_label' to store the original label\n","    mydata_bothlabels = mydata.copy()\n","\n","\n","    mydata_bothlabels['Old_label'] = mydata_bothlabels['Label'].copy()\n","\n","    # The split we would get after the schedule learning convergence\n","    mydata_final_split, index, label = regression_to_classification(mydata, convergence_point, convergence_point)\n","\n","    del mydata\n","    # Fill in the missing values\n","    mydata_imputed = impute_missing_values(mydata_final_split, method = method, limit = filling_limit)\n","    del mydata_final_split\n","    # Cheсk if the dataset still has any missing values\n","    ifmissing = mydata_imputed.isnull().sum().sum()\n","\n","    # Split the data into train, validation and test sets\n","    _, _, _, _, _, _,  train_subjects, test_subjects, _ = create_sliding_windows(data = mydata_imputed,\n","                                                                        window_size_minutes=window_size_minutes,\n","                                                                        ifmissing=ifmissing, random_seed=random_seed,\n","                                                                        ifoutputsubjects = 1,\n","                                                                        if_stratified_sampling = if_stratified_sampling,\n","                                                                        resampling_method = None,\n","                                                                        train_proportion = 0.8, iftest = 0, ifoutput_end_points= 0)\n","    del mydata_imputed\n","    mydata_test = mydata_bothlabels[mydata_bothlabels['Sbj_ID'].isin(test_subjects)]\n","    mydata_train = mydata_bothlabels[~mydata_bothlabels['Sbj_ID'].isin(test_subjects)]\n","\n","    ########################## PREPARE THE TESTING DATA  #############################\n","    # Get the sequences (windows) and labels (categorical 1 or 0 for classification) for the test set,\n","    # also get the end points time of the sequences (in minutes) and their corresponding sleep stages (at the end of the sequence)\n","    test_sequences, test_labels, test_end_points, test_sleep_stages, _, _,= prepare_classification_data(mydata_train = mydata_test, sleep_onset_threshold = convergence_point,\n","                                                                                                        awake_threshold = convergence_point, method = method,\n","                                                                                                        filling_limit = filling_limit,\n","                                                                                                        window_size_minutes = window_size_minutes,\n","                                                                                                        ifmissing = ifmissing, random_seed = random_seed,\n","                                                                                                        resampling_method = None,\n","                                                                                                        )\n","\n","    # Get the tuple of test labels and test end points\n","    test_labels_and_end_points = [(test_labels[i], test_end_points[i]) for i in range(len(test_labels))]\n","\n","    # Create the test dataset and dataloader with labels and end points (for the regression head)\n","    test_dataset = SleepDataset(test_sequences, test_labels_and_end_points )\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    ########################## PREPARE INDIVIDUAL SUBJECTS TO TEST ON  #############################\n","    if selected_participants is not None:\n","        selected_participant_bothlabels = selected_participants_data.copy()\n","        selected_participant_bothlabels['Old_label'] = selected_participant_bothlabels['Label'].copy()\n","\n","\n","    ############################ INITIALISE THE SCHEDULED LEARNING  ################################\n","    # set the sleep onset and awake thresholds to initial values\n","    sleep_onset_threshold = initial_sleep_onset_threshold\n","    awake_threshold = initial_awake_threshold\n","\n","    # Calculate the deltas for the thresholds (how much the thresholds should change each iteration of the schedule until convergence)\n","    distance_to_convergence_sleep_onset = convergence_point - sleep_onset_threshold\n","    sleep_onset_delta = distance_to_convergence_sleep_onset / total_epochs\n","    distance_to_convergence_awake = awake_threshold - convergence_point\n","    awake_delta = distance_to_convergence_awake / total_epochs\n","\n","    ################################## INITIALISE MODELS  #########################################\n","    # Initialize the classification model\n","    model_classification = SleepOnsetRNNClassifier(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, l2=l2).to(device)\n","\n","    # Define the optimizer\n","    optimizer_class = torch.optim.AdamW(model_classification.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","    criterion_regression = CustomMSELoss()\n","\n","    # Empty lists to store training and validation losses\n","    train_losses_reg_all, test_losses_reg_all = [], []\n","\n","\n","    ############################# BEGIN THE SCHEDULED LEARNING  ###################################\n","\n","    for epoch in range(total_epochs):\n","\n","\n","        train_losses_class, test_losses_class, train_losses_reg, test_losses_reg = [], [], [], []\n","\n","        # Check if the thresholds have converged\n","        if abs(sleep_onset_threshold - convergence_point) < convergence_sensitivity:\n","            if ifplot:\n","                print('Sleep onset threshold has converged')\n","            break\n","        if abs(awake_threshold - convergence_point) < convergence_sensitivity:\n","            if ifplot:\n","                print('Awake threshold has converged')\n","            break\n","\n","        # Reinitilize the optimizer\n","        #optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","        # save each epoch in a separate folder\n","        save_directory_epoch = save_directory + '/epoch_' + str(epoch)\n","        if not os.path.exists(save_directory_epoch):\n","            os.makedirs(save_directory_epoch)\n","\n","\n","        sleep_onset_threshold, awake_threshold = update_thresholds(epoch, sleep_onset_threshold, awake_threshold, awake_delta, sleep_onset_delta)\n","\n","        if epoch == total_epochs - 1:\n","          sleep_onset_threshold = convergence_point\n","          awake_threshold = convergence_point\n","\n","\n","        #if ifplot:\n","        print('New sleep onset threshold: ', sleep_onset_threshold)\n","        print('New awake threshold: ', awake_threshold)\n","        model_parameters = {'pre_sleep_window': sleep_onset_threshold,\n","                            'awake_window': awake_threshold,\n","                            'convergence_point': convergence_point,\n","                            'sliding_window_size': window_size_minutes}\n","\n","\n","        # Prepare the data\n","        train_sequences, train_labels, train_end_points, train_sleep_stages, index, label = prepare_classification_data(mydata_train = mydata_train, sleep_onset_threshold = sleep_onset_threshold,\n","                                                                                                                        awake_threshold = awake_threshold ,\n","                                                                                                                        method = method, filling_limit = filling_limit,\n","                                                                                                                        window_size_minutes = window_size_minutes, ifmissing = ifmissing,\n","                                                                                                                        random_seed = random_seed, resampling_method = resampling_method)\n","        #print('Train sequences: ', train_sequences.shape)\n","        #print('Train labels: ', train_labels.shape)\n","        # adjust the classification training loss accordidng to the class proportion in the training set to prevent imbalance\n","        # only works if ifclassweights = 1\n","        criterion_class, pos_weight = define_loss_function(ifclassweights, train_labels, device)\n","\n","        # Create the training set and dataloader\n","        train_labels_and_end_points = [(train_labels[i], train_end_points[i]) for i in range(len(train_labels))]\n","\n","        train_dataset = SleepDataset(train_sequences, train_labels_and_end_points)\n","        if shuffle_train_loader:\n","            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","        else:\n","            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n","\n","        print('--------------------------------------Evaluate classification ----------------------------------------')\n","\n","        ########################## TRAIN AND EVALUATE  #############################\n","        train_losses_class, test_losses_class, model_classification, optimizer_class, test_predictions_class, test_actuals_class, test_prediction_probabilities_class, test_metrics_class = train_and_evaluate_classification_model(model = model_classification, train_loader = train_loader,\n","                                                                                                                                                                                                                                    label = label, test_labels = test_labels,\n","                                                                                                                                                                                                                                    test_end_points = test_end_points,\n","                                                                                                                                                                                                                                    test_sleep_stages= test_sleep_stages,\n","                                                                                                                                                                                                                                    test_loader = test_loader, device = device,\n","                                                                                                                                                                                                                                    criterion = criterion_class,\n","                                                                                                                                                                                                                                    optimizer = optimizer_class,\n","                                                                                                                                                                                                                                    num_epochs = num_epochs,\n","                                                                                                                                                                                                                                    index = index,\n","                                                                                                                                                                                                                                    ifprobabilities = ifprobabilitiesanalysis,\n","                                                                                                                                                                                                                                    ifplot = ifplot,\n","                                                                                                                                                                                                                                    ifsaveplots=ifsaveplots,\n","                                                                                                                                                                                                                                    save_directory_epoch = save_directory_epoch,\n","                                                                                                                                                                                                                                    awake_threshold= awake_threshold,\n","                                                                                                                                                                                                                                    sleep_onset_threshold=sleep_onset_threshold,\n","                                                                                                                                                                                                                                    window_size_minutes=window_size_minutes)\n","\n","        # plot the losses for the classification model\n","        title = 'Training and validation losses during classification with window size of ' + str(window_size_minutes) + ' minutes'\n","        filename = 'classification_losses.png'\n","        plot_losses(train_losses_class, test_losses_class, title, ifplot, ifsaveplots,\n","                    save_directory = save_directory_epoch, filename = filename)\n","\n","        # Save the pos class weight in the test metrics dictionary\n","        if ifclassweights:\n","            test_metrics_class['pos_class_weight'] = pos_weight.cpu().detach().numpy()[0]\n","\n","\n","        # Get the stats for probability change points and mean prediction times in participants on the validaiton set\n","        _,  participant_dict = test_on_random_participants(data = mydata_test, model =  model_classification, criterion = criterion_class, random_seed = random_seed,\n","                                                                                            ifplot = False, device = device, output_path_new =  save_directory_epoch, index = index,\n","                                                                                            convergence_point = convergence_point,\n","                                                                                            awake_window = awake_threshold, pre_sleep_window = sleep_onset_threshold,\n","                                                                                            ifsaveplots = False, window_size_minutes = window_size_minutes, ifmissing = True,\n","                                                                                            filling_limit = filling_limit, method = method, resampling_method = None)\n","\n","\n","        if selected_participants is not None:\n","            participant_dataloaders, _ = test_on_random_participants(data = selected_participant_bothlabels, model = model_classification, criterion = criterion_class,\n","                                                                                                random_seed = random_seed, ifplot = ifplot, device = device, output_path_new =  save_directory_epoch, index = index,\n","                                                                                                convergence_point = convergence_point,\n","                                                                                                awake_window = awake_threshold, pre_sleep_window = sleep_onset_threshold, ifsaveplots = True,\n","                                                                                                window_size_minutes = window_size_minutes, ifmissing = True,\n","                                                                                                filling_limit = filling_limit, method = method, resampling_method = None)\n","\n","\n","\n","        # From the pretrained classification model, create a new model for regression and train it\n","        model_regression = PretrainedModelForRegression(model_classification).to(device)\n","        optimizer_reg = torch.optim.AdamW(model_regression.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","        print('--------------------------------------Evaluate regression ----------------------------------------')\n","\n","        train_losses_reg, test_losses_reg, model_regression, optimizer_reg, test_predictions_red, test_actuals, test_metrics_reg = train_and_evaluate_regression_model(model = model_regression, train_loader =train_loader,\n","                                                                                                                                        test_loader = test_loader, criterion = criterion_regression,\n","                                                                                                                                        optimizer = optimizer_reg, device = device, num_epochs = num_epochs, ifplot = ifplot,\n","                                                                                                                                        ifsaveplots = ifsaveplots, output_path = save_directory_epoch)\n","        if selected_participants is not None:\n","            test_reg_on_random_participants(participant_dataloaders = participant_dataloaders, model = model_regression,  criterion = criterion_regression,\n","                                                 device = device, random_seed = random_seed, ifplot = ifplot, output_path_new =save_directory_epoch, index = index)\n","\n","\n","\n","        # After each epoch, save the model\n","        save_models(model_classification, model_regression, save_directory_epoch, sleep_onset_threshold, awake_threshold, window_size_minutes)\n","\n","        # plot the losses for the regression model\n","        filename = 'regression_losses.png'\n","        title = 'Training and validation losses during regression with window size of ' + str(window_size_minutes) + ' minutes'\n","        plot_losses(train_losses_reg, test_losses_reg, title,\n","                    ifplot, ifsaveplots,\n","                    save_directory = save_directory_epoch, filename = filename)\n","\n","        # Add results to dataframe\n","        ifconverged = 0\n","        if epoch + 1 == total_epochs:\n","            ifconverged = 1\n","\n","        model_parameters['ifconverged'] = ifconverged\n","        model_parameters['initial_sleep_onset_threshold'] = initial_sleep_onset_threshold\n","\n","        # joing two dictionaries\n","        test_metrics = {**model_parameters, **test_metrics_class,  **participant_dict, **test_metrics_reg}\n","        # populate the results_df dataframe with the results from the current epoch\n","        new_row = pd.DataFrame(test_metrics, index = [epoch])\n","\n","        # Open the results dataframe and add the new results\n","        results_df_all = pd.read_csv(os.path.join(results_saving_dir, 'results.csv'))\n","        results_df_all = pd.concat([results_df_all, new_row], ignore_index=True)\n","        results_df_all.to_csv(os.path.join(results_saving_dir, 'results.csv'), index=False)\n","        del results_df_all\n","        del new_row\n","        del test_metrics\n","        del train_loader\n","\n","    # flatten the list of lists\n","    test_losses_reg_all = [item for sublist in test_losses_reg_all for item in sublist]\n","    train_losses_reg_all = [item for sublist in train_losses_reg_all for item in sublist]\n","\n","    # plot the losses across all epochs\n","    filename = 'all_regression_losses.png'\n","    title = 'Training and validation losses during all regression with window size of ' + str(window_size_minutes) + ' minutes'\n","    plot_losses(train_losses_reg_all, test_losses_reg_all, title,\n","                ifplot, ifsaveplots,\n","                save_directory = save_directory, filename = filename)\n","\n","\n","    # Clean up the cuda memory\n","    torch.cuda.empty_cache()\n","    # close all the plots\n","    plt.close('all')\n","    # clean up the memory\n","    gc.collect()\n","\n","    del mydata_train, mydata_test\n","\n","\n","\n"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"8PMRL5JIVJpI","executionInfo":{"status":"ok","timestamp":1693954108606,"user_tz":-60,"elapsed":5,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["selected_participants = np.array(selected_participants)"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1693954108606,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"},"user_tz":-60},"id":"Xc85VmglVJpI","outputId":"4682478d-3295-499a-f7f2-c69d5c1e4b3d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 323,  694,  912, 1476, 2040, 2651, 2780, 3920, 4014, 4190, 4437,\n","       5749, 5782, 5838, 5876, 6454,   10, 2574, 5489, 5881])"]},"metadata":{},"execution_count":37}],"source":["selected_participants"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":444},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1693954108606,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"},"user_tz":-60},"id":"cX1eWnpgVJpJ","outputId":"803960e9-13a4-4d3f-bf11-ebce4d0be1f7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["      Label  Sbj_ID  Age  Gender  Race1  Race2  Race3  Race4  ifCleanOnset  \\\n","0      29.9     323   79       1    1.0    0.0    0.0    0.0             1   \n","1      29.8     323   79       1    1.0    0.0    0.0    0.0             1   \n","2      29.7     323   79       1    1.0    0.0    0.0    0.0             1   \n","3      29.6     323   79       1    1.0    0.0    0.0    0.0             1   \n","4      29.5     323   79       1    1.0    0.0    0.0    0.0             1   \n","...     ...     ...  ...     ...    ...    ...    ...    ...           ...   \n","5995    0.4    5881   62       0    0.0    1.0    0.0    0.0             1   \n","5996    0.3    5881   62       0    0.0    1.0    0.0    0.0             1   \n","5997    0.2    5881   62       0    0.0    1.0    0.0    0.0             1   \n","5998    0.1    5881   62       0    0.0    1.0    0.0    0.0             1   \n","5999    0.0    5881   62       0    0.0    1.0    0.0    0.0             1   \n","\n","      Time2Sleep  ...  wrseltr5  slpapnea5  cpap5  dntaldv5  uvula5  insmnia5  \\\n","0           68.5  ...      -1.0        0.0    0.0       0.0     0.0       0.0   \n","1           68.5  ...      -1.0        0.0    0.0       0.0     0.0       0.0   \n","2           68.5  ...      -1.0        0.0    0.0       0.0     0.0       0.0   \n","3           68.5  ...      -1.0        0.0    0.0       0.0     0.0       0.0   \n","4           68.5  ...      -1.0        0.0    0.0       0.0     0.0       0.0   \n","...          ...  ...       ...        ...    ...       ...     ...       ...   \n","5995        58.5  ...      -1.0        0.0    0.0       0.0     0.0       0.0   \n","5996        58.5  ...      -1.0        0.0    0.0       0.0     0.0       0.0   \n","5997        58.5  ...      -1.0        0.0    0.0       0.0     0.0       0.0   \n","5998        58.5  ...      -1.0        0.0    0.0       0.0     0.0       0.0   \n","5999        58.5  ...      -1.0        0.0    0.0       0.0     0.0       0.0   \n","\n","      rstlesslgs5  whiirs5c  epslpscl5c  hoostmeq5c  \n","0             0.0       8.0        11.0        15.0  \n","1             0.0       8.0        11.0        15.0  \n","2             0.0       8.0        11.0        15.0  \n","3             0.0       8.0        11.0        15.0  \n","4             0.0       8.0        11.0        15.0  \n","...           ...       ...         ...         ...  \n","5995          0.0      11.0         2.0        15.0  \n","5996          0.0      11.0         2.0        15.0  \n","5997          0.0      11.0         2.0        15.0  \n","5998          0.0      11.0         2.0        15.0  \n","5999          0.0      11.0         2.0        15.0  \n","\n","[6000 rows x 90 columns]"],"text/html":["\n","  <div id=\"df-990bee45-d062-4309-8a17-4dfae9d98d43\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Label</th>\n","      <th>Sbj_ID</th>\n","      <th>Age</th>\n","      <th>Gender</th>\n","      <th>Race1</th>\n","      <th>Race2</th>\n","      <th>Race3</th>\n","      <th>Race4</th>\n","      <th>ifCleanOnset</th>\n","      <th>Time2Sleep</th>\n","      <th>...</th>\n","      <th>wrseltr5</th>\n","      <th>slpapnea5</th>\n","      <th>cpap5</th>\n","      <th>dntaldv5</th>\n","      <th>uvula5</th>\n","      <th>insmnia5</th>\n","      <th>rstlesslgs5</th>\n","      <th>whiirs5c</th>\n","      <th>epslpscl5c</th>\n","      <th>hoostmeq5c</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>29.9</td>\n","      <td>323</td>\n","      <td>79</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>68.5</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8.0</td>\n","      <td>11.0</td>\n","      <td>15.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>29.8</td>\n","      <td>323</td>\n","      <td>79</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>68.5</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8.0</td>\n","      <td>11.0</td>\n","      <td>15.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>29.7</td>\n","      <td>323</td>\n","      <td>79</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>68.5</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8.0</td>\n","      <td>11.0</td>\n","      <td>15.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>29.6</td>\n","      <td>323</td>\n","      <td>79</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>68.5</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8.0</td>\n","      <td>11.0</td>\n","      <td>15.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>29.5</td>\n","      <td>323</td>\n","      <td>79</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>68.5</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8.0</td>\n","      <td>11.0</td>\n","      <td>15.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5995</th>\n","      <td>0.4</td>\n","      <td>5881</td>\n","      <td>62</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>58.5</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>2.0</td>\n","      <td>15.0</td>\n","    </tr>\n","    <tr>\n","      <th>5996</th>\n","      <td>0.3</td>\n","      <td>5881</td>\n","      <td>62</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>58.5</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>2.0</td>\n","      <td>15.0</td>\n","    </tr>\n","    <tr>\n","      <th>5997</th>\n","      <td>0.2</td>\n","      <td>5881</td>\n","      <td>62</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>58.5</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>2.0</td>\n","      <td>15.0</td>\n","    </tr>\n","    <tr>\n","      <th>5998</th>\n","      <td>0.1</td>\n","      <td>5881</td>\n","      <td>62</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>58.5</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>2.0</td>\n","      <td>15.0</td>\n","    </tr>\n","    <tr>\n","      <th>5999</th>\n","      <td>0.0</td>\n","      <td>5881</td>\n","      <td>62</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>58.5</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>2.0</td>\n","      <td>15.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>6000 rows × 90 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-990bee45-d062-4309-8a17-4dfae9d98d43')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-990bee45-d062-4309-8a17-4dfae9d98d43 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-990bee45-d062-4309-8a17-4dfae9d98d43');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-119766a0-2fc0-4f4d-8b8b-37c0124e2a98\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-119766a0-2fc0-4f4d-8b8b-37c0124e2a98')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-119766a0-2fc0-4f4d-8b8b-37c0124e2a98 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":38}],"source":["selected_participants_data"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"Us8gQFtbVJpJ","executionInfo":{"status":"ok","timestamp":1693954108606,"user_tz":-60,"elapsed":4,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["convergence_points =[4]"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"kG0FK-IhVJpJ","executionInfo":{"status":"ok","timestamp":1693954108606,"user_tz":-60,"elapsed":3,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["convergence_points = [4, 6, 8, 10, 12, 14]\n","convergence_starting_points4 = [2]\n","convergence_starting_points6 = [2, 4]\n","convergence_starting_points8 = [2, 4, 6]\n","convergence_starting_points10_14 = [2, 4, 6, 8]\n","convergence_starting_points = {4: convergence_starting_points4, 6: convergence_starting_points6, 8: convergence_starting_points8, 10: convergence_starting_points10_14, 12: convergence_starting_points10_14, 14: convergence_starting_points10_14}\n","\n","window_size2 = [0.5, 1, 1.5]\n","window_size4 = [0.5, 1, 2, 3]\n","window_size6_8= [0.5, 1, 2, 3, 4, 5]\n","starting_window_size = {4: window_size4, 6: window_size6_8, 8: window_size6_8}"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"f41rOmEXVJpJ","executionInfo":{"status":"ok","timestamp":1693954108606,"user_tz":-60,"elapsed":3,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["def hyperparameter_tuning_loop(mydata, convergence_starting_points_dict, starting_window_size_dict,\n","                            resampling_method = 'undersampling', results_saving_dir = 'regression_head_rnn_schedule_learning',\n","                            selected_participants = None, selected_participants_data = None, ifresetresults = False):\n","\n","    if not os.path.exists(results_saving_dir):\n","        os.makedirs(results_saving_dir)\n","\n","    # Initialize a DataFrame to store results\n","        # Initialize a DataFrame to store results\n","    columns = ['convergence_point', 'initial_sleep_onset_threshold','ifconverged', 'pre_sleep_window', 'awake_window', 'pos_class_weight',\n","                'sliding_window_size', 'Accuracy', 'precision', 'recall', 'f1_weighted',\n","                'f1_macro', 'f1_pre_sleep', 'f1_awake', 'auc',\n","                'one_crossing_percentage', 'mean_time_of_prediction',\n","                'std_time_of_prediction', 'one_crossing_percentage_smoothed',\n","                'mean_time_of_prediction_smoothed', 'std_time_of_prediction_smoothed',\n","                'MAE', 'MSE', 'RMSE', 'R2', 'custom_MSE']\n","\n","    if ifresetresults:\n","        results_df_all = pd.DataFrame(columns=columns)\n","        results_df_all.to_csv(os.path.join(results_saving_dir, 'results.csv'))\n","\n","\n","    if torch.cuda.is_available():\n","        device = torch.device(\"cuda\")\n","    else:\n","        device = torch.device('cpu')\n","    print(\"Using {}.\".format(device))\n","    for convergence_point in list(convergence_starting_points_dict.keys()):\n","        for starting_point in convergence_starting_points_dict[convergence_point]:\n","            window_sizes = starting_window_size_dict[starting_point]\n","            for window_size in window_sizes:\n","\n","                gc.collect()\n","                torch.cuda.empty_cache()\n","\n","                experiment_dir = os.path.join(results_saving_dir, f'convergence_point_{convergence_point}_starting_point_{starting_point}_window_size_{window_size}')\n","                if not os.path.exists(experiment_dir):\n","                    os.makedirs(experiment_dir)\n","\n","                print('Convergence point: ', convergence_point)\n","                print('Starting point: ', starting_point)\n","                print('Window size: ', window_size)\n","                training_loop_RNN_regclass_schedule(mydata, convergence_point, total_epochs = 3, filling_limit = 40, method = 'LOCF', ifclassweights = 1, input_size = 86,\n","                                            num_epochs = 10, hidden_size = 128, num_layers = 2, dropout = 0, l2 = 0, lr = 0.001, weight_decay = 0.001, batch_size = 64,\n","                                            device = device, random_seed = 42, window_size_minutes = window_size, ifprobabilitiesanalysis = 1,\n","                                            if_stratified_sampling = 1, initial_awake_threshold = 30, initial_sleep_onset_threshold = starting_point,\n","                                            convergence_sensitivity = 0.05, resampling_method = resampling_method, ifplot = False, ifsaveplots = True,\n","                                            output_path = experiment_dir, shuffle_train_loader = True, results_saving_dir = results_saving_dir, selected_participants_data = selected_participants_data,\n","                                            selected_participants = selected_participants, ifresetresults = False)\n","\n"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"8qZ5bEfgVJpJ","executionInfo":{"status":"ok","timestamp":1693954109312,"user_tz":-60,"elapsed":709,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["import gc\n","import torch\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1693954109312,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"},"user_tz":-60},"id":"vsFbyf8xai7t","outputId":"77d53f99-800c-4be8-8fb4-4cd096b26c41"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":43}],"source":["torch.cuda.is_available()"]},{"cell_type":"markdown","metadata":{"id":"WrpCngQKVJpJ"},"source":["## Try and see if the loop works"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":687},"executionInfo":{"elapsed":55773,"status":"error","timestamp":1693591848985,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"},"user_tz":-60},"id":"Qt2augi5VJpJ","outputId":"286cf401-39a7-4815-be8a-54b601b0cb99"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda.\n","Convergence point:  4\n","Starting point:  2\n","Window size:  0.5\n","New sleep onset threshold:  2.7\n","New awake threshold:  21.3\n","--------------------------------------Evaluate classification ----------------------------------------\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-796abfc392c9>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstarting_window_size_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfolder_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/sleep onset datasets/regression_head_rnn_schedule_learning_test'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mhyperparameter_tuning_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvergence_starting_points_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarting_window_size_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresampling_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'undersampling'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_saving_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfolder_name\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mselected_participants\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselected_participants\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_participants_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselected_participants_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mifresetresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-41-cdb3c3a52791>\u001b[0m in \u001b[0;36mhyperparameter_tuning_loop\u001b[0;34m(mydata, convergence_starting_points_dict, starting_window_size_dict, resampling_method, results_saving_dir, selected_participants, selected_participants_data, ifresetresults)\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Starting point: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarting_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Window size: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 training_loop_RNN_regclass_schedule(mydata, convergence_point, total_epochs = 3, filling_limit = 40, method = 'LOCF', ifclassweights = 1, input_size = 86,\n\u001b[0m\u001b[1;32m     44\u001b[0m                                             \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                                             \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size_minutes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mifprobabilitiesanalysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-35-3f3f7e4ffd02>\u001b[0m in \u001b[0;36mtraining_loop_RNN_regclass_schedule\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# Get the stats for probability change points and mean prediction times in participants on the validaiton set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         _,  participant_dict = test_on_random_participants(data = mydata_test, model =  model_classification, criterion = criterion_class, random_seed = random_seed,\n\u001b[0m\u001b[1;32m    210\u001b[0m                                                                                             \u001b[0mifplot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path_new\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0msave_directory_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                                                                                             \u001b[0mconvergence_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvergence_point\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-27-dd53258434d0>\u001b[0m in \u001b[0;36mtest_on_random_participants\u001b[0;34m(data, model, criterion, random_seed, ifplot, device, output_path_new, index, convergence_point, awake_window, pre_sleep_window, ifsaveplots, window_size_minutes, ifmissing, method, resampling_method, filling_limit)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mtest_loader_sbj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset_sbj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactuals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader_sbj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mifprobabilities\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mtimeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_end_points_sbj\u001b[0m \u001b[0;31m#participant_data['Old_label']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mparticipant_dataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msbj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_loader_sbj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-486bfce4a993>\u001b[0m in \u001b[0;36mtest_classification\u001b[0;34m(model, test_loader, device, criterion, index, threshold, ifprobabilities, ifplot)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_and_endpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# these are logits now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-f7b2a71dc3f2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mc0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m        \u001b[0;31m#prob = self.sigmoid(out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    813\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    814\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import gc\n","import torch\n","gc.collect()\n","torch.cuda.empty_cache()\n","convergence_starting_points_test = {4:[2]}\n","starting_window_size_test = {2:[0.5, 1]}\n","folder_name = '/content/drive/MyDrive/sleep onset datasets/regression_head_rnn_schedule_learning_test'\n","hyperparameter_tuning_loop(data_train, convergence_starting_points_test, starting_window_size_test, resampling_method = 'undersampling', results_saving_dir = folder_name , selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = True)\n"]},{"cell_type":"markdown","metadata":{"id":"4PFX09CVVJpJ"},"source":["## Hypereparameter tune for window selection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_-aZwUg7VJpJ"},"outputs":[],"source":["convergence_points = [4, 6, 8, 10, 12, 14]\n","convergence_starting_points4 = [2]\n","convergence_starting_points6 = [2, 4]\n","convergence_starting_points8 = [2, 4, 6]\n","convergence_starting_points10_14 = [2, 4, 6, 8]\n","convergence_starting_points = {4: convergence_starting_points4, 6: convergence_starting_points6, 8: convergence_starting_points8, 10: convergence_starting_points10_14, 12: convergence_starting_points10_14, 14: convergence_starting_points10_14}\n","\n","window_size2 = [0.5, 1, 1.5]\n","window_size4 = [0.5, 1, 2, 3]\n","window_size6_8= [0.5, 1, 2, 3, 4, 5]\n","starting_window_size = {2:window_size2, 4: window_size4, 6: window_size6_8, 8: window_size6_8}"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"y2bexW4JJKNt","executionInfo":{"status":"ok","timestamp":1693915164556,"user_tz":-60,"elapsed":10,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["folder_name = '/content/drive/MyDrive/sleep onset datasets/FIXED_regression_head_rnn_schedule_learning_perfectly_clean_onset'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N9eMy08NVJpJ"},"outputs":[],"source":["\n","convergence_starting_points = {4: [2]}\n","window_size2 = [0.5, 1, 1.5]\n","starting_window_size = {2:window_size2}\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lo-4wccK2L9y"},"outputs":[],"source":["\n","convergence_starting_points = {6: [2]}\n","window_size2 = [0.5, 1, 1.5]\n","starting_window_size = {2:window_size2}\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"]},{"cell_type":"code","source":["\n","convergence_starting_points = {6: [2]}\n","window_size2 = [1.5]\n","starting_window_size = {2:window_size2}\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"],"metadata":{"id":"Z4tIuYPD_Qy8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O_E2aGee2Syv"},"outputs":[],"source":["\n","convergence_starting_points = {6: [4]}\n","window_size4 = [0.5, 1, 2, 3]\n","starting_window_size = {4:window_size4}\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OcBTSiV07afE"},"outputs":[],"source":["convergence_starting_points = {8: [2]}\n","window_size2 = [0.5, 1, 1.5]\n","starting_window_size = {2:window_size2}\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tR96qZ5h7riA"},"outputs":[],"source":["\n","convergence_starting_points = {8: [4]}\n","window_size4 = [0.5, 1, 2, 3]\n","starting_window_size = {4:window_size4}\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iL6zuFs47yvn"},"outputs":[],"source":["\n","convergence_starting_points = {8: [6]}\n","window_size6_8= [0.5, 1, 2, 3, 4, 5]\n","starting_window_size = {6:window_size6_8}\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"]},{"cell_type":"code","source":["\n","convergence_starting_points = {8: [6]}\n","window_size6_8= [ 2, 3, 4, 5]\n","starting_window_size = {6:window_size6_8}\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"],"metadata":{"id":"ttyO1fYV9phT"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Avb3utd9n0r0"},"outputs":[],"source":["\n","convergence_starting_points = {8: [6]}\n","window_size6_8= [ 4, 5]\n","starting_window_size = {6:window_size6_8}\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YAmt5rQQXeHW"},"outputs":[],"source":["\n","convergence_starting_points = {12: [2, 4]}\n","window_size2 = [0.5, 1, 1.5]\n","window_size4 = [0.5, 1, 2, 3]\n","starting_window_size = {2:window_size2, 4: window_size4}\n","\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3085037,"status":"ok","timestamp":1693594950317,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"},"user_tz":-60},"id":"05pq301SgKVx","outputId":"0be06154-90df-4c25-9864-f8aaa8d61d63"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda.\n","Convergence point:  12\n","Starting point:  4\n","Window size:  2\n","New sleep onset threshold:  6.7\n","New awake threshold:  24.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 55.81395348837209%\n","Mean time of prediction for participants with only one crossing: 17.508333333333336 minutes\n","STD of time of prediction for participants with only one crossing: 7.229391206887494 minutes\n","Percentage of participants with only one crossing (smoothed): 65.11627906976744%\n","Mean time of prediction for participants with only one crossing (smoothed): 16.860714285714284 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.466376324096661 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 16.354545454545455 minutes\n","STD of time of prediction for participants with only one crossing: 6.48429762599461 minutes\n","Percentage of participants with only one crossing (smoothed): 50.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.599999999999998 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.335613624582863 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  9.4\n","New awake threshold:  18.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 55.81395348837209%\n","Mean time of prediction for participants with only one crossing: 16.520833333333332 minutes\n","STD of time of prediction for participants with only one crossing: 6.552351942029841 minutes\n","Percentage of participants with only one crossing (smoothed): 53.48837209302325%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.395652173913039 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.899833694499914 minutes\n","Percentage of participants with only one crossing: 65.0%\n","Mean time of prediction for participants with only one crossing: 16.353846153846153 minutes\n","STD of time of prediction for participants with only one crossing: 5.865363177766624 minutes\n","Percentage of participants with only one crossing (smoothed): 65.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 16.39230769230769 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.678145935519429 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  12\n","New awake threshold:  12\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 67.44186046511628%\n","Mean time of prediction for participants with only one crossing: 15.36896551724138 minutes\n","STD of time of prediction for participants with only one crossing: 7.188741362583668 minutes\n","Percentage of participants with only one crossing (smoothed): 62.7906976744186%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.496296296296293 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.911851470419947 minutes\n","Percentage of participants with only one crossing: 60.0%\n","Mean time of prediction for participants with only one crossing: 15.691666666666665 minutes\n","STD of time of prediction for participants with only one crossing: 6.804466466144785 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.745454545454544 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.606224403831486 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  12\n","Starting point:  4\n","Window size:  3\n","New sleep onset threshold:  6.7\n","New awake threshold:  24.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 55.81395348837209%\n","Mean time of prediction for participants with only one crossing: 14.758333333333335 minutes\n","STD of time of prediction for participants with only one crossing: 6.711427857683407 minutes\n","Percentage of participants with only one crossing (smoothed): 46.51162790697674%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.574999999999998 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.860894620965986 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 14.545454545454549 minutes\n","STD of time of prediction for participants with only one crossing: 5.143285762015568 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.427272727272724 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.97861201725723 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  9.4\n","New awake threshold:  18.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 62.7906976744186%\n","Mean time of prediction for participants with only one crossing: 13.692592592592591 minutes\n","STD of time of prediction for participants with only one crossing: 5.873539277882972 minutes\n","Percentage of participants with only one crossing (smoothed): 58.139534883720934%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.379999999999997 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.26239570771442 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 16.40909090909091 minutes\n","STD of time of prediction for participants with only one crossing: 6.37686935807198 minutes\n","Percentage of participants with only one crossing (smoothed): 50.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.499999999999996 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.273518749374084 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  12\n","New awake threshold:  12\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 69.76744186046511%\n","Mean time of prediction for participants with only one crossing: 13.91333333333333 minutes\n","STD of time of prediction for participants with only one crossing: 5.516262825097763 minutes\n","Percentage of participants with only one crossing (smoothed): 69.76744186046511%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.999999999999996 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.290998860806341 minutes\n","Percentage of participants with only one crossing: 60.0%\n","Mean time of prediction for participants with only one crossing: 13.200000000000001 minutes\n","STD of time of prediction for participants with only one crossing: 5.281729514720218 minutes\n","Percentage of participants with only one crossing (smoothed): 60.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.991666666666665 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 4.43686419545256 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n"]}],"source":["\n","convergence_starting_points = {12: [4]}\n","\n","window_size4 = [2, 3]\n","starting_window_size = {4: window_size4}\n","\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LOG5tGxUGfor","outputId":"356eb5a4-1768-4e07-f439-2eb769cb94d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda.\n","Convergence point:  10\n","Starting point:  2\n","Window size:  0.5\n","New sleep onset threshold:  4.7\n","New awake threshold:  23.3\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 72.09302325581395%\n","Mean time of prediction for participants with only one crossing: 10.66774193548387 minutes\n","STD of time of prediction for participants with only one crossing: 6.34331688430375 minutes\n","Percentage of participants with only one crossing (smoothed): 58.139534883720934%\n","Mean time of prediction for participants with only one crossing (smoothed): 9.819999999999999 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.701368256830986 minutes\n","Percentage of participants with only one crossing: 40.0%\n","Mean time of prediction for participants with only one crossing: 12.7125 minutes\n","STD of time of prediction for participants with only one crossing: 5.498053632877729 minutes\n","Percentage of participants with only one crossing (smoothed): 50.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.45 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.615323121359984 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  7.4\n","New awake threshold:  16.6\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 65.11627906976744%\n","Mean time of prediction for participants with only one crossing: 12.535714285714286 minutes\n","STD of time of prediction for participants with only one crossing: 6.505449050368374 minutes\n","Percentage of participants with only one crossing (smoothed): 58.139534883720934%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.419999999999998 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.587629193137283 minutes\n","Percentage of participants with only one crossing: 70.0%\n","Mean time of prediction for participants with only one crossing: 14.121428571428572 minutes\n","STD of time of prediction for participants with only one crossing: 5.821533741381288 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.018181818181818 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 4.578967367077465 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10\n","New awake threshold:  10\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 69.76744186046511%\n","Mean time of prediction for participants with only one crossing: 10.6 minutes\n","STD of time of prediction for participants with only one crossing: 6.553116307020145 minutes\n","Percentage of participants with only one crossing (smoothed): 69.76744186046511%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.099999999999996 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.702487100571945 minutes\n","Percentage of participants with only one crossing: 65.0%\n","Mean time of prediction for participants with only one crossing: 13.546153846153844 minutes\n","STD of time of prediction for participants with only one crossing: 6.997319858853144 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.018181818181818 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.31832298000501 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  10\n","Starting point:  2\n","Window size:  1\n","New sleep onset threshold:  4.7\n","New awake threshold:  23.3\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 65.11627906976744%\n","Mean time of prediction for participants with only one crossing: 10.903571428571428 minutes\n","STD of time of prediction for participants with only one crossing: 5.733079585232726 minutes\n","Percentage of participants with only one crossing (smoothed): 65.11627906976744%\n","Mean time of prediction for participants with only one crossing (smoothed): 9.789285714285713 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.336956013477712 minutes\n","Percentage of participants with only one crossing: 60.0%\n","Mean time of prediction for participants with only one crossing: 12.016666666666667 minutes\n","STD of time of prediction for participants with only one crossing: 4.58181793129709 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 9.563636363636364 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 4.834021989507424 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  7.4\n","New awake threshold:  16.6\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 51.162790697674424%\n","Mean time of prediction for participants with only one crossing: 11.568181818181817 minutes\n","STD of time of prediction for participants with only one crossing: 5.298746538194959 minutes\n","Percentage of participants with only one crossing (smoothed): 53.48837209302325%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.504347826086953 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.578957223810195 minutes\n","Percentage of participants with only one crossing: 50.0%\n","Mean time of prediction for participants with only one crossing: 10.579999999999998 minutes\n","STD of time of prediction for participants with only one crossing: 4.746956919964621 minutes\n","Percentage of participants with only one crossing (smoothed): 40.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.325 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.406882188470542 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10\n","New awake threshold:  10\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 58.139534883720934%\n","Mean time of prediction for participants with only one crossing: 12.920000000000002 minutes\n","STD of time of prediction for participants with only one crossing: 5.96409255461382 minutes\n","Percentage of participants with only one crossing (smoothed): 58.139534883720934%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.439999999999998 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.285093475836297 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 12.909090909090908 minutes\n","STD of time of prediction for participants with only one crossing: 6.160055275055211 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.063636363636364 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.090503230575133 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  10\n","Starting point:  2\n","Window size:  1.5\n","New sleep onset threshold:  4.7\n","New awake threshold:  23.3\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 48.837209302325576%\n","Mean time of prediction for participants with only one crossing: 12.98095238095238 minutes\n","STD of time of prediction for participants with only one crossing: 6.836500012894306 minutes\n","Percentage of participants with only one crossing (smoothed): 55.81395348837209%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.345833333333331 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.783065873105531 minutes\n","Percentage of participants with only one crossing: 50.0%\n","Mean time of prediction for participants with only one crossing: 11.11 minutes\n","STD of time of prediction for participants with only one crossing: 4.035950941228102 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.018181818181818 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.32691590489762 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  7.4\n","New awake threshold:  16.6\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 51.162790697674424%\n","Mean time of prediction for participants with only one crossing: 12.35 minutes\n","STD of time of prediction for participants with only one crossing: 5.232915926222119 minutes\n","Percentage of participants with only one crossing (smoothed): 53.48837209302325%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.199999999999996 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.585557320181134 minutes\n","Percentage of participants with only one crossing: 60.0%\n","Mean time of prediction for participants with only one crossing: 11.125000000000002 minutes\n","STD of time of prediction for participants with only one crossing: 5.1265851207212 minutes\n","Percentage of participants with only one crossing (smoothed): 60.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.950000000000001 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.246030245179047 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10\n","New awake threshold:  10\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 60.46511627906976%\n","Mean time of prediction for participants with only one crossing: 9.580769230769231 minutes\n","STD of time of prediction for participants with only one crossing: 4.794870595091034 minutes\n","Percentage of participants with only one crossing (smoothed): 62.7906976744186%\n","Mean time of prediction for participants with only one crossing (smoothed): 9.18148148148148 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.076236360039827 minutes\n","Percentage of participants with only one crossing: 60.0%\n","Mean time of prediction for participants with only one crossing: 10.583333333333334 minutes\n","STD of time of prediction for participants with only one crossing: 5.429216231546584 minutes\n","Percentage of participants with only one crossing (smoothed): 60.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.033333333333333 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.653111218737358 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  10\n","Starting point:  4\n","Window size:  0.5\n","New sleep onset threshold:  6.0\n","New awake threshold:  23.3\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 60.46511627906976%\n","Mean time of prediction for participants with only one crossing: 11.507692307692308 minutes\n","STD of time of prediction for participants with only one crossing: 6.567221814958279 minutes\n","Percentage of participants with only one crossing (smoothed): 65.11627906976744%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.521428571428569 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.185320210722153 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 12.20909090909091 minutes\n","STD of time of prediction for participants with only one crossing: 5.119917354704885 minutes\n","Percentage of participants with only one crossing (smoothed): 60.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.616666666666665 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.051285446544028 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  8.0\n","New awake threshold:  16.6\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 67.44186046511628%\n","Mean time of prediction for participants with only one crossing: 12.068965517241379 minutes\n","STD of time of prediction for participants with only one crossing: 6.017050683669319 minutes\n","Percentage of participants with only one crossing (smoothed): 60.46511627906976%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.87307692307692 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.576388867193329 minutes\n","Percentage of participants with only one crossing: 65.0%\n","Mean time of prediction for participants with only one crossing: 11.100000000000001 minutes\n","STD of time of prediction for participants with only one crossing: 5.297023837822663 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.518181818181818 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 4.228005801539449 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10\n","New awake threshold:  10\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 55.81395348837209%\n","Mean time of prediction for participants with only one crossing: 10.6875 minutes\n","STD of time of prediction for participants with only one crossing: 5.694830440847207 minutes\n","Percentage of participants with only one crossing (smoothed): 60.46511627906976%\n","Mean time of prediction for participants with only one crossing (smoothed): 9.142307692307693 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.803392447984665 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 9.899999999999999 minutes\n","STD of time of prediction for participants with only one crossing: 3.683377896343617 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 9.836363636363636 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.551784762541392 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  10\n","Starting point:  4\n","Window size:  1\n","New sleep onset threshold:  6.0\n","New awake threshold:  23.3\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 62.7906976744186%\n","Mean time of prediction for participants with only one crossing: 14.385185185185184 minutes\n","STD of time of prediction for participants with only one crossing: 7.153227901454762 minutes\n","Percentage of participants with only one crossing (smoothed): 60.46511627906976%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.526923076923074 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.307717611292224 minutes\n","Percentage of participants with only one crossing: 50.0%\n","Mean time of prediction for participants with only one crossing: 10.81 minutes\n","STD of time of prediction for participants with only one crossing: 2.5200992043965256 minutes\n","Percentage of participants with only one crossing (smoothed): 40.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.325 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 2.161452058223823 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  8.0\n","New awake threshold:  16.6\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 72.09302325581395%\n","Mean time of prediction for participants with only one crossing: 11.993548387096773 minutes\n","STD of time of prediction for participants with only one crossing: 6.616935041575735 minutes\n","Percentage of participants with only one crossing (smoothed): 67.44186046511628%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.92413793103448 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.760159923246755 minutes\n","Percentage of participants with only one crossing: 60.0%\n","Mean time of prediction for participants with only one crossing: 11.258333333333333 minutes\n","STD of time of prediction for participants with only one crossing: 6.058665740536901 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 9.836363636363636 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.032128182944745 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10\n","New awake threshold:  10\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 65.11627906976744%\n","Mean time of prediction for participants with only one crossing: 10.985714285714286 minutes\n","STD of time of prediction for participants with only one crossing: 6.537130993995229 minutes\n","Percentage of participants with only one crossing (smoothed): 51.162790697674424%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.336363636363632 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.7993679822964195 minutes\n"]}],"source":["convergence_starting_points = {10: [2, 4]}\n","window_size2 = [0.5, 1, 1.5]\n","window_size4 = [0.5, 1, 2, 3]\n","starting_window_size = {2:window_size2, 4: window_size4}\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)"]},{"cell_type":"code","source":["convergence_starting_points = {10: [4]}\n","\n","window_size4 = [1, 2, 3]\n","starting_window_size = { 4: window_size4}\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)"],"metadata":{"id":"keZkaK8D-Mdf","executionInfo":{"status":"ok","timestamp":1693619358185,"user_tz":-60,"elapsed":6091566,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"32c1ce47-e63f-48f9-8d57-5456e16f9a15"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda.\n","Convergence point:  10\n","Starting point:  4\n","Window size:  1\n","New sleep onset threshold:  6.0\n","New awake threshold:  23.3\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 62.7906976744186%\n","Mean time of prediction for participants with only one crossing: 14.02962962962963 minutes\n","STD of time of prediction for participants with only one crossing: 7.722828824207549 minutes\n","Percentage of participants with only one crossing (smoothed): 51.162790697674424%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.995454545454542 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.418473145590091 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 11.681818181818182 minutes\n","STD of time of prediction for participants with only one crossing: 4.633243156662548 minutes\n","Percentage of participants with only one crossing (smoothed): 60.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.491666666666665 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 4.399139751764605 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  8.0\n","New awake threshold:  16.6\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 55.81395348837209%\n","Mean time of prediction for participants with only one crossing: 10.875 minutes\n","STD of time of prediction for participants with only one crossing: 4.383040231011651 minutes\n","Percentage of participants with only one crossing (smoothed): 67.44186046511628%\n","Mean time of prediction for participants with only one crossing (smoothed): 9.665517241379309 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 4.968748948354274 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 9.790909090909093 minutes\n","STD of time of prediction for participants with only one crossing: 4.482988967490845 minutes\n","Percentage of participants with only one crossing (smoothed): 65.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.46923076923077 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.796601738603087 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10\n","New awake threshold:  10\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 60.46511627906976%\n","Mean time of prediction for participants with only one crossing: 10.303846153846154 minutes\n","STD of time of prediction for participants with only one crossing: 5.8657730010125935 minutes\n","Percentage of participants with only one crossing (smoothed): 48.837209302325576%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.866666666666664 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.527110739306833 minutes\n","Percentage of participants with only one crossing: 60.0%\n","Mean time of prediction for participants with only one crossing: 12.475000000000001 minutes\n","STD of time of prediction for participants with only one crossing: 6.404051452010672 minutes\n","Percentage of participants with only one crossing (smoothed): 60.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.408333333333331 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.832698141209582 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  10\n","Starting point:  4\n","Window size:  2\n","New sleep onset threshold:  6.0\n","New awake threshold:  23.3\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 53.48837209302325%\n","Mean time of prediction for participants with only one crossing: 14.600000000000003 minutes\n","STD of time of prediction for participants with only one crossing: 6.8623737812103665 minutes\n","Percentage of participants with only one crossing (smoothed): 37.2093023255814%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.16875 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.145294821039915 minutes\n","Percentage of participants with only one crossing: 45.0%\n","Mean time of prediction for participants with only one crossing: 10.444444444444445 minutes\n","STD of time of prediction for participants with only one crossing: 1.5549999007582043 minutes\n","Percentage of participants with only one crossing (smoothed): 60.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.033333333333333 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 3.2871804872193366 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  8.0\n","New awake threshold:  16.6\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 48.837209302325576%\n","Mean time of prediction for participants with only one crossing: 12.723809523809523 minutes\n","STD of time of prediction for participants with only one crossing: 5.31779624620009 minutes\n","Percentage of participants with only one crossing (smoothed): 48.837209302325576%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.414285714285711 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.249912033245363 minutes\n","Percentage of participants with only one crossing: 50.0%\n","Mean time of prediction for participants with only one crossing: 11.8 minutes\n","STD of time of prediction for participants with only one crossing: 4.763821994995195 minutes\n","Percentage of participants with only one crossing (smoothed): 50.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.45 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 4.643543905251677 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10\n","New awake threshold:  10\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 48.837209302325576%\n","Mean time of prediction for participants with only one crossing: 12.3 minutes\n","STD of time of prediction for participants with only one crossing: 5.651801229949364 minutes\n","Percentage of participants with only one crossing (smoothed): 55.81395348837209%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.491666666666665 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.253193628503403 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 11.636363636363637 minutes\n","STD of time of prediction for participants with only one crossing: 5.088538409061669 minutes\n","Percentage of participants with only one crossing (smoothed): 40.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.7625 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 3.7950419958150663 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  10\n","Starting point:  4\n","Window size:  3\n","New sleep onset threshold:  6.0\n","New awake threshold:  23.3\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 53.48837209302325%\n","Mean time of prediction for participants with only one crossing: 17.86086956521739 minutes\n","STD of time of prediction for participants with only one crossing: 6.931825729164612 minutes\n","Percentage of participants with only one crossing (smoothed): 62.7906976744186%\n","Mean time of prediction for participants with only one crossing (smoothed): 17.03333333333333 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.16214327532592 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 14.572727272727274 minutes\n","STD of time of prediction for participants with only one crossing: 6.067213065334469 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.109090909090906 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.255245732467568 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  8.0\n","New awake threshold:  16.6\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 74.4186046511628%\n","Mean time of prediction for participants with only one crossing: 13.721875 minutes\n","STD of time of prediction for participants with only one crossing: 6.229151947446377 minutes\n","Percentage of participants with only one crossing (smoothed): 65.11627906976744%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.84285714285714 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.751794923861176 minutes\n","Percentage of participants with only one crossing: 80.0%\n","Mean time of prediction for participants with only one crossing: 14.2625 minutes\n","STD of time of prediction for participants with only one crossing: 6.168860814607506 minutes\n","Percentage of participants with only one crossing (smoothed): 85.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.788235294117646 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.348309858717022 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10\n","New awake threshold:  10\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 55.81395348837209%\n","Mean time of prediction for participants with only one crossing: 12.104166666666666 minutes\n","STD of time of prediction for participants with only one crossing: 6.041624999856321 minutes\n","Percentage of participants with only one crossing (smoothed): 39.53488372093023%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.84705882352941 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.053919865374714 minutes\n","Percentage of participants with only one crossing: 50.0%\n","Mean time of prediction for participants with only one crossing: 12.520000000000001 minutes\n","STD of time of prediction for participants with only one crossing: 6.118790730201516 minutes\n","Percentage of participants with only one crossing (smoothed): 40.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.325 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.243746871871088 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QjJYbaVPGjGh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"301b0f92-15a0-4d06-eb33-87f7c586751b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda.\n","Convergence point:  10\n","Starting point:  6\n","Window size:  0.5\n","New sleep onset threshold:  7.3\n","New awake threshold:  23.3\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 67.44186046511628%\n","Mean time of prediction for participants with only one crossing: 12.486206896551723 minutes\n","STD of time of prediction for participants with only one crossing: 7.205300928790912 minutes\n","Percentage of participants with only one crossing (smoothed): 58.139534883720934%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.719999999999997 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.750525905438775 minutes\n","Percentage of participants with only one crossing: 65.0%\n","Mean time of prediction for participants with only one crossing: 11.615384615384615 minutes\n","STD of time of prediction for participants with only one crossing: 4.595624366025738 minutes\n","Percentage of participants with only one crossing (smoothed): 50.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.85 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.186761995696351 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  8.6\n","New awake threshold:  16.6\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 72.09302325581395%\n","Mean time of prediction for participants with only one crossing: 11.916129032258064 minutes\n","STD of time of prediction for participants with only one crossing: 6.7637600143513215 minutes\n","Percentage of participants with only one crossing (smoothed): 65.11627906976744%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.110714285714282 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.040765952940179 minutes\n","Percentage of participants with only one crossing: 65.0%\n","Mean time of prediction for participants with only one crossing: 13.207692307692307 minutes\n","STD of time of prediction for participants with only one crossing: 5.79740811297621 minutes\n","Percentage of participants with only one crossing (smoothed): 60.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.658333333333333 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.176090771857407 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10\n","New awake threshold:  10\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 51.162790697674424%\n","Mean time of prediction for participants with only one crossing: 11.31818181818182 minutes\n","STD of time of prediction for participants with only one crossing: 4.859726550821593 minutes\n","Percentage of participants with only one crossing (smoothed): 58.139534883720934%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.799999999999997 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.524490926773253 minutes\n","Percentage of participants with only one crossing: 70.0%\n","Mean time of prediction for participants with only one crossing: 13.471428571428573 minutes\n","STD of time of prediction for participants with only one crossing: 6.716337072651236 minutes\n","Percentage of participants with only one crossing (smoothed): 60.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.325000000000001 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.171591501927737 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  10\n","Starting point:  6\n","Window size:  1\n","New sleep onset threshold:  7.3\n","New awake threshold:  23.3\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 55.81395348837209%\n","Mean time of prediction for participants with only one crossing: 13.162500000000001 minutes\n","STD of time of prediction for participants with only one crossing: 6.693579790864278 minutes\n","Percentage of participants with only one crossing (smoothed): 48.837209302325576%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.390476190476187 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.152216090928592 minutes\n","Percentage of participants with only one crossing: 40.0%\n","Mean time of prediction for participants with only one crossing: 9.175 minutes\n","STD of time of prediction for participants with only one crossing: 1.6953981833185972 minutes\n","Percentage of participants with only one crossing (smoothed): 45.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 8.255555555555555 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 2.4882439640620766 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  8.6\n","New awake threshold:  16.6\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 65.11627906976744%\n","Mean time of prediction for participants with only one crossing: 11.050000000000002 minutes\n","STD of time of prediction for participants with only one crossing: 5.670380435712783 minutes\n","Percentage of participants with only one crossing (smoothed): 53.48837209302325%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.917391304347822 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.134762360524079 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 12.090909090909092 minutes\n","STD of time of prediction for participants with only one crossing: 6.779977815122534 minutes\n","Percentage of participants with only one crossing (smoothed): 45.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.144444444444444 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.409387405484259 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10\n","New awake threshold:  10\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 58.139534883720934%\n","Mean time of prediction for participants with only one crossing: 12.164000000000001 minutes\n","STD of time of prediction for participants with only one crossing: 5.21918614345187 minutes\n","Percentage of participants with only one crossing (smoothed): 51.162790697674424%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.81363636363636 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.031563597074468 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 10.336363636363636 minutes\n","STD of time of prediction for participants with only one crossing: 5.176743927021155 minutes\n","Percentage of participants with only one crossing (smoothed): 50.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.7 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.06951674225463 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  10\n","Starting point:  6\n","Window size:  2\n","New sleep onset threshold:  7.3\n","New awake threshold:  23.3\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 51.162790697674424%\n","Mean time of prediction for participants with only one crossing: 14.786363636363639 minutes\n","STD of time of prediction for participants with only one crossing: 7.185863360195834 minutes\n","Percentage of participants with only one crossing (smoothed): 37.2093023255814%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.16875 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.124904724724158 minutes\n","Percentage of participants with only one crossing: 50.0%\n","Mean time of prediction for participants with only one crossing: 13.39 minutes\n","STD of time of prediction for participants with only one crossing: 5.661881312779348 minutes\n","Percentage of participants with only one crossing (smoothed): 50.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.2 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.89915248150105 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  8.6\n","New awake threshold:  16.6\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 58.139534883720934%\n","Mean time of prediction for participants with only one crossing: 15.315999999999999 minutes\n","STD of time of prediction for participants with only one crossing: 6.795862270529032 minutes\n","Percentage of participants with only one crossing (smoothed): 53.48837209302325%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.395652173913039 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.162309456354301 minutes\n","Percentage of participants with only one crossing: 65.0%\n","Mean time of prediction for participants with only one crossing: 14.846153846153848 minutes\n","STD of time of prediction for participants with only one crossing: 7.03875157031693 minutes\n","Percentage of participants with only one crossing (smoothed): 45.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.644444444444444 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.273597428219158 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10\n","New awake threshold:  10\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 65.11627906976744%\n","Mean time of prediction for participants with only one crossing: 13.035714285714286 minutes\n","STD of time of prediction for participants with only one crossing: 6.216580036236867 minutes\n","Percentage of participants with only one crossing (smoothed): 55.81395348837209%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.908333333333331 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.162583107395433 minutes\n","Percentage of participants with only one crossing: 45.0%\n","Mean time of prediction for participants with only one crossing: 11.744444444444447 minutes\n","STD of time of prediction for participants with only one crossing: 4.710062776442018 minutes\n","Percentage of participants with only one crossing (smoothed): 40.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.0125 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.332315983697891 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  10\n","Starting point:  6\n","Window size:  3\n","New sleep onset threshold:  7.3\n","New awake threshold:  23.3\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 65.11627906976744%\n","Mean time of prediction for participants with only one crossing: 16.12857142857143 minutes\n","STD of time of prediction for participants with only one crossing: 6.750018896421017 minutes\n","Percentage of participants with only one crossing (smoothed): 65.11627906976744%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.878571428571425 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.874141875944389 minutes\n","Percentage of participants with only one crossing: 60.0%\n","Mean time of prediction for participants with only one crossing: 16.11666666666667 minutes\n","STD of time of prediction for participants with only one crossing: 6.907222467983944 minutes\n","Percentage of participants with only one crossing (smoothed): 60.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.574999999999996 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.919552610778629 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  8.6\n","New awake threshold:  16.6\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 67.44186046511628%\n","Mean time of prediction for participants with only one crossing: 14.562068965517241 minutes\n","STD of time of prediction for participants with only one crossing: 6.068824449771167 minutes\n","Percentage of participants with only one crossing (smoothed): 67.44186046511628%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.32068965517241 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.567494631232849 minutes\n","Percentage of participants with only one crossing: 60.0%\n","Mean time of prediction for participants with only one crossing: 11.533333333333333 minutes\n","STD of time of prediction for participants with only one crossing: 3.8529930645610504 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.745454545454544 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 4.484592539789304 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10\n","New awake threshold:  10\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 69.76744186046511%\n","Mean time of prediction for participants with only one crossing: 13.91 minutes\n","STD of time of prediction for participants with only one crossing: 6.632412833954169 minutes\n","Percentage of participants with only one crossing (smoothed): 58.139534883720934%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.979999999999997 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.4235192846289495 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 12.52727272727273 minutes\n","STD of time of prediction for participants with only one crossing: 5.088148602227249 minutes\n","Percentage of participants with only one crossing (smoothed): 60.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.658333333333333 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.528179075327507 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  10\n","Starting point:  6\n","Window size:  4\n","New sleep onset threshold:  7.3\n","New awake threshold:  23.3\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 65.11627906976744%\n","Mean time of prediction for participants with only one crossing: 15.514285714285716 minutes\n","STD of time of prediction for participants with only one crossing: 7.291811174466988 minutes\n","Percentage of participants with only one crossing (smoothed): 62.7906976744186%\n","Mean time of prediction for participants with only one crossing (smoothed): 17.088888888888885 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.599139002561206 minutes\n","Percentage of participants with only one crossing: 70.0%\n","Mean time of prediction for participants with only one crossing: 14.442857142857145 minutes\n","STD of time of prediction for participants with only one crossing: 7.051313381786635 minutes\n","Percentage of participants with only one crossing (smoothed): 65.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.699999999999998 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.090079743502944 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  8.6\n","New awake threshold:  16.6\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 62.7906976744186%\n","Mean time of prediction for participants with only one crossing: 14.188888888888892 minutes\n","STD of time of prediction for participants with only one crossing: 7.168208275192848 minutes\n","Percentage of participants with only one crossing (smoothed): 67.44186046511628%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.458620689655168 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.533397696665406 minutes\n","Percentage of participants with only one crossing: 65.0%\n","Mean time of prediction for participants with only one crossing: 14.20769230769231 minutes\n","STD of time of prediction for participants with only one crossing: 6.718340172584015 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.836363636363632 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.217689472560734 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10\n","New awake threshold:  10\n"]}],"source":["convergence_starting_points = {10: [6]}\n","window_size6_8= [0.5, 1, 2, 3, 4]\n","starting_window_size = {6:window_size6_8}\n","gc.collect()\n","torch.cuda.empty_cache()\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"]},{"cell_type":"code","source":["convergence_starting_points = {10: [6]}\n","window_size6_8= [4, 5]\n","starting_window_size = {6:window_size6_8}\n","gc.collect()\n","torch.cuda.empty_cache()\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"],"metadata":{"id":"dD7qbPVnuM3F","executionInfo":{"status":"ok","timestamp":1693662030358,"user_tz":-60,"elapsed":3461574,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9d5ee868-83bd-46c8-991b-45164a273293"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda.\n","Convergence point:  10\n","Starting point:  6\n","Window size:  4\n","New sleep onset threshold:  7.3\n","New awake threshold:  23.3\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 55.81395348837209%\n","Mean time of prediction for participants with only one crossing: 16.133333333333336 minutes\n","STD of time of prediction for participants with only one crossing: 6.8984700397181475 minutes\n","Percentage of participants with only one crossing (smoothed): 51.162790697674424%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.177272727272724 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.031547858704809 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 13.718181818181819 minutes\n","STD of time of prediction for participants with only one crossing: 6.712649955232851 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.381818181818181 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.906099831021345 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  8.6\n","New awake threshold:  16.6\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 62.7906976744186%\n","Mean time of prediction for participants with only one crossing: 14.092592592592595 minutes\n","STD of time of prediction for participants with only one crossing: 7.2033390367567485 minutes\n","Percentage of participants with only one crossing (smoothed): 55.81395348837209%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.824999999999998 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.379941847106023 minutes\n","Percentage of participants with only one crossing: 75.0%\n","Mean time of prediction for participants with only one crossing: 13.100000000000003 minutes\n","STD of time of prediction for participants with only one crossing: 6.1009288910241635 minutes\n","Percentage of participants with only one crossing (smoothed): 65.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.238461538461536 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.461309519753652 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10\n","New awake threshold:  10\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 60.46511627906976%\n","Mean time of prediction for participants with only one crossing: 14.865384615384615 minutes\n","STD of time of prediction for participants with only one crossing: 7.18967969264558 minutes\n","Percentage of participants with only one crossing (smoothed): 60.46511627906976%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.219230769230766 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.503179799297876 minutes\n","Percentage of participants with only one crossing: 60.0%\n","Mean time of prediction for participants with only one crossing: 12.25 minutes\n","STD of time of prediction for participants with only one crossing: 5.312328428602032 minutes\n","Percentage of participants with only one crossing (smoothed): 65.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.507692307692306 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.41374264174641 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  10\n","Starting point:  6\n","Window size:  5\n","New sleep onset threshold:  7.3\n","New awake threshold:  23.3\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 27.906976744186046%\n","Mean time of prediction for participants with only one crossing: 14.216666666666667 minutes\n","STD of time of prediction for participants with only one crossing: 5.704944249411109 minutes\n","Percentage of participants with only one crossing (smoothed): 41.86046511627907%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.727777777777776 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 3.827793904175028 minutes\n","Percentage of participants with only one crossing: 15.0%\n","Mean time of prediction for participants with only one crossing: 19.433333333333334 minutes\n","STD of time of prediction for participants with only one crossing: 6.491190611556215 minutes\n","Percentage of participants with only one crossing (smoothed): 25.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.3 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.944745579080739 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  8.6\n","New awake threshold:  16.6\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 41.86046511627907%\n","Mean time of prediction for participants with only one crossing: 13.149999999999999 minutes\n","STD of time of prediction for participants with only one crossing: 5.400951562250654 minutes\n","Percentage of participants with only one crossing (smoothed): 48.837209302325576%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.723809523809521 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 4.948487480500602 minutes\n","Percentage of participants with only one crossing: 35.0%\n","Mean time of prediction for participants with only one crossing: 13.242857142857144 minutes\n","STD of time of prediction for participants with only one crossing: 6.777453618507501 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.745454545454544 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.606224403831487 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10\n","New awake threshold:  10\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 46.51162790697674%\n","Mean time of prediction for participants with only one crossing: 14.555000000000001 minutes\n","STD of time of prediction for participants with only one crossing: 6.068399706677206 minutes\n","Percentage of participants with only one crossing (smoothed): 51.162790697674424%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.609090909090906 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.0125557333201485 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 16.53636363636364 minutes\n","STD of time of prediction for participants with only one crossing: 6.786922281092275 minutes\n","Percentage of participants with only one crossing (smoothed): 65.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.62307692307692 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.366517445094942 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EWTuIOJq2SbV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e9df10be-0482-4c28-f802-7f0c0f33dbf1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda.\n","Convergence point:  12\n","Starting point:  6\n","Window size:  0.5\n","New sleep onset threshold:  8.0\n","New awake threshold:  24.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 69.76744186046511%\n","Mean time of prediction for participants with only one crossing: 13.42333333333333 minutes\n","STD of time of prediction for participants with only one crossing: 6.8834915236059935 minutes\n","Percentage of participants with only one crossing (smoothed): 62.7906976744186%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.922222222222219 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.866900391563454 minutes\n","Percentage of participants with only one crossing: 65.0%\n","Mean time of prediction for participants with only one crossing: 12.06153846153846 minutes\n","STD of time of prediction for participants with only one crossing: 4.979579602511791 minutes\n","Percentage of participants with only one crossing (smoothed): 65.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.930769230769231 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.479385790310193 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10.0\n","New awake threshold:  18.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 62.7906976744186%\n","Mean time of prediction for participants with only one crossing: 11.933333333333335 minutes\n","STD of time of prediction for participants with only one crossing: 5.910505399208606 minutes\n","Percentage of participants with only one crossing (smoothed): 67.44186046511628%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.596551724137928 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.844805871588938 minutes\n","Percentage of participants with only one crossing: 60.0%\n","Mean time of prediction for participants with only one crossing: 11.983333333333334 minutes\n","STD of time of prediction for participants with only one crossing: 5.564745177354386 minutes\n","Percentage of participants with only one crossing (smoothed): 60.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.491666666666667 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.186202164674398 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  12\n","New awake threshold:  12\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 72.09302325581395%\n","Mean time of prediction for participants with only one crossing: 10.748387096774193 minutes\n","STD of time of prediction for participants with only one crossing: 5.835147728330998 minutes\n","Percentage of participants with only one crossing (smoothed): 62.7906976744186%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.274074074074072 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.016495386456619 minutes\n","Percentage of participants with only one crossing: 65.0%\n","Mean time of prediction for participants with only one crossing: 10.076923076923078 minutes\n","STD of time of prediction for participants with only one crossing: 4.122201399579868 minutes\n","Percentage of participants with only one crossing (smoothed): 65.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.7 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.589618875403715 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  12\n","Starting point:  6\n","Window size:  1\n","New sleep onset threshold:  8.0\n","New awake threshold:  24.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 69.76744186046511%\n","Mean time of prediction for participants with only one crossing: 11.726666666666668 minutes\n","STD of time of prediction for participants with only one crossing: 5.870600953527292 minutes\n","Percentage of participants with only one crossing (smoothed): 65.11627906976744%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.21785714285714 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.466233921105003 minutes\n","Percentage of participants with only one crossing: 65.0%\n","Mean time of prediction for participants with only one crossing: 15.023076923076925 minutes\n","STD of time of prediction for participants with only one crossing: 7.217409917495318 minutes\n","Percentage of participants with only one crossing (smoothed): 65.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.930769230769226 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.821605640333665 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10.0\n","New awake threshold:  18.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 62.7906976744186%\n","Mean time of prediction for participants with only one crossing: 11.81851851851852 minutes\n","STD of time of prediction for participants with only one crossing: 5.410623092552009 minutes\n","Percentage of participants with only one crossing (smoothed): 67.44186046511628%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.768965517241377 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.054863983550438 minutes\n","Percentage of participants with only one crossing: 70.0%\n","Mean time of prediction for participants with only one crossing: 15.099999999999998 minutes\n","STD of time of prediction for participants with only one crossing: 7.139827929099052 minutes\n","Percentage of participants with only one crossing (smoothed): 65.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.969230769230764 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.769801959201218 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  12\n","New awake threshold:  12\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 58.139534883720934%\n","Mean time of prediction for participants with only one crossing: 11.332 minutes\n","STD of time of prediction for participants with only one crossing: 5.065547946668751 minutes\n","Percentage of participants with only one crossing (smoothed): 60.46511627906976%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.430769230769227 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.583682905833694 minutes\n","Percentage of participants with only one crossing: 60.0%\n","Mean time of prediction for participants with only one crossing: 14.591666666666667 minutes\n","STD of time of prediction for participants with only one crossing: 7.9275320175253094 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.381818181818181 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.020634545495554 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  12\n","Starting point:  6\n","Window size:  2\n","New sleep onset threshold:  8.0\n","New awake threshold:  24.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 53.48837209302325%\n","Mean time of prediction for participants with only one crossing: 14.756521739130436 minutes\n","STD of time of prediction for participants with only one crossing: 6.682021699205851 minutes\n","Percentage of participants with only one crossing (smoothed): 58.139534883720934%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.539999999999997 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.895213739981965 minutes\n","Percentage of participants with only one crossing: 60.0%\n","Mean time of prediction for participants with only one crossing: 13.458333333333336 minutes\n","STD of time of prediction for participants with only one crossing: 4.691385426739336 minutes\n","Percentage of participants with only one crossing (smoothed): 65.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.89230769230769 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.465058453971507 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10.0\n","New awake threshold:  18.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 62.7906976744186%\n","Mean time of prediction for participants with only one crossing: 17.485185185185188 minutes\n","STD of time of prediction for participants with only one crossing: 7.002893777718091 minutes\n","Percentage of participants with only one crossing (smoothed): 72.09302325581395%\n","Mean time of prediction for participants with only one crossing (smoothed): 18.474193548387102 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.6240691760534745 minutes\n","Percentage of participants with only one crossing: 60.0%\n","Mean time of prediction for participants with only one crossing: 15.65 minutes\n","STD of time of prediction for participants with only one crossing: 5.9389252675322775 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.79090909090909 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.342021160819622 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  12\n","New awake threshold:  12\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 69.76744186046511%\n","Mean time of prediction for participants with only one crossing: 16.0 minutes\n","STD of time of prediction for participants with only one crossing: 6.856140799798868 minutes\n","Percentage of participants with only one crossing (smoothed): 72.09302325581395%\n","Mean time of prediction for participants with only one crossing (smoothed): 16.812903225806448 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.849090431606581 minutes\n","Percentage of participants with only one crossing: 50.0%\n","Mean time of prediction for participants with only one crossing: 14.09 minutes\n","STD of time of prediction for participants with only one crossing: 5.053998417095122 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.836363636363632 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.060643939275568 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  12\n","Starting point:  6\n","Window size:  3\n","New sleep onset threshold:  8.0\n","New awake threshold:  24.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 53.48837209302325%\n","Mean time of prediction for participants with only one crossing: 17.682608695652174 minutes\n","STD of time of prediction for participants with only one crossing: 6.915053030317143 minutes\n","Percentage of participants with only one crossing (smoothed): 46.51162790697674%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.149999999999997 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.595263451902432 minutes\n","Percentage of participants with only one crossing: 50.0%\n","Mean time of prediction for participants with only one crossing: 13.9 minutes\n","STD of time of prediction for participants with only one crossing: 3.860569906114899 minutes\n","Percentage of participants with only one crossing (smoothed): 50.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.999999999999996 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 4.075536774462967 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10.0\n","New awake threshold:  18.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 60.46511627906976%\n","Mean time of prediction for participants with only one crossing: 14.296153846153848 minutes\n","STD of time of prediction for participants with only one crossing: 5.469881222816084 minutes\n","Percentage of participants with only one crossing (smoothed): 65.11627906976744%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.896428571428569 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.692665609395384 minutes\n","Percentage of participants with only one crossing: 65.0%\n","Mean time of prediction for participants with only one crossing: 15.523076923076925 minutes\n","STD of time of prediction for participants with only one crossing: 6.060611018466682 minutes\n","Percentage of participants with only one crossing (smoothed): 60.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.699999999999996 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.968668193156661 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  12\n","New awake threshold:  12\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 65.11627906976744%\n","Mean time of prediction for participants with only one crossing: 15.064285714285717 minutes\n","STD of time of prediction for participants with only one crossing: 6.274450134457673 minutes\n","Percentage of participants with only one crossing (smoothed): 60.46511627906976%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.930769230769227 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.7456977303924965 minutes\n","Percentage of participants with only one crossing: 45.0%\n","Mean time of prediction for participants with only one crossing: 14.744444444444447 minutes\n","STD of time of prediction for participants with only one crossing: 5.144168459119751 minutes\n","Percentage of participants with only one crossing (smoothed): 45.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.477777777777776 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.202088849208722 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  12\n","Starting point:  6\n","Window size:  4\n","New sleep onset threshold:  8.0\n","New awake threshold:  24.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 44.18604651162791%\n","Mean time of prediction for participants with only one crossing: 14.79473684210526 minutes\n","STD of time of prediction for participants with only one crossing: 5.3247657407538425 minutes\n","Percentage of participants with only one crossing (smoothed): 46.51162790697674%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.724999999999998 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.030080845229191 minutes\n","Percentage of participants with only one crossing: 45.0%\n","Mean time of prediction for participants with only one crossing: 13.71111111111111 minutes\n","STD of time of prediction for participants with only one crossing: 3.5482216028892384 minutes\n","Percentage of participants with only one crossing (smoothed): 45.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.922222222222222 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 4.64943910064615 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10.0\n","New awake threshold:  18.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 58.139534883720934%\n","Mean time of prediction for participants with only one crossing: 17.108 minutes\n","STD of time of prediction for participants with only one crossing: 5.781274599947661 minutes\n","Percentage of participants with only one crossing (smoothed): 46.51162790697674%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.374999999999996 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.336945241991602 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 14.427272727272726 minutes\n","STD of time of prediction for participants with only one crossing: 4.3734926565079455 minutes\n","Percentage of participants with only one crossing (smoothed): 50.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.6 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 4.340506882842141 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  12\n","New awake threshold:  12\n"]}],"source":["\n","convergence_starting_points = {12: [6]}\n","window_size6_8= [0.5, 1, 2, 3, 4, 5]\n","starting_window_size = {6:window_size6_8}\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"]},{"cell_type":"code","source":["\n","convergence_starting_points = {12: [6]}\n","window_size6_8= [4, 5]\n","starting_window_size = {6:window_size6_8}\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0irlo6Tljm8C","executionInfo":{"status":"ok","timestamp":1693689681338,"user_tz":-60,"elapsed":3296701,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}},"outputId":"79feb872-c8fa-464a-92d4-1c443aaed98a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda.\n","Convergence point:  12\n","Starting point:  6\n","Window size:  4\n","New sleep onset threshold:  8.0\n","New awake threshold:  24.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 51.162790697674424%\n","Mean time of prediction for participants with only one crossing: 16.972727272727273 minutes\n","STD of time of prediction for participants with only one crossing: 5.673878088396283 minutes\n","Percentage of participants with only one crossing (smoothed): 48.837209302325576%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.366666666666664 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.05988632089928 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 13.42727272727273 minutes\n","STD of time of prediction for participants with only one crossing: 3.333055084805397 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.745454545454544 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 2.675261630829484 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10.0\n","New awake threshold:  18.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 60.46511627906976%\n","Mean time of prediction for participants with only one crossing: 16.23076923076923 minutes\n","STD of time of prediction for participants with only one crossing: 5.831134553199298 minutes\n","Percentage of participants with only one crossing (smoothed): 44.18604651162791%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.857894736842104 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.7515504353150115 minutes\n","Percentage of participants with only one crossing: 40.0%\n","Mean time of prediction for participants with only one crossing: 12.8625 minutes\n","STD of time of prediction for participants with only one crossing: 4.685066034753405 minutes\n","Percentage of participants with only one crossing (smoothed): 45.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.366666666666665 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.953523699830583 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  12\n","New awake threshold:  12\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 62.7906976744186%\n","Mean time of prediction for participants with only one crossing: 15.311111111111112 minutes\n","STD of time of prediction for participants with only one crossing: 5.20151924625256 minutes\n","Percentage of participants with only one crossing (smoothed): 62.7906976744186%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.625925925925923 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.765002495427978 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 15.981818181818182 minutes\n","STD of time of prediction for participants with only one crossing: 5.820780669575669 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.972727272727269 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.894387300889938 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  12\n","Starting point:  6\n","Window size:  5\n","New sleep onset threshold:  8.0\n","New awake threshold:  24.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 53.48837209302325%\n","Mean time of prediction for participants with only one crossing: 15.747826086956524 minutes\n","STD of time of prediction for participants with only one crossing: 5.653461391827549 minutes\n","Percentage of participants with only one crossing (smoothed): 60.46511627906976%\n","Mean time of prediction for participants with only one crossing (smoothed): 16.988461538461536 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.092512865620513 minutes\n","Percentage of participants with only one crossing: 50.0%\n","Mean time of prediction for participants with only one crossing: 15.610000000000003 minutes\n","STD of time of prediction for participants with only one crossing: 4.628055747287407 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.836363636363636 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 4.922540498429412 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10.0\n","New awake threshold:  18.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 62.7906976744186%\n","Mean time of prediction for participants with only one crossing: 17.129629629629626 minutes\n","STD of time of prediction for participants with only one crossing: 5.6671774158593085 minutes\n","Percentage of participants with only one crossing (smoothed): 60.46511627906976%\n","Mean time of prediction for participants with only one crossing (smoothed): 16.969230769230766 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.83792436973498 minutes\n","Percentage of participants with only one crossing: 60.0%\n","Mean time of prediction for participants with only one crossing: 15.650000000000004 minutes\n","STD of time of prediction for participants with only one crossing: 5.3340884882048964 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.56363636363636 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.209654102507056 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  12\n","New awake threshold:  12\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 65.11627906976744%\n","Mean time of prediction for participants with only one crossing: 16.04642857142857 minutes\n","STD of time of prediction for participants with only one crossing: 5.256525342226497 minutes\n","Percentage of participants with only one crossing (smoothed): 62.7906976744186%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.625925925925923 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.0784652531888375 minutes\n","Percentage of participants with only one crossing: 60.0%\n","Mean time of prediction for participants with only one crossing: 16.083333333333336 minutes\n","STD of time of prediction for participants with only one crossing: 4.6466535150459505 minutes\n","Percentage of participants with only one crossing (smoothed): 50.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.399999999999997 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 4.980963762164908 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n"]}]},{"cell_type":"markdown","source":["## Analysing the results of hyperparam tuning\n"],"metadata":{"id":"5Z4ywljm784r"}},{"cell_type":"code","source":["folder_name = '/content/drive/MyDrive/sleep onset datasets/FIXED_regression_head_rnn_schedule_learning_perfectly_clean_onset'"],"metadata":{"id":"C79u0PnF8pFM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output_path = folder_name\n","results_df_all = pd.read_csv(f'{output_path}/results.csv')\n","results_df_all = results_df_all.drop(columns=['Unnamed: 0'])\n","results_df_all.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9_IJOMY080b7","executionInfo":{"status":"ok","timestamp":1693860465836,"user_tz":-60,"elapsed":208,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}},"outputId":"94780242-b046-42b7-d7c1-02c29a54cb5b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['convergence_point', 'initial_sleep_onset_threshold', 'ifconverged',\n","       'pre_sleep_window', 'awake_window', 'pos_class_weight',\n","       'sliding_window_size', 'Accuracy', 'precision', 'recall', 'f1_weighted',\n","       'f1_macro', 'f1_pre_sleep', 'f1_awake', 'auc',\n","       'one_crossing_percentage', 'mean_time_of_prediction',\n","       'std_time_of_prediction', 'one_crossing_percentage_smoothed',\n","       'mean_time_of_prediction_smoothed', 'std_time_of_prediction_smoothed',\n","       'MAE', 'MSE', 'RMSE', 'R2', 'custom_MSE', 'accuracy'],\n","      dtype='object')"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["output_path = folder_name\n","results_df_all = pd.read_csv(f'{output_path}/results.csv')\n","results_df_all = results_df_all.drop(columns=['Unnamed: 0'])\n","# only keep results where ifconverged = 1\n","results_df_all_selected = results_df_all[results_df_all['ifconverged'] == 1]\n","\n","# show how many results we have\n","print(f'We have {len(results_df_all_selected)} results')\n","\n","#Only keep the colums:\n","columns = ['convergence_point', 'initial_sleep_onset_threshold', 'sliding_window_size','f1_weighted', 'f1_pre_sleep',\n","       'f1_awake', 'precision',\n","       'recall', 'auc', 'one_crossing_percentage',\n","       'mean_time_of_prediction',\n","       'std_time_of_prediction']\n","results_df_all_selected = results_df_all_selected[columns]\n","\n","\n","sorted_results = results_df_all_selected.sort_values(by=['f1_weighted'], ascending=False)\n","# make the values in the table only have 2 decimal places after saving\n","sorted_results = sorted_results.round(2)\n","\n","headers = ['t_{PS}', 't_0_{PS}', 't_{SW}', 'F1_W', 'F1_{PS}','F1_{AW}', 'P', 'S', 'AUC', 'R_ps', 'Mean(t_{pred})', 'STD(t_{pred})']\n","sorted_results.columns = headers\n","# Save only the top 3 results\n","sorted_results = sorted_results.head(5)\n","\n","sorted_results.to_latex(f'{output_path}/results_top5_f1.tex', index=False, float_format=\"%.2f\")\n","\n","print(sorted_results.head(5))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eIppp5Ac8Jq6","executionInfo":{"status":"ok","timestamp":1693860782755,"user_tz":-60,"elapsed":259,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}},"outputId":"6de33057-ba20-442e-e41a-46b6c1713c6e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["We have 52 results\n","    t_{PS}  t_0_{PS}  t_{SW}  F1_W  F1_{PS}  F1_{AW}     P     S   AUC   R_ps  \\\n","5        4         2     1.0  0.85     0.90     0.52  0.44  0.63  0.82  30.23   \n","8        4         2     1.5  0.84     0.90     0.50  0.44  0.59  0.81  16.28   \n","2        4         2     0.5  0.82     0.87     0.46  0.36  0.64  0.81  44.19   \n","25       6         4     1.0  0.79     0.86     0.52  0.49  0.55  0.75  44.19   \n","19       6         2     1.5  0.78     0.84     0.53  0.46  0.62  0.79  41.86   \n","\n","    Mean(t_{pred})  STD(t_{pred})  \n","5             9.02           7.02  \n","8            14.63           2.72  \n","2             7.67           5.20  \n","25            8.68           5.91  \n","19           14.42           8.28  \n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-16-47cc9ed3c933>:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n","  sorted_results.to_latex(f'{output_path}/results_top5_f1.tex', index=False, float_format=\"%.2f\")\n"]}]},{"cell_type":"code","source":["output_path = folder_name\n","results_df_all = pd.read_csv(f'{output_path}/results.csv')\n","results_df_all = results_df_all.drop(columns=['Unnamed: 0'])\n","# only keep results where ifconverged = 1\n","#results_df_all_selected = results_df_all[results_df_all['ifconverged'] == 1]\n","#results_df_all_selected = results_df_all[results_df_all['ifconverged'] == 1]\n","# show how many results we have\n","print(f'We have {len(results_df_all_selected)} results')\n","\n","\n","#Only keep the colums:\n","#Only keep the colums:\n","columns = ['convergence_point', 'initial_sleep_onset_threshold', 'sliding_window_size','one_crossing_percentage',\n","       'mean_time_of_prediction',\n","       'std_time_of_prediction', 'f1_weighted', 'f1_pre_sleep',\n","       'f1_awake', 'precision',\n","       'recall', 'auc']\n","results_df_all_selected = results_df_all_selected[columns]\n","\n","\n","\n","sorted_results = results_df_all_selected.sort_values(by=['one_crossing_percentage'], ascending=False)\n","# make the values in the table only have 2 decimal places after saving\n","sorted_results = sorted_results.round(2)\n","\n","headers = ['t_{PS}', 't_0_{PS}', 't_{SW}', 'R_ps', 'Mean(t_{pred})', 'STD(t_{pred})', 'F1_W', 'F1_{PS}','F1_{AW}', 'P', 'S', 'AUC']\n","sorted_results.columns = headers\n","# Save only the top 3 results\n","sorted_results = sorted_results.head(5)\n","\n","sorted_results.to_latex(f'{output_path}/results_top5_crossing.tex', index=False, float_format=\"%.2f\")\n","\n","print(sorted_results.head(5))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bFKVZfCY8CRN","executionInfo":{"status":"ok","timestamp":1693860949447,"user_tz":-60,"elapsed":218,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}},"outputId":"021e4dbf-25b4-4586-cb73-bc8c16e87662"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["We have 52 results\n","     t_{PS}  t_0_{PS}  t_{SW}   R_ps  Mean(t_{pred})  STD(t_{pred})  F1_W  \\\n","150      12         6     0.5  72.09           10.75           5.84  0.71   \n","89       12         2     0.5  72.09           12.93           6.61  0.69   \n","156      12         6     2.0  69.77           16.00           6.86  0.67   \n","139      10         6     3.0  69.77           13.91           6.63  0.72   \n","107      10         2     0.5  69.77           10.60           6.55  0.69   \n","\n","     F1_{PS}  F1_{AW}     P     S   AUC  \n","150     0.75     0.65  0.65  0.65  0.75  \n","89      0.73     0.64  0.61  0.67  0.73  \n","156     0.70     0.64  0.61  0.68  0.72  \n","139     0.76     0.64  0.61  0.67  0.77  \n","107     0.74     0.58  0.52  0.66  0.73  \n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-18-47e954bc0a4e>:31: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n","  sorted_results.to_latex(f'{output_path}/results_top5_crossing.tex', index=False, float_format=\"%.2f\")\n"]}]},{"cell_type":"code","source":["convergence_points = [4,12]\n","initial_sleep_onset_thresholds = [2, 6]\n","sliding_window_sizes = [1, 0.5]"],"metadata":{"id":"RDPOSX0FM3Ys"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test the best models on the test set"],"metadata":{"id":"jUr8N5p2MZW9"}},{"cell_type":"markdown","source":["### Helper functions"],"metadata":{"id":"LY3gwSSUMg3V"}},{"cell_type":"code","source":["def all_test_classification_model(model, label,  test_labels, test_end_points,\n","                                test_sleep_stages,\n","                                test_loader, device, criterion,\n","                                index, ifprobabilities,\n","                                ifplot, ifsaveplots, save_directory_epoch,\n","                                awake_threshold, sleep_onset_threshold, window_size_minutes):\n","    if ifprobabilities:\n","        test_predictions, test_actuals, test_prediction_probabilities = test_classification(model, test_loader, device, criterion, index, threshold=0.5, ifprobabilities=ifprobabilities, ifplot = ifplot)\n","\n","        test_metrics = evaluate_classification(true_labels=test_actuals, predictions=test_predictions,\n","                                               prediction_probabilities=test_prediction_probabilities,\n","                                               index=index, columns = label, ifsaveplots=ifsaveplots,\n","                                               output_path=save_directory_epoch, ifplot=ifplot)\n","\n","        if ifplot:\n","                print('------------------------------------------- No averaging ----------------------------------------')\n","                print('Accuracy')\n","        accuracies, precision, recall, f1 = check_accuracy_timeline(test_predictions, test_labels, test_end_points,\n","                                                                    test_sleep_stages, window_for_averaging= 1, awake_window=awake_threshold,\n","                                                                    pre_sleep_window=sleep_onset_threshold, rnn_window = window_size_minutes,\n","                                                                    ifsaveplots = ifsaveplots, output_path= save_directory_epoch, ifplot = ifplot)\n","        if ifplot:\n","            print('Probability of sleep as predicted by the model')\n","        y_pred_sorted = check_accuracy_timeline(test_prediction_probabilities, test_labels, test_end_points,\n","                                                test_sleep_stages, window_for_averaging= 1, if_proba = True,\n","                                                classes = index, awake_window=awake_threshold, pre_sleep_window=sleep_onset_threshold,\n","                                                rnn_window = window_size_minutes, ifsaveplots = ifsaveplots, output_path= save_directory_epoch,\n","                                                ifplot = ifplot)\n","        if ifplot:\n","            print('-------------------------------Averaging of accuracy over 30 seconds---------------------------------')\n","            print('Accuracy')\n","        accuracies, precision, recall, f1 = check_accuracy_timeline(test_predictions, test_labels, test_end_points,\n","                                                                    test_sleep_stages, window_for_averaging= 5,\n","                                                                    awake_window=awake_threshold, pre_sleep_window=sleep_onset_threshold,\n","                                                                    rnn_window = window_size_minutes, ifsaveplots = ifsaveplots,\n","                                                                    output_path= save_directory_epoch, ifplot = ifplot)\n","        if ifplot:\n","            print('Probability of sleep as predicted by the model')\n","        y_pred_sorted = check_accuracy_timeline(test_prediction_probabilities, test_labels, test_end_points,\n","                                                test_sleep_stages, window_for_averaging= 5, if_proba = True,\n","                                                classes = index,  awake_window=awake_threshold,\n","                                                pre_sleep_window=sleep_onset_threshold, rnn_window = window_size_minutes,\n","                                                ifsaveplots = ifsaveplots, output_path= save_directory_epoch, ifplot = ifplot)\n","\n","        return  test_predictions, test_actuals, test_prediction_probabilities, test_metrics\n","\n","    else:\n","        test_predictions, test_actuals = test_classification(model, test_loader, device, criterion, index, threshold=0.5, ifprobabilities=ifprobabilities, ifplot = ifplot)\n","\n","\n","        test_metrics = evaluate_classification(true_labels=test_actuals, predictions=test_predictions,\n","                                               prediction_probabilities=None,\n","                                               index=index, columns = label, ifsaveplots=ifsaveplots,\n","                                               output_path=save_directory_epoch, ifplot=ifplot)\n","\n","        return  test_predictions, test_actuals, None, test_metrics\n"],"metadata":{"id":"savWIRv5MiY5","executionInfo":{"status":"ok","timestamp":1693954109313,"user_tz":-60,"elapsed":5,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["from sklearn.utils.class_weight import compute_class_weight\n","import warnings\n","def test_loop_RNN_regclass_schedule(test_data, models_to_test, convergence_points, initial_sleep_onset_thresholds, sliding_window_sizes,\n","                                    filling_limit, method, batch_size, results_saving_dir = None, ifresetresults = True, random_seed = 42):\n","\n","    ################## INITIALISATION OF RESULTS STORAGE ##################\n","    warnings.filterwarnings('ignore', category=RuntimeWarning)\n","    warnings.filterwarnings('ignore', category=UserWarning)\n","\n","    #if results_saving_dir is None:\n","     #   results_saving_dir = output_path\n","\n","    save_directory = results_saving_dir\n","\n","    if not os.path.exists(save_directory):\n","        os.makedirs(save_directory)\n","\n","    # Initialize a DataFrame to store results\n","    columns = ['convergence_point', 'initial_sleep_onset_threshold',\n","                'sliding_window_size', 'accuracy', 'precision', 'recall', 'f1_weighted',\n","                'f1_macro', 'f1_pre_sleep', 'f1_awake', 'auc',\n","                'one_crossing_percentage', 'mean_time_of_prediction',\n","                'std_time_of_prediction', 'one_crossing_percentage_smoothed',\n","                'mean_time_of_prediction_smoothed', 'std_time_of_prediction_smoothed', 'Accuracy'\n","                'MAE', 'MSE', 'RMSE', 'R2', 'custom_MSE']\n","\n","    if ifresetresults:\n","        results_df = pd.DataFrame(columns=columns)\n","        results_df.to_csv(os.path.join(results_saving_dir, 'results_final.csv'))\n","\n","    for model, convergence_point, initial_sleep_onset_threshold, sliding_window_size in zip(models_to_test, convergence_points, initial_sleep_onset_thresholds, sliding_window_sizes):\n","\n","      ################## PREPARE DATA FOR TRAINING AND TESTING ##################\n","      # Firstly, get the datasets for a classification and regressions problem\n","      # Create a new column 'old_label' to store the original label\n","      test_data_bothlabels = test_data.copy()\n","\n","      test_data_bothlabels['Old_label'] = test_data_bothlabels['Label'].copy()\n","\n","      test_sequences, test_labels, test_end_points, test_sleep_stages, index, label = prepare_classification_data(mydata_train = test_data_bothlabels, sleep_onset_threshold = convergence_point,\n","                                                                                                                          awake_threshold = convergence_point,\n","                                                                                                                   method = method, filling_limit = filling_limit,\n","                                                                                                                  window_size_minutes = sliding_window_size,\n","                                                                                                                  ifmissing = True,\n","                                                                                                                  random_seed = random_seed, resampling_method = None)\n","\n","      test_labels_and_end_points = [(test_labels[i], test_end_points[i]) for i in range(len(test_labels))]\n","\n","      # Create the test dataset and dataloader with labels and end points (for the regression head)\n","      test_dataset = SleepDataset(test_sequences, test_labels_and_end_points )\n","      test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","      model_parameters = {'convergence_point': convergence_point,\n","                          'initial_sleep_onset_threshold': initial_sleep_onset_threshold,\n","                          'sliding_window_size': sliding_window_size}\n","\n","      criterion= nn.BCEWithLogitsLoss()\n","\n","      model_string = f'{convergence_point}_convpoint_{initial_sleep_onset_threshold}_init_{sliding_window_size}_window'\n","      save_directory_model =  save_directory + '/' + model_string\n","      if not os.path.exists(save_directory_model):\n","          os.makedirs(save_directory_model)\n","\n","\n","      ########################## TRAIN AND EVALUATE  #############################\n","      test_predictions_class, test_actuals_class, test_prediction_probabilities_class, test_metrics_class = all_test_classification_model(model = model,\n","                                                                                                                                          label = label, test_labels = test_labels,\n","                                                                                                                                          test_end_points = test_end_points,\n","                                                                                                                                          test_sleep_stages= test_sleep_stages,\n","                                                                                                                                          test_loader = test_loader, device = device,\n","                                                                                                                                          criterion = criterion, index = index,\n","                                                                                                                                          ifprobabilities = True,\n","                                                                                                                                          ifplot = False,\n","                                                                                                                                          ifsaveplots=True,\n","                                                                                                                                          save_directory_epoch = save_directory_model,\n","                                                                                                                                          awake_threshold= convergence_point,\n","                                                                                                                                          sleep_onset_threshold=convergence_point,\n","                                                                                                                                          window_size_minutes= sliding_window_size)\n","      # Get the stats for probability change points and mean prediction times in participants on the validaiton set\n","      participant_dataloaders,  participant_dict = test_on_random_participants(data = test_data_bothlabels, model =  model, criterion = criterion, random_seed = random_seed,\n","                                                                                          ifplot = False, device = device, output_path_new =  save_directory_model, index = index,\n","                                                                                          convergence_point = convergence_point,\n","                                                                                          awake_window = convergence_point, pre_sleep_window = convergence_point,\n","                                                                                          ifsaveplots = True, window_size_minutes = sliding_window_size, ifmissing = True,\n","                                                                                          filling_limit = filling_limit, method = method, resampling_method = None)\n","\n","\n","      # joing two dictionaries\n","      test_metrics = {**model_parameters, **test_metrics_class,  **participant_dict}\n","      # populate the results_df dataframe with the results from the current epoch\n","      new_row = pd.DataFrame(test_metrics, index = [0])\n","\n","      # Open the results dataframe and add the new results\n","      results_df_all = pd.read_csv(os.path.join(results_saving_dir, 'results_final.csv'))\n","      results_df_all = pd.concat([results_df_all, new_row], ignore_index=True)\n","      results_df_all.to_csv(os.path.join(results_saving_dir, 'results_final.csv'), index=False)\n","      del results_df_all\n","      del new_row\n","      del test_metrics\n","\n","      # Clean up the cuda memory\n","      torch.cuda.empty_cache()\n","      # close all the plots\n","      plt.close('all')\n","      # clean up the memory\n","      gc.collect()\n","\n"],"metadata":{"id":"i5owgrbgMkhD","executionInfo":{"status":"ok","timestamp":1693954109313,"user_tz":-60,"elapsed":6,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["def extract_models_from_folder(folder_name,  convergence_points, initial_sleep_onset_thresholds, sliding_window_sizes, epoch = 2):\n","  models = []\n","  for convergence_point, initial_sleep_onset_threshold, sliding_window in zip(convergence_points, initial_sleep_onset_thresholds, sliding_window_sizes):\n","      model_dir = folder_name + '/' + f'convergence_point_{convergence_point}_starting_point_{initial_sleep_onset_threshold}_window_size_{sliding_window}'\n","      epoch_dir = model_dir + '/epoch_' + str(epoch)\n","      model_name = f'model_classification_presleep_{convergence_point}_awake_{convergence_point}_window_{sliding_window}.pt'\n","      model_dict = torch.load(epoch_dir + '/' + model_name)\n","      model = SleepOnsetRNNClassifier(input_size=86, hidden_size=128,\n","                                                     num_layers=2, dropout=0.0, l2=0.0).to(device)\n","      model.load_state_dict(model_dict)\n","\n","      models.append(model)\n","\n","  return models\n"],"metadata":{"id":"okpHSA-RMmOo","executionInfo":{"status":"ok","timestamp":1693954109313,"user_tz":-60,"elapsed":5,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":["### Actually test it"],"metadata":{"id":"JFIumtg5MouZ"}},{"cell_type":"code","source":["folder_name = '/content/drive/MyDrive/sleep onset datasets/FIXED_regression_head_rnn_schedule_learning_perfectly_clean_onset'\n","convergence_points = [4,12]\n","initial_sleep_onset_thresholds = [2, 6]\n","sliding_window_sizes = [1, 0.5]"],"metadata":{"id":"Ba86gfFYMpeP","executionInfo":{"status":"ok","timestamp":1693954109313,"user_tz":-60,"elapsed":5,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["models_to_test = extract_models_from_folder(folder_name,  convergence_points, initial_sleep_onset_thresholds, sliding_window_sizes)"],"metadata":{"id":"690cSQ8zMsZt","executionInfo":{"status":"ok","timestamp":1693954126106,"user_tz":-60,"elapsed":16098,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["results_saving_dir = folder_name + '/test_best_models'\n","test_loop_RNN_regclass_schedule(data_test, models_to_test, convergence_points, initial_sleep_onset_thresholds, sliding_window_sizes,\n","                                filling_limit = 40, method = 'LOCF', batch_size = 64, results_saving_dir = results_saving_dir, ifresetresults = True, random_seed = 42)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1rr7GIbsMuU7","executionInfo":{"status":"ok","timestamp":1693957228071,"user_tz":-60,"elapsed":851256,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}},"outputId":"d4e786ba-0d4f-4171-fe8e-841a13f6310a"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["Participant 1 of 54\n","Participant 2 of 54\n","Participant 3 of 54\n","Participant 4 of 54\n","Participant 5 of 54\n","Participant 6 of 54\n","Participant 7 of 54\n","Participant 8 of 54\n","Participant 9 of 54\n","Participant 10 of 54\n","Participant 11 of 54\n","Participant 12 of 54\n","Participant 13 of 54\n","Participant 14 of 54\n","Participant 15 of 54\n","Participant 16 of 54\n","Participant 17 of 54\n","Participant 18 of 54\n","Participant 19 of 54\n","Participant 20 of 54\n","Participant 21 of 54\n","Participant 22 of 54\n","Participant 23 of 54\n","Participant 24 of 54\n","Participant 25 of 54\n","Participant 26 of 54\n","Participant 27 of 54\n","Participant 28 of 54\n","Participant 29 of 54\n","Participant 30 of 54\n","Participant 31 of 54\n","Participant 32 of 54\n","Participant 33 of 54\n","Participant 34 of 54\n","Participant 35 of 54\n","Participant 36 of 54\n","Participant 37 of 54\n","Participant 38 of 54\n","Participant 39 of 54\n","Participant 40 of 54\n","Participant 41 of 54\n","Participant 42 of 54\n","Participant 43 of 54\n","Participant 44 of 54\n","Participant 45 of 54\n","Participant 46 of 54\n","Participant 47 of 54\n","Participant 48 of 54\n","Participant 49 of 54\n","Participant 50 of 54\n","Participant 51 of 54\n","Participant 52 of 54\n","Participant 53 of 54\n","Participant 54 of 54\n","Percentage of participants with only one crossing: 22.22222222222222%\n","Mean time of prediction for participants with only one crossing: 25.741666666666664 minutes\n","STD of time of prediction for participants with only one crossing: 2.889192024694024 minutes\n","Percentage of participants with only one crossing (smoothed): 12.962962962962962%\n","Mean time of prediction for participants with only one crossing (smoothed): 25.77142857142857 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 2.094696899802168 minutes\n","Participant 1 of 54\n","Participant 2 of 54\n","Participant 3 of 54\n","Participant 4 of 54\n","Participant 5 of 54\n","Participant 6 of 54\n","Participant 7 of 54\n","Participant 8 of 54\n","Participant 9 of 54\n","Participant 10 of 54\n","Participant 11 of 54\n","Participant 12 of 54\n","Participant 13 of 54\n","Participant 14 of 54\n","Participant 15 of 54\n","Participant 16 of 54\n","Participant 17 of 54\n","Participant 18 of 54\n","Participant 19 of 54\n","Participant 20 of 54\n","Participant 21 of 54\n","Participant 22 of 54\n","Participant 23 of 54\n","Participant 24 of 54\n","Participant 25 of 54\n","Participant 26 of 54\n","Participant 27 of 54\n","Participant 28 of 54\n","Participant 29 of 54\n","Participant 30 of 54\n","Participant 31 of 54\n","Participant 32 of 54\n","Participant 33 of 54\n","Participant 34 of 54\n","Participant 35 of 54\n","Participant 36 of 54\n","Participant 37 of 54\n","Participant 38 of 54\n","Participant 39 of 54\n","Participant 40 of 54\n","Participant 41 of 54\n","Participant 42 of 54\n","Participant 43 of 54\n","Participant 44 of 54\n","Participant 45 of 54\n","Participant 46 of 54\n","Participant 47 of 54\n","Participant 48 of 54\n","Participant 49 of 54\n","Participant 50 of 54\n","Participant 51 of 54\n","Participant 52 of 54\n","Participant 53 of 54\n","Participant 54 of 54\n","Percentage of participants with only one crossing: 35.18518518518518%\n","Mean time of prediction for participants with only one crossing: 19.47894736842105 minutes\n","STD of time of prediction for participants with only one crossing: 8.220806654450735 minutes\n","Percentage of participants with only one crossing (smoothed): 31.48148148148148%\n","Mean time of prediction for participants with only one crossing (smoothed): 19.2 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.394676184064167 minutes\n"]}]},{"cell_type":"code","source":["results_saving_dir = folder_name + '/test_best_models'\n","results_final_test = pd.read_csv(f'{results_saving_dir}/results_final.csv')\n","columns =  ['convergence_point', 'initial_sleep_onset_threshold', 'sliding_window_size',  'one_crossing_percentage',\n","       'mean_time_of_prediction',\n","       'std_time_of_prediction',  'f1_weighted', 'f1_pre_sleep',\n","       'f1_awake', 'precision',\n","       'recall', 'auc']\n","results_final_test_selected = results_final_test[columns]\n","\n","sorted_results = results_final_test_selected.sort_values(by=['one_crossing_percentage'], ascending=False)\n","sorted_results = sorted_results.round(2)\n","headers = ['t_{PS}', 't_0_{PS}', 't_{SW}', 'R_ps', 'Mean(t_{pred})', 'STD(t_{pred})', 'F1_W', 'F1_{PS}','F1_{AW}', 'P', 'S', 'AUC']\n","sorted_results.columns = headers\n","\n","sorted_results.to_latex(f'{folder_name}/results_test_best.tex', index=False, float_format=\"%.2f\")\n","print(sorted_results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3oCkwMQhQYCo","executionInfo":{"status":"ok","timestamp":1693957228519,"user_tz":-60,"elapsed":472,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}},"outputId":"497a16fd-1aee-470e-a64e-a5286deb37f7"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-68-14df6d366762>:15: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n","  sorted_results.to_latex(f'{folder_name}/results_test_best.tex', index=False, float_format=\"%.2f\")\n"]},{"output_type":"stream","name":"stdout","text":["   t_{PS}  t_0_{PS}  t_{SW}   R_ps  Mean(t_{pred})  STD(t_{pred})  F1_W  \\\n","1      12         6     0.5  35.19           19.48           8.22  0.48   \n","0       4         2     1.0  22.22           25.74           2.89  0.32   \n","\n","   F1_{PS}  F1_{AW}     P     S   AUC  \n","1     0.40     0.59  0.44  0.88  0.64  \n","0     0.34     0.21  0.12  0.90  0.75  \n"]}]}],"metadata":{"colab":{"collapsed_sections":["YkCnikvOVK8A","ElvUVfxvVJo4","D-Bj-76pVJpA","WrpCngQKVJpJ"],"provenance":[{"file_id":"1cgQyySjwhR8WBsGM3Cg5563oBqFKsobC","timestamp":1693309847076},{"file_id":"10DE9W2gEgKOAHYHhPC3S1ihsqqeGvMFC","timestamp":1693309189354}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}