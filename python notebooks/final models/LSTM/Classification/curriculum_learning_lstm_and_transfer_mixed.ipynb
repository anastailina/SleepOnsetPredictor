{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"mlxNtzmEYQCn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693956063965,"user_tz":-60,"elapsed":20915,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}},"outputId":"4accbf2b-687c-4163-cb7c-18f5398430e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"mE-11idpVXaH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693956068249,"user_tz":-60,"elapsed":4286,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}},"outputId":"f7668225-2fd8-4e1e-e10b-29cb0d97cbd7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.12.2)\n","Collecting imblearn\n","  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.42.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n","Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (from imblearn) (0.10.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n","Installing collected packages: imblearn\n","Successfully installed imblearn-0.0\n"]}],"source":["!pip install scikit-learn pandas numpy matplotlib seaborn imblearn scipy"]},{"cell_type":"markdown","metadata":{"id":"YkCnikvOVK8A"},"source":["### Attach google drive\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"V0dTG0LwOctB","executionInfo":{"status":"ok","timestamp":1693956089273,"user_tz":-60,"elapsed":858,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ElvUVfxvVJo4"},"source":["## Import all the necessary libraries and packages\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"TH40xgi5VJo8","executionInfo":{"status":"ok","timestamp":1693956089843,"user_tz":-60,"elapsed":1,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import sklearn\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import cross_val_predict\n","from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import cross_val_predict\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import r2_score\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","from sklearn.preprocessing import StandardScaler\n","import seaborn as sns\n","import scipy\n","import random"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"61iz5piwVJo9","executionInfo":{"status":"ok","timestamp":1693956093137,"user_tz":-60,"elapsed":2194,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["data_train = pd.read_csv('/content/drive/MyDrive/sleep onset datasets/data_train_cleaned_sbjs.csv')\n","data_test = pd.read_csv('/content/drive/MyDrive/sleep onset datasets/data_test_cleaned_sbjs.csv')\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"PfEQQjzAVJo-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693956093138,"user_tz":-60,"elapsed":3,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}},"outputId":"7e3e73e5-8316-4ec9-e902-da4f2e2b5cad"},"outputs":[{"output_type":"stream","name":"stdout","text":["(165000, 90)\n"]}],"source":["data_all = pd.concat([data_train, data_test], axis=0)\n","print(data_all.shape)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"UlTmXkEgVJo-","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1693956093638,"user_tz":-60,"elapsed":502,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}},"outputId":"857462f6-ae35-44f9-ca45-02004789d2da"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["       Label  Sbj_ID  Age  Gender  Race1  Race2  Race3  Race4  ifCleanOnset  \\\n","0       29.9      77   61       1    0.0    0.0    0.0    1.0             1   \n","1       29.8      77   61       1    0.0    0.0    0.0    1.0             1   \n","2       29.7      77   61       1    0.0    0.0    0.0    1.0             1   \n","3       29.6      77   61       1    0.0    0.0    0.0    1.0             1   \n","4       29.5      77   61       1    0.0    0.0    0.0    1.0             1   \n","...      ...     ...  ...     ...    ...    ...    ...    ...           ...   \n","16495    0.4    6739   77       0    1.0    0.0    0.0    0.0             0   \n","16496    0.3    6739   77       0    1.0    0.0    0.0    0.0             0   \n","16497    0.2    6739   77       0    1.0    0.0    0.0    0.0             0   \n","16498    0.1    6739   77       0    1.0    0.0    0.0    0.0             0   \n","16499    0.0    6739   77       0    1.0    0.0    0.0    0.0             0   \n","\n","       Time2Sleep  ...  wrseltr5  slpapnea5  cpap5  dntaldv5  uvula5  \\\n","0            37.5  ...       1.0        0.0    0.0       0.0     0.0   \n","1            37.5  ...       1.0        0.0    0.0       0.0     0.0   \n","2            37.5  ...       1.0        0.0    0.0       0.0     0.0   \n","3            37.5  ...       1.0        0.0    0.0       0.0     0.0   \n","4            37.5  ...       1.0        0.0    0.0       0.0     0.0   \n","...           ...  ...       ...        ...    ...       ...     ...   \n","16495        62.5  ...      -1.0        0.0    0.0       0.0     0.0   \n","16496        62.5  ...      -1.0        0.0    0.0       0.0     0.0   \n","16497        62.5  ...      -1.0        0.0    0.0       0.0     0.0   \n","16498        62.5  ...      -1.0        0.0    0.0       0.0     0.0   \n","16499        62.5  ...      -1.0        0.0    0.0       0.0     0.0   \n","\n","       insmnia5  rstlesslgs5  whiirs5c  epslpscl5c  hoostmeq5c  \n","0           0.0          0.0       5.0         1.0        10.0  \n","1           0.0          0.0       5.0         1.0        10.0  \n","2           0.0          0.0       5.0         1.0        10.0  \n","3           0.0          0.0       5.0         1.0        10.0  \n","4           0.0          0.0       5.0         1.0        10.0  \n","...         ...          ...       ...         ...         ...  \n","16495       0.0          0.0       9.0         2.0        10.0  \n","16496       0.0          0.0       9.0         2.0        10.0  \n","16497       0.0          0.0       9.0         2.0        10.0  \n","16498       0.0          0.0       9.0         2.0        10.0  \n","16499       0.0          0.0       9.0         2.0        10.0  \n","\n","[16500 rows x 90 columns]"],"text/html":["\n","  <div id=\"df-a2c19b72-5df7-4890-95e9-ec5433ca3d0f\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Label</th>\n","      <th>Sbj_ID</th>\n","      <th>Age</th>\n","      <th>Gender</th>\n","      <th>Race1</th>\n","      <th>Race2</th>\n","      <th>Race3</th>\n","      <th>Race4</th>\n","      <th>ifCleanOnset</th>\n","      <th>Time2Sleep</th>\n","      <th>...</th>\n","      <th>wrseltr5</th>\n","      <th>slpapnea5</th>\n","      <th>cpap5</th>\n","      <th>dntaldv5</th>\n","      <th>uvula5</th>\n","      <th>insmnia5</th>\n","      <th>rstlesslgs5</th>\n","      <th>whiirs5c</th>\n","      <th>epslpscl5c</th>\n","      <th>hoostmeq5c</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>29.9</td>\n","      <td>77</td>\n","      <td>61</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>37.5</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>10.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>29.8</td>\n","      <td>77</td>\n","      <td>61</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>37.5</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>10.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>29.7</td>\n","      <td>77</td>\n","      <td>61</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>37.5</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>10.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>29.6</td>\n","      <td>77</td>\n","      <td>61</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>37.5</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>10.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>29.5</td>\n","      <td>77</td>\n","      <td>61</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>37.5</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>10.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>16495</th>\n","      <td>0.4</td>\n","      <td>6739</td>\n","      <td>77</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>62.5</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>9.0</td>\n","      <td>2.0</td>\n","      <td>10.0</td>\n","    </tr>\n","    <tr>\n","      <th>16496</th>\n","      <td>0.3</td>\n","      <td>6739</td>\n","      <td>77</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>62.5</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>9.0</td>\n","      <td>2.0</td>\n","      <td>10.0</td>\n","    </tr>\n","    <tr>\n","      <th>16497</th>\n","      <td>0.2</td>\n","      <td>6739</td>\n","      <td>77</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>62.5</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>9.0</td>\n","      <td>2.0</td>\n","      <td>10.0</td>\n","    </tr>\n","    <tr>\n","      <th>16498</th>\n","      <td>0.1</td>\n","      <td>6739</td>\n","      <td>77</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>62.5</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>9.0</td>\n","      <td>2.0</td>\n","      <td>10.0</td>\n","    </tr>\n","    <tr>\n","      <th>16499</th>\n","      <td>0.0</td>\n","      <td>6739</td>\n","      <td>77</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>62.5</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>9.0</td>\n","      <td>2.0</td>\n","      <td>10.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>16500 rows × 90 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a2c19b72-5df7-4890-95e9-ec5433ca3d0f')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a2c19b72-5df7-4890-95e9-ec5433ca3d0f button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a2c19b72-5df7-4890-95e9-ec5433ca3d0f');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-1b32f919-882f-42f1-93c6-0d5aaeb6dfef\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1b32f919-882f-42f1-93c6-0d5aaeb6dfef')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-1b32f919-882f-42f1-93c6-0d5aaeb6dfef button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":12}],"source":["data_test"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"y4l3h3K2VJo_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693956094263,"user_tz":-60,"elapsed":628,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}},"outputId":"72cf739f-799c-4aeb-bb3c-f664de9ccc60"},"outputs":[{"output_type":"stream","name":"stdout","text":["{2251: 1, 4512: 3, 188: 5, 5801: 7, 3689: 9, 4618: 9, 4998: 9, 6279: 10, 2780: 12, 933: 14, 5255: 14, 2443: 15, 1077: 18, 5568: 18, 2574: 21, 912: 22, 6321: 22, 6475: 22, 110: 25, 2019: 25, 6416: 25, 977: 26, 3801: 27, 2616: 28, 6414: 28, 2167: 29, 6261: 29, 1021: 30, 1385: 30, 2452: 30, 5887: 30, 1951: 31, 5882: 31, 4186: 35, 4806: 35, 3347: 36, 6240: 37, 916: 38, 6739: 40, 949: 41, 2750: 42, 4582: 42, 962: 44, 4969: 45, 3708: 47, 1717: 48, 77: 51, 4038: 53, 5354: 67, 1033: 69, 2897: 75, 4393: 75, 2881: 85, 4756: 88, 2191: 104}\n"]}],"source":["# plot how many nans there are for each participant in data_test\n","\n","test_unique_ids = data_test['Sbj_ID'].unique()\n","test_unique_ids = test_unique_ids.tolist()\n","len(test_unique_ids)\n","\n","# calculate how many nans there are for each participant in data_test\n","nans_per_participant = {}\n","for i in test_unique_ids:\n","    nans_per_participant[i] = data_test[data_test['Sbj_ID'] == i]['delta'].isnull().sum()\n","\n","# get the ids of the pariticpants with the least nans\n","nans_per_participant = {k: v for k, v in sorted(nans_per_participant.items(), key=lambda item: item[1])}\n","print(nans_per_participant)\n","\n","# get the top 20 participants with the least nans\n","top_20 = list(nans_per_participant.keys())[:20]\n","\n","# get data_test with only the top 20 participants with the least nans\n","selected_participants_data = data_test[data_test['Sbj_ID'].isin(top_20)]\n","\n","# save the data_test with only the top 20 participants with the least nans\n","selected_participants_data.to_csv('data_test_cleaned_sbjs_top_20.csv', index=False)\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"trKQkdGYVJo_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693956094263,"user_tz":-60,"elapsed":7,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}},"outputId":"06743431-3414-4d50-b2e6-fde5be4c8d13"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[110,\n"," 188,\n"," 912,\n"," 933,\n"," 1077,\n"," 2019,\n"," 2251,\n"," 2443,\n"," 2574,\n"," 2780,\n"," 3689,\n"," 4512,\n"," 4618,\n"," 4998,\n"," 5255,\n"," 5568,\n"," 5801,\n"," 6279,\n"," 6321,\n"," 6475]"]},"metadata":{},"execution_count":14}],"source":["selected_participants_data = pd.read_csv('/content/drive/MyDrive/sleep onset datasets/data_test_cleaned_sbjs_top_20.csv')\n","selected_participants = selected_participants_data['Sbj_ID'].unique().tolist()\n","selected_participants"]},{"cell_type":"markdown","metadata":{"id":"D-Bj-76pVJpA"},"source":["## Define the helper functions"]},{"cell_type":"markdown","metadata":{"id":"HVIDKf7IVJpA"},"source":["### Define fuctions to create custom Dataset and DataLoader\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"NNvTzx_lVJpA","executionInfo":{"status":"ok","timestamp":1693956094264,"user_tz":-60,"elapsed":6,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["# Define the device:\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","#Create a PyTorch dataset\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","class SleepDataset(Dataset):\n","    def __init__(self, sequences, labels):\n","        self.sequences = sequences\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.sequences)\n","\n","    def __getitem__(self, idx):\n","        return torch.from_numpy(self.sequences[idx]).float(), torch.from_numpy(np.array(self.labels[idx])).float()"]},{"cell_type":"markdown","metadata":{"id":"B2QNmhZ7VJpA"},"source":["### Define functions to preprocess the data -> break up into sequences, resample, stratify, etc."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"o_L_STiNVJpB","executionInfo":{"status":"ok","timestamp":1693956094264,"user_tz":-60,"elapsed":6,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["# function to turn dataset from regression to classification\n","def regression_to_classification(dataframe, sleep_onset_threshold, awake_threshold):\n","    ''' This function takes a dataframe with regression labels and turns it into a classification problem '''\n","\n","    # Try classification on 5 minutes before sleep and 20+ minutes before sleep\n","    dataframe_transformed = dataframe[(dataframe['Label'] <= sleep_onset_threshold) | (dataframe['Label'] > awake_threshold)].copy()\n","\n","    # Swap Label columns into 1  (if Label <= 5) and 0\n","    dataframe_transformed['Label'] = dataframe_transformed['Label'].apply(lambda x: 1 if x <= sleep_onset_threshold else 0)\n","\n","    index = [1, 0]\n","    label = [f'{sleep_onset_threshold} minutes', f'{awake_threshold}-30 minutes']\n","    return dataframe_transformed, index, label\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"7EwSJqMPVJpB","executionInfo":{"status":"ok","timestamp":1693956094264,"user_tz":-60,"elapsed":6,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["from sklearn.model_selection import StratifiedShuffleSplit\n","from imblearn.over_sampling import SMOTE\n","from imblearn.under_sampling import RandomUnderSampler\n","\n","def handle_missing_data(data, ifmissing):\n","    if not ifmissing:\n","        assert data.isnull().sum().sum() == 0, \"There are NaNs in the data\"\n","\n","def ss_split_remaining_subjects(subjects, labels, random_seed):\n","        sss_val_test = StratifiedShuffleSplit(n_splits=1, train_size=0.5,\n","                                              test_size=0.5, random_state=random_seed)\n","        for val_idx, test_idx in sss_val_test.split(subjects, labels):\n","            val_subjects = subjects[val_idx]\n","            test_subjects = subjects[test_idx]\n","        return val_subjects, test_subjects\n","\n","\n","def split_data_into_sets(data, train_proportion, if_stratified_sampling, iftest, random_seed):\n","     # Create a list of all unique subjects\n","    subjects = data['Sbj_ID'].unique()\n","\n","    # Randomly select some of the 80% of subjects to be in the training set, 20% to the validation set and 20% in the test set\n","    np.random.seed(random_seed)\n","\n","    val_subjects = []\n","    test_subjects = []\n","    train_subjects = []\n","    if if_stratified_sampling == 1:\n","        # Get representative labels for each subject for stratification\n","        subject_labels = data.groupby('Sbj_ID')['Label'].apply(lambda x: x.value_counts().idxmax())\n","\n","        sss = StratifiedShuffleSplit(n_splits=1, train_size=train_proportion, test_size=1-train_proportion, random_state=random_seed)\n","\n","        for train_idx, temp_idx in sss.split(subjects, subject_labels):\n","            train_subjects = subjects[train_idx]\n","            temp_subjects = subjects[temp_idx]\n","\n","        if iftest == 1:\n","            # Now split the remaining subjects into validation and test sets\n","            sss_val_test = StratifiedShuffleSplit(n_splits=1, train_size=0.5, test_size=0.5, random_state=random_seed)\n","\n","            for val_idx, test_idx in sss_val_test.split(temp_subjects, subject_labels.iloc[temp_idx]):\n","                val_subjects = temp_subjects[val_idx]\n","                test_subjects = temp_subjects[test_idx]\n","        else:\n","            val_subjects = temp_subjects\n","    else:\n","        train_subjects = np.random.choice(subjects, size=int(train_proportion*len(subjects)), replace=False)\n","\n","\n","        if iftest == 1:\n","            val_proportions = (1 - train_proportion)/2\n","            val_subjects = np.random.choice(np.setdiff1d(subjects, train_subjects), size=int(val_proportions*len(subjects)), replace=False)\n","            test_subjects = np.setdiff1d(subjects, np.concatenate((train_subjects, val_subjects)))\n","        else:\n","            val_proportions = 1 - train_proportion\n","            val_subjects = np.setdiff1d(subjects, train_subjects)\n","            test_subjects = []\n","\n","    # Print the number of subjects in each set\n","    print(f'There are {len(train_subjects)} subjects in the training set, {len(val_subjects)} subjects in the validation set and {len(test_subjects)} subjects in the test set')\n","\n","    return train_subjects, val_subjects, test_subjects\n","\n","def assign_data_set(data, train_subjects, val_subjects, test_subjects):\n","    data['Set'] = 'train'\n","    data.loc[data['Sbj_ID'].isin(val_subjects), 'Set'] = 'val'\n","    data.loc[data['Sbj_ID'].isin(test_subjects), 'Set'] = 'test'\n","    return data.sort_values('Sbj_ID')\n","\n","from imblearn.over_sampling import SMOTE\n","from imblearn.under_sampling import RandomUnderSampler\n","import numpy as np\n","\n","def generate_sequences_and_labels(group, window_size, ifoutput_end_points=0):\n","    sequences = []\n","    labels = []\n","\n","    group_features = group.drop(['Sbj_ID', 'Label', 'Set'], axis=1).to_numpy()\n","    ifmissing_column = group['ifmissing_after_imputation'].to_numpy()\n","\n","    # If window size is longer than the group, skip\n","    if window_size > len(group_features):\n","        return sequences, labels\n","\n","    for i in range(len(group_features) - window_size):\n","        if ifmissing_column[i : i + window_size].sum() > 0:\n","            continue\n","\n","        if ifoutput_end_points == 0 and group['Label'].iloc[i : i + window_size].nunique() > 1:\n","            continue\n","\n","        sequences.append(group_features[i : i + window_size])\n","        labels.append(group['Label'].iloc[i])\n","\n","    return sequences, labels\n","\n","def resample_data(sequences, labels, method, window_size, random_seed=42):\n","    if method == 'oversampling':\n","        print('Performing SMOTE oversampling')\n","        smote = SMOTE(random_state=random_seed)\n","        sequences_resampled, labels_resampled = smote.fit_resample(\n","            sequences.reshape(sequences.shape[0], -1), labels)\n","        sequences = sequences_resampled.reshape(-1, window_size, sequences.shape[-1])\n","        labels = labels_resampled\n","    elif method == 'undersampling':\n","        print('Performing undersampling')\n","        rus = RandomUnderSampler(random_state=random_seed)\n","        sequences_resampled, labels_resampled = rus.fit_resample(\n","            sequences.reshape(sequences.shape[0], -1), labels)\n","        sequences = sequences_resampled.reshape(-1, window_size, sequences.shape[-1])\n","        labels = labels_resampled\n","\n","    return sequences, labels\n","\n","def determine_output_end_points(train_proportion, iftest, train_end_points, test_end_points, val_end_points):\n","    if int(train_proportion) == 1:\n","        return train_end_points\n","    else:\n","        if iftest == 1:\n","            return test_end_points\n","        else:\n","            return val_end_points\n","\n","def format_output(train_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels, ifoutputsubjects, train_subjects, val_subjects, test_subjects, ifoutput_end_points, end_points=None, sleep_stages=None):\n","\n","    if ifoutputsubjects:\n","\n","        if ifoutput_end_points == 1:\n","            return train_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels, train_subjects, val_subjects, test_subjects, end_points, sleep_stages\n","        else:\n","            return train_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels, train_subjects, val_subjects, test_subjects\n","\n","    else:\n","        if ifoutput_end_points == 1:\n","            return train_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels, end_points, sleep_stages\n","        else:\n","            return train_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels\n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"zcqKZE3pVJpC","executionInfo":{"status":"ok","timestamp":1693956094264,"user_tz":-60,"elapsed":6,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["def create_sliding_windows_new(data, window_size_minutes=0.5, ifmissing=True, random_seed=42, ifoutputsubjects=0,\n","                           train_proportion=0.8, ifoutput_end_points=0, if_stratified_sampling=1,\n","                           resampling_method=None, iftest=1):\n","\n","    handle_missing_data(data, ifmissing)\n","\n","    train_subjects, val_subjects, test_subjects = split_data_into_sets(data, train_proportion, if_stratified_sampling, iftest, random_seed)\n","\n","    data = assign_data_set(data, train_subjects, val_subjects, test_subjects)\n","\n","    # Convert the window size from minutes to 6-second epochs\n","    window_size = int((window_size_minutes * 60) / 6)\n","\n","    train_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels, train_end_points, train_sleep_stages, val_end_points, val_sleep_stages, test_end_points, test_sleep_stages = generate_sequences_and_labels(data, window_size, ifoutput_end_points)\n","\n","    train_sequences, train_labels = resample_data(train_sequences, train_labels, resampling_method, random_seed, window_size)\n","\n","    if ifoutput_end_points:\n","        end_points, sleep_stages = determine_output_end_points(train_proportion, iftest, train_end_points, val_end_points, test_end_points, train_sleep_stages, val_sleep_stages, test_sleep_stages)\n","\n","    return format_output(train_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels, ifoutputsubjects, ifoutput_end_points, train_subjects, val_subjects, test_subjects, end_points, sleep_stages)"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"_yuh7kX4VJpC","executionInfo":{"status":"ok","timestamp":1693956094264,"user_tz":-60,"elapsed":6,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["\n","\n","# Create a function for creating sliding windows of selected length\n","def create_sliding_windows(data,  window_size_minutes=0.5,\n","                           ifmissing = True, random_seed = 42, ifoutputsubjects = 0,\n","                           train_proportion = 0.8, ifoutput_end_points = 0,\n","                           if_stratified_sampling = 1,\n","                           resampling_method = None, iftest = 0, ifvocal = False):\n","\n","    # Assert whether therea are any NaNs in the data\n","    if not ifmissing:\n","        assert data.isnull().sum().sum() == 0, \"There are NaNs in the data\"\n","\n","    # extract 'Old_label' column\n","    labels = data['Old_label'].to_numpy()\n","\n","    # Create a list of all unique subjects\n","    subjects = data['Sbj_ID'].unique()\n","\n","    # Randomly select some of the 80% of subjects to be in the training set, 20% to the validation set and 20% in the test set\n","    np.random.seed(random_seed)\n","\n","    val_subjects = []\n","    test_subjects = []\n","    train_subjects = []\n","    if if_stratified_sampling == 1:\n","        # Get representative labels for each subject for stratification\n","        subject_labels = data.groupby('Sbj_ID')['Label'].apply(lambda x: x.value_counts().idxmax())\n","\n","        sss = StratifiedShuffleSplit(n_splits=1, train_size=train_proportion, test_size=1-train_proportion, random_state=random_seed)\n","\n","        for train_idx, temp_idx in sss.split(subjects, subject_labels):\n","            train_subjects = subjects[train_idx]\n","            temp_subjects = subjects[temp_idx]\n","\n","        if iftest == 1:\n","            # Now split the remaining subjects into validation and test sets\n","            sss_val_test = StratifiedShuffleSplit(n_splits=1, train_size=0.5, test_size=0.5, random_state=random_seed)\n","\n","            for val_idx, test_idx in sss_val_test.split(temp_subjects, subject_labels.iloc[temp_idx]):\n","                val_subjects = temp_subjects[val_idx]\n","                test_subjects = temp_subjects[test_idx]\n","        else:\n","            val_subjects = temp_subjects\n","    else:\n","        train_subjects = np.random.choice(subjects, size=int(train_proportion*len(subjects)), replace=False)\n","\n","\n","        if iftest == 1:\n","            val_proportions = (1 - train_proportion)/2\n","            val_subjects = np.random.choice(np.setdiff1d(subjects, train_subjects), size=int(val_proportions*len(subjects)), replace=False)\n","            test_subjects = np.setdiff1d(subjects, np.concatenate((train_subjects, val_subjects)))\n","        else:\n","            val_proportions = 1 - train_proportion\n","            val_subjects = np.setdiff1d(subjects, train_subjects)\n","            test_subjects = []\n","\n","    # Print the number of subjects in each set\n","    if ifvocal:\n","        print(f'There are {len(train_subjects)} subjects in the training set, {len(val_subjects)} subjects in the validation set and {len(test_subjects)} subjects in the test set')\n","\n","\n","    # Create a new column in the dataframe that indicates whether the subject is in the training set, val set or the test set\n","    data['Set'] = 'train'\n","    data.loc[data['Sbj_ID'].isin(val_subjects), 'Set'] = 'val'\n","    data.loc[data['Sbj_ID'].isin(test_subjects), 'Set'] = 'test'\n","\n","    # Sort your dataframe by Sbj_ID if not already sorted\n","    data = data.sort_values('Sbj_ID')\n","\n","    # Convert the window size from minutes to 6-second epochs\n","    window_size = int((window_size_minutes*60)/6)\n","\n","    # Create empty lists to store your sequences and labels\n","    train_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels = [], [], [], [], [], []\n","    if ifoutput_end_points == 1:\n","        test_end_points, test_sleep_stages, val_end_points, val_sleep_stages, train_end_points, train_sleep_stages = [], [], [], [], [], []\n","\n","    # Group the DataFrame by subject ID and iterate over each group, dividing into training and test sets\n","    for _, group in data.groupby('Sbj_ID'):\n","\n","        group = group.sort_values('Old_label', ascending=False)\n","\n","        # For each group, get the feature columns and convert them into numpy array\n","        if ifoutput_end_points == 1:\n","             group_features = group.drop(['Sbj_ID', 'Label', 'Set', 'ifmissing_after_imputation', 'Old_label', 'ifCleanOnset', 'Time2Sleep', 'SleepStage'], axis=1).to_numpy()\n","             group_old_labels = group['Old_label'].to_numpy()\n","             group_sleep_stages = group['SleepStage'].to_numpy()\n","             group_labels = group['Label'].to_numpy()\n","        else:\n","            group_features = group.drop(['Sbj_ID', 'Label', 'Set', 'ifmissing_after_imputation', 'ifCleanOnset', 'Time2Sleep', 'SleepStage'], axis=1).to_numpy()\n","\n","        ifmissing_column = group['ifmissing_after_imputation'].to_numpy()\n","\n","        train_test = group['Set'].iloc[0]\n","\n","        #if window_size == len(group_features):\n","        #    if train_test == 'train':\n","        #        print('got here')\n","        #        train_sequences.append(group_features)\n","        #        train_labels.append(group['Label'].iloc[0])\n","        #    elif train_test == 'val':\n","        #        val_sequences.append(group_features)\n","        #        val_labels.append(group['Label'].iloc[0])\n","        #    elif train_test == 'test':\n","        #        test_sequences.append(group_features)\n","        #        test_labels.append(group['Label'].iloc[0])\n","        #elif window_size > len(group_features):\n","        #    continue\n","\n","\n","        # Iterate over the group array with a sliding window\n","        for i in range(len(group_features) - window_size):\n","\n","            # If there are any NaNs in the window, skip it\n","\n","            if ifmissing_column[i : i + window_size].sum() > 0:\n","                continue\n","\n","            # Check if the labels are consistent in the window (only if we're not outputting probabilities for the whole duration)\n","            if ifoutput_end_points == 0:\n","                if group['Label'].iloc[i : i + window_size].nunique() > 1:\n","                    continue\n","\n","            if train_test == 'train':\n","                # Append the window data to your sequences\n","                train_sequences.append(group_features[i : i + window_size])\n","                # Append the label corresponding to the end of the window\n","                train_labels.append(group['Label'].iloc[i])\n","\n","                if ifoutput_end_points == 1:\n","                    # Append the starting point of the window to the list of starting points\n","                    train_end_points.append(group_old_labels[i+window_size])\n","                    train_sleep_stages.append(group_sleep_stages[i+window_size])\n","\n","            elif train_test == 'val':\n","                # Append the window data to your sequences\n","                val_sequences.append(group_features[i : i + window_size])\n","\n","                # Append the label corresponding to the end of the window\n","                val_labels.append(group['Label'].iloc[i])\n","\n","                if ifoutput_end_points == 1:\n","                    # Append the starting point of the window to the list of starting points\n","                    val_end_points.append(group_old_labels[i+window_size])\n","                    val_sleep_stages.append(group_sleep_stages[i+window_size])\n","\n","\n","            elif train_test == 'test':\n","\n","                # Append the window data to your sequences\n","                test_sequences.append(group_features[i : i + window_size])\n","\n","                # Append the label corresponding to the end of the window\n","                test_labels.append(group['Label'].iloc[i])\n","\n","                if ifoutput_end_points == 1:\n","                    # Append the starting point of the window to the list of starting points\n","                    test_end_points.append(group_old_labels[i+window_size])\n","                    test_sleep_stages.append(group_sleep_stages[i+window_size])\n","\n","    # Convert the sequences and labels into numpy arrays\n","    train_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels = np.array(train_sequences), np.array(train_labels), np.array(val_sequences), np.array(val_labels), np.array(test_sequences), np.array(test_labels)\n","    if ifoutput_end_points == 1:\n","        train_end_points, train_sleep_stages, val_end_points, val_sleep_stages, test_end_points, test_sleep_stages = np.array(train_end_points), np.array(train_sleep_stages), np.array(val_end_points), np.array(val_sleep_stages), np.array(test_end_points), np.array(test_sleep_stages)\n","\n","\n","    if resampling_method == 'oversampling':\n","        if ifvocal:\n","            print('Performing SMOTE oversampling')\n","        smote = SMOTE(random_state=random_seed)\n","        if ifoutput_end_points == 1:\n","            train_sequences_and_end_points = np.concatenate((train_sequences.reshape(train_sequences.shape[0], -1), train_end_points.reshape(-1, 1)), axis=1)\n","            train_sequences_and_end_points_resampled, train_labels_resampled = smote.fit_resample(train_sequences_and_end_points, train_labels)\n","            train_sequences_resampled = train_sequences_and_end_points_resampled[:, :-1]\n","            train_end_points_resampled = train_sequences_and_end_points_resampled[:, -1]\n","            train_sequences = train_sequences_resampled.reshape(-1, window_size, train_sequences.shape[-1])\n","            train_end_points = train_end_points_resampled\n","        else:\n","            train_sequences_resampled, train_labels_resampled = smote.fit_resample(train_sequences.reshape(train_sequences.shape[0], -1), train_labels)\n","        train_sequences = train_sequences_resampled.reshape(-1, window_size, train_sequences.shape[-1])\n","        train_labels = train_labels_resampled\n","    elif resampling_method == 'undersampling':\n","        if ifvocal:\n","            print('Performing undersampling')\n","        rus = RandomUnderSampler(random_state=random_seed)\n","        if ifoutput_end_points == 1:\n","            train_sequences_and_end_points = np.concatenate((train_sequences.reshape(train_sequences.shape[0], -1), train_end_points.reshape(-1, 1)), axis=1)\n","            train_sequences_and_end_points_resampled, train_labels_resampled = rus.fit_resample(train_sequences_and_end_points, train_labels)\n","            train_sequences_resampled = train_sequences_and_end_points_resampled[:, :-1]\n","            train_end_points_resampled = train_sequences_and_end_points_resampled[:, -1]\n","            train_sequences = train_sequences_resampled.reshape(-1, window_size, train_sequences.shape[-1])\n","            train_end_points = train_end_points_resampled\n","        else:\n","            train_sequences_resampled, train_labels_resampled = rus.fit_resample(train_sequences.reshape(train_sequences.shape[0], -1), train_labels)\n","        train_sequences = train_sequences_resampled.reshape(-1, window_size, train_sequences.shape[-1])\n","        train_labels = train_labels_resampled\n","    elif resampling_method is None:\n","        pass\n","\n","    if ifoutput_end_points == 1:\n","        if int(train_proportion) == 1:\n","\n","            end_points = train_end_points\n","            sleep_stages = train_sleep_stages\n","            if ifvocal:\n","                print('end_points shape', end_points.shape)\n","\n","        else:\n","            if iftest == 1:\n","                end_points = test_end_points\n","                sleep_stages = test_sleep_stages\n","            else:\n","                end_points = val_end_points\n","                sleep_stages = val_sleep_stages\n","\n","    del data\n","    gc.collect()\n","\n","\n","    if ifoutputsubjects:\n","        if ifoutput_end_points == 1:\n","            return train_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels, train_subjects, val_subjects, test_subjects, end_points, sleep_stages\n","        else:\n","            return train_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels, train_subjects, val_subjects, test_subjects\n","    else:\n","        if ifoutput_end_points == 1:\n","            return train_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels, end_points, sleep_stages\n","        else:\n","            return train_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels\n","\n","\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"iebM535UVJpD","executionInfo":{"status":"ok","timestamp":1693956094264,"user_tz":-60,"elapsed":5,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["import numpy as np\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","from sklearn.ensemble import RandomForestRegressor\n","\n","# Create a function to impute the missing values in the dataset\n","def impute_missing_values (dataframe, method = 'LOCF', limit = 20):\n","\n","    # create a missing mask (column) fthat would indicate whether the values in any of the columns of mydata are missing\n","    # 1 indicates missing, 0 indicates not missing\n","    missing_mask = dataframe.isnull().sum(axis=1).astype(bool).astype(int)\n","    dataframe_imputed = dataframe.copy()\n","    del dataframe\n","    dataframe_imputed = pd.concat([dataframe_imputed, missing_mask.rename('ifmissing')], axis=1)\n","\n","    if method == 'LOCF':\n","\n","        dataframe_imputed.fillna(method='ffill', inplace=True, limit = limit)\n","\n","        # fill the rest with backward fill\n","        #dataframe_imputed.fillna(method='bfill', inplace=True, limit = limit)\n","\n","    elif method =='NOCB':\n","\n","        dataframe_imputed.fillna(method='bfill', inplace=True, limit = limit)\n","\n","        # fill the rest with forward fill\n","        #dataframe_imputed.fillna(method='ffill', inplace=True, limit = limit)\n","\n","    elif method == 'linear interpolation':\n","\n","        dataframe_imputed.interpolate(method='linear', inplace=True, limit = limit)\n","\n","    elif method == 'quadratic interpolation':\n","\n","        dataframe_imputed.interpolate(method='quadratic', inplace=True, limit = limit)\n","\n","    elif method == 'mean':\n","\n","        dataframe_imputed.fillna(dataframe.mean(), inplace=True, limit = limit)\n","\n","    elif method == 'median':\n","\n","        dataframe_imputed.fillna(dataframe.median(), inplace=True, limit = limit)\n","\n","    elif method == 'MICE':\n","\n","        # Define an imputer\n","        imp = IterativeImputer(estimator=RandomForestRegressor(n_estimators=10, random_state=0),\n","                            missing_values=np.nan,\n","                            sample_posterior=False,\n","                            max_iter=10,\n","                            random_state=0,\n","                            verbose=2)\n","\n","\n","        columns = dataframe_imputed.columns\n","\n","        # Apply the imputer\n","        df_imputed = imp.fit_transform(dataframe_imputed)\n","\n","        # Convert back to DataFrame\n","        dataframe_imputed = pd.DataFrame(df_imputed, columns=columns)\n","\n","    elif method == 'None':\n","        dataframe_imputed = dataframe_imputed\n","\n","    missing_mask_new = dataframe_imputed.isnull().sum(axis=1).astype(bool).astype(int)\n","\n","    dataframe_imputed = pd.concat([dataframe_imputed, missing_mask_new.rename('ifmissing_after_imputation')], axis=1)\n","\n","    return dataframe_imputed\n"]},{"cell_type":"markdown","metadata":{"id":"_lpQFlysVJpD"},"source":["### Define the model architectures"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"z-gUwVY8VJpD","executionInfo":{"status":"ok","timestamp":1693956094264,"user_tz":-60,"elapsed":5,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class SleepOnsetRNNClassifier(nn.Module):\n","\n","    def __init__(self, input_size, hidden_size, num_layers, output_size=1, dropout=0.0, l2=0.0):\n","        super(SleepOnsetRNNClassifier, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.l2 = l2\n","\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","\n","        out, _ = self.lstm(x, (h0, c0))\n","        out = self.fc(out[:, -1, :])\n","       #prob = self.sigmoid(out)\n","        return out\n"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"792nzIN_VJpD","executionInfo":{"status":"ok","timestamp":1693956094264,"user_tz":-60,"elapsed":5,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["class PretrainedModelForRegression(nn.Module):\n","    def __init__(self, original_model):\n","        super(PretrainedModelForRegression, self).__init__()\n","\n","        # Extract feature parts from the original model\n","        self.features = nn.Sequential(\n","            original_model.lstm,\n","            # You can add more layers from the original model here if needed\n","        )\n","\n","        # Regression head\n","        self.regression_head = nn.Linear(original_model.hidden_size, 1)\n","\n","    def forward(self, x):\n","        # Note: Here we're not initializing h0 and c0 as the original model does.\n","        # This means they will default to 0s.\n","        out, _ = self.features(x)\n","        out = self.regression_head(out[:, -1, :])\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"S6bXm4MTVJpD"},"source":["### Define the training and evaluation functions for classification"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"ZMxVzbhDVJpD","executionInfo":{"status":"ok","timestamp":1693956094264,"user_tz":-60,"elapsed":5,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["from sklearn.metrics import f1_score\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","# Train function\n","def train_classification(model, train_loader, val_loader, num_epochs,\n","                        criterion, optimizer, device, index,\n","                        threshold=0.5, ifprobabilities=False, ifplot = False):\n","\n","    train_losses = []\n","    val_losses = []\n","    sigmoid_function = torch.nn.Sigmoid()  # define a sigmoid function\n","\n","    for epoch in range(num_epochs):\n","\n","        # Train the model\n","        model.train()\n","        train_predictions, train_actuals = [], []\n","        for i, (inputs, labels_and_endpoints) in enumerate(train_loader):\n","            inputs = inputs.to(device)\n","            labels = labels_and_endpoints[:, 0].to(device)\n","\n","\n","            # Forward pass\n","            outputs = model(inputs)\n","            #print(outputs)\n","            #print(labels)\n","            outputs_detach = outputs.detach()\n","            #print(outputs_detach)\n","            probabilities = sigmoid_function(outputs_detach).cpu().numpy() # convert logits to probabilities\n","            #print(probabilities)\n","\n","            loss = criterion(outputs, labels.view(-1, 1))\n","\n","            # Backward and optimize\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            train_actuals.extend(labels.cpu().numpy())\n","            train_predictions.extend((probabilities > threshold).astype(int).flatten())\n","\n","\n","        report = classification_report(train_actuals, train_predictions)\n","        if ifplot:\n","            print('-'*100)\n","            print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item()}')\n","            print('-'*100)\n","            print('Training Report:')\n","            print(report)\n","            print('-'*100)\n","\n","        # Validate the model\n","        model.eval()\n","        with torch.no_grad():\n","            val_loss = 0\n","            predictions, actuals, prediction_probabilities = [], [], []\n","            for inputs, labels_and_endpoints in val_loader:\n","                inputs = inputs.to(device)\n","                labels = labels_and_endpoints[:, 0].to(device)\n","\n","                logits = model(inputs)  # these are logits now\n","                val_loss += criterion(logits, labels.view(-1, 1)).item()\n","\n","                probabilities = sigmoid_function(logits).cpu().numpy()  # convert logits to probabilities\n","\n","                predictions.extend((probabilities > threshold).astype(int).flatten())\n","\n","\n","                if ifprobabilities:\n","                    prediction_probabilities.extend(probabilities)\n","\n","                actuals.extend(labels.cpu().numpy())\n","\n","\n","            # Calculate metrics\n","            f1_scores = f1_score(actuals, predictions, average='weighted', labels = index)\n","            if ifplot:\n","                print('-'*100)\n","                print(f'Validation Loss: {val_loss/len(val_loader)}')\n","                print('F1-Scores for each class:')\n","                print(f1_scores)\n","\n","                # Calculate metrics\n","                report = classification_report(actuals, predictions)\n","\n","                print('Validation Report:')\n","                print(report)\n","\n","\n","        # Save the model training and validation losses for plotting\n","        train_losses.append(loss.item())\n","        val_losses.append(val_loss/len(val_loader))\n","\n","\n","    del train_predictions, train_actuals\n","\n","    if ifprobabilities:\n","        return train_losses, val_losses, model, optimizer, predictions, actuals, prediction_probabilities\n","    else:\n","        return train_losses, val_losses, model, optimizer, predictions, actuals\n","\n","\n","# Test function\n","def test_classification(model, test_loader, device, criterion, index, threshold=0.5, ifprobabilities=False, ifplot = False):\n","    model.eval()\n","    predictions, actuals, prediction_probabilities = [], [], []\n","\n","    sigmoid_function = torch.nn.Sigmoid()  # define a sigmoid function\n","\n","    with torch.no_grad():\n","        test_loss = 0\n","        for inputs, labels_and_endpoints in test_loader:\n","            inputs = inputs.to(device)\n","            labels = labels_and_endpoints[:, 0].to(device)\n","\n","            logits = model(inputs)  # these are logits now\n","            test_loss += criterion(logits, labels.view(-1, 1)).item()\n","\n","            probabilities = sigmoid_function(logits).cpu().numpy()  # convert logits to probabilities\n","\n","            predictions.extend((probabilities > threshold).astype(int).flatten())\n","            if ifprobabilities:\n","                prediction_probabilities.extend(probabilities)\n","            actuals.extend(labels.cpu().numpy())\n","        if ifplot:\n","            print(f'Test Loss: {test_loss/len(test_loader)}')\n","\n","    # Calculate metrics\n","    f1_scores = f1_score(actuals, predictions, average=None, labels=index)\n","\n","\n","    if ifplot:\n","        print('F1-Scores for each class:')\n","        print(f1_scores)\n","\n","    # Calculate metrics\n","    report = classification_report(actuals, predictions)\n","\n","    if ifprobabilities:\n","        return predictions, actuals, prediction_probabilities\n","    else:\n","        return predictions, actuals"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"nyQKkVpbVJpD","executionInfo":{"status":"ok","timestamp":1693956094264,"user_tz":-60,"elapsed":5,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["\n","def get_timeline(mydata_no_nan):\n","\n","    # assert that the data has no NaNs\n","    assert mydata_no_nan.isnull().sum().sum() == 0, 'There are NaNs in the data'\n","\n","\n","    # Create X and y from dataframes to use in scikit-learn (drop Label, SleepStage, Sbj_ID and ifCleanOnset columns)\n","    X_all = mydata_no_nan.drop(['Label', 'Old_label', 'SleepStage', 'Sbj_ID', 'ifCleanOnset', 'Time2Sleep'], axis=1)\n","    y_all = mydata_no_nan['Label']\n","    y_old_all = mydata_no_nan['Old_label']\n","    y_sleepstage_all = mydata_no_nan['SleepStage']\n","\n","    del mydata_no_nan\n","\n","\n","\n","    return X_all, y_all, y_old_all, y_sleepstage_all"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"qvur9v4NVJpD","executionInfo":{"status":"ok","timestamp":1693956094265,"user_tz":-60,"elapsed":5,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","import pandas as pd\n","import os\n","from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n","                             roc_curve, roc_auc_score, auc)\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.metrics import f1_score\n","\n","def evaluate_classification(true_labels, predictions, prediction_probabilities,\n","                            index=['10 minutes before sleep', 'Awake'],\n","                            columns=['10 minutes before sleep', 'Awake'],\n","                            save_path='confusion_matrix.png', ifsaveplots = False,\n","                            output_path=None, iftest = False, ifplot = True, classes = None, ifvocal = True):\n","\n","    # Evaluate the performance of the model\n","    accuracy = accuracy_score(true_labels, predictions)\n","    if iftest:\n","        if ifvocal:\n","            print(f'Accuracy: {accuracy}')\n","\n","    # Classification report\n","    report = classification_report(true_labels, predictions)\n","    if iftest:\n","        if ifvocal:\n","            print(report)\n","\n","    # Decide on the filename based on iftest\n","    report_filename = 'test_classification_report.txt' if iftest else 'crossval_classification_report.txt'\n","\n","     # Dictionary to store all metrics\n","    metrics_dict = {}\n","    metrics_dict['accuracy'] = accuracy\n","    # Classification report\n","    precision, recall, _,  _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n","    f1 = f1_score(true_labels, predictions, average='weighted')\n","    f1_macro = f1_score(true_labels, predictions, average='macro')\n","    f1_pos, f1_neg = f1_score(true_labels, predictions, average=None)\n","\n","\n","    metrics_dict['precision'] = precision\n","    metrics_dict['recall'] = recall\n","    metrics_dict['f1_weighted'] = f1\n","    metrics_dict['f1_macro'] = f1_macro\n","    metrics_dict['f1_pre_sleep'] = f1_pos\n","    metrics_dict['f1_awake'] = f1_neg\n","\n","\n","\n","    if iftest:\n","        if ifvocal:\n","            print('F1 pre-sleep: ', f1_pos)\n","            print('F1 awake: ', f1_neg)\n","\n","\n","\n","    # Save classification report to a .txt file\n","    if ifsaveplots:\n","        if output_path:\n","            report_filename = os.path.join(output_path, report_filename)\n","        with open(report_filename, 'w') as f:\n","            f.write(report)\n","    # Plot the confusion matrix\n","\n","    # Include the names of the classes in the confusion matrix\n","    cm = confusion_matrix(true_labels, predictions, normalize='true', labels=index)\n","    cm = pd.DataFrame(cm, index=index, columns=columns)\n","\n","    # Rename the columns and rows to the names of the classes\n","    cm.index = columns\n","    plt.figure(figsize=(5.5, 4))\n","    sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues')\n","\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.title('Confusion Matrix for times before sleep')\n","\n","    if iftest:\n","        saving_prefix = 'test'\n","    else:\n","        saving_prefix = 'crossval'\n","\n","    save_path = f'{saving_prefix}_{save_path}'\n","\n","    if ifsaveplots:\n","        if output_path:\n","            save_path = os.path.join(output_path, save_path)\n","        # Save the confusion matrix to a file\n","        plt.savefig(save_path)\n","\n","    # Display the confusion matrix\n","    if ifplot:\n","        plt.show()\n","    plt.close()\n","\n","    roc_path = f'{saving_prefix}_roc_curve.png'\n","\n","    # ROC and AUC\n","\n","    # if array is 2D\n","    if type(prediction_probabilities) == list:\n","        prediction_probabilities = np.array(prediction_probabilities)\n","        prediction_probabilities = prediction_probabilities.ravel()\n","\n","\n","    fpr, tpr, _ = roc_curve(true_labels, prediction_probabilities)\n","    roc_auc = auc(fpr, tpr)\n","    #metrics_dict['roc_curve'] = (fpr, tpr)\n","    metrics_dict['auc'] = roc_auc\n","\n","    plt.figure(figsize=(7, 7))\n","    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n","    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC)')\n","    plt.legend(loc=\"lower right\")\n","\n","\n","    if ifsaveplots:\n","        if output_path:\n","            roc_path = os.path.join(output_path, roc_path)\n","        plt.savefig(roc_path)\n","\n","    if ifplot:\n","        plt.show()\n","    plt.close()\n","\n","    del true_labels, predictions, cm, report, f1, f1_macro, f1_pos, f1_neg, roc_auc\n","\n","\n","\n","    return metrics_dict\n"]},{"cell_type":"markdown","metadata":{"id":"9mFIiM_vVJpE"},"source":["### Define the functions for probability analysis"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"CLYgxXP6VJpE","executionInfo":{"status":"ok","timestamp":1693956094265,"user_tz":-60,"elapsed":5,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","\n","def preprocess_data(y_pred, y_true, timeline, sleep_stage):\n","    y_pred = np.array(y_pred)\n","    y_true = np.array(y_true)\n","    timeline = np.array(timeline)\n","    sleep_stage = np.array(sleep_stage)\n","    timeline = np.round(timeline, 1)\n","    return y_pred, y_true, timeline, sleep_stage\n","\n","def get_indices_for_unique_values(timeline):\n","    unique_values = np.unique(timeline)\n","    unique_values_sorted = np.sort(unique_values)\n","    indices = {}\n","    for value in unique_values:\n","        indices[value] = np.where(timeline == value)[0]\n","    return dict(sorted(indices.items())), unique_values_sorted\n","\n","def update_indices_with_missing_values(indices, rnn_window):\n","    timegrid = np.arange(0, 30.1 - rnn_window, 0.1)\n","    timegrid = np.round(timegrid, 1)\n","    missing_values = np.setdiff1d(timegrid, list(indices.keys()))\n","    for value in missing_values:\n","        indices[value] = np.nan\n","    return dict(sorted(indices.items())), timegrid\n","\n","def sort_predictions_by_time(y_pred, indices, timegrid):\n","    y_pred_sorted = [np.mean(y_pred[indices[value]]) if not np.isnan(indices[value]).any() else np.nan for value in timegrid]\n","    y_pred_std = [np.std(y_pred[indices[value]]) if not np.isnan(indices[value]).any() else np.nan for value in timegrid]\n","    return y_pred_sorted, y_pred_std\n","\n","def downsample_data(data, window_for_averaging):\n","    return [np.nanmean(data[i:i+window_for_averaging]) for i in range(0, len(data), window_for_averaging)]\n","\n","def plot_data(y_pred_mean, y_pred_std, sleep_stage_mean, sleep_stage_std, timegrid, window_for_averaging, rnn_window, pre_sleep_window=None, awake_window=None, ifsaveplots=False, output_path=None, ifplot = False, ifsubjecttest = False, change_points_times = None):\n","\n","    # convert to numpy arrays to avoid errors\n","    y_pred_mean = np.array(y_pred_mean)\n","    y_pred_std = np.array(y_pred_std)\n","    sleep_stage_mean = np.array(sleep_stage_mean)\n","    sleep_stage_std = np.array(sleep_stage_std)\n","\n","    def plot_graph(include_std, include_crossings, filename_suffix):\n","        plt.figure(figsize=(14, 7))\n","        plt.xlabel('Time to sleep onset (min)', fontsize=14, fontweight='bold')\n","        plt.ylabel('Probability', fontsize=14, fontweight='bold')\n","        xticks = np.arange(0, len(y_pred_mean), 20 // window_for_averaging)\n","        plt.xticks(xticks, timegrid[xticks * window_for_averaging], fontsize=12, fontweight='bold')\n","        plt.yticks(fontsize=12, fontweight='bold')\n","\n","        plt.plot(y_pred_mean, label='Predicted Probability' if ifsubjecttest else 'Mean Predicted Probability', color='blue')\n","        if include_std:\n","            plt.fill_between(np.arange(len(y_pred_mean)), y_pred_mean - y_pred_std, y_pred_mean + y_pred_std, color='blue', alpha=0.1)\n","\n","        plt.plot(sleep_stage_mean, label='Sleep Stages' if ifsubjecttest else 'Mean Sleep Stages', color='purple', linestyle='--', linewidth=2.5)\n","        if include_std:\n","            plt.fill_between(np.arange(len(sleep_stage_mean)), sleep_stage_mean - sleep_stage_std, sleep_stage_mean + sleep_stage_std, color='purple', alpha=0.1)\n","\n","        plt.hlines(0.5, linestyle='--', color='r', xmin=0, xmax=len(y_pred_mean), label='Threshold for classifying as pre-sleep onset', linewidth=2.5)\n","        if include_crossings and change_points_times is not None:\n","            plt.vlines(change_points_times, ymin=-0.1, ymax=1.1, color='seagreen', label='Detected change points', linewidth=2.5, linestyle='--')\n","\n","        if awake_window:\n","            awake_window_probas = int((awake_window - rnn_window) * 10 // window_for_averaging)\n","            plt.axvspan(awake_window_probas, len(y_pred_mean), facecolor='g', alpha=0.2, label='Awake window')\n","        if pre_sleep_window:\n","            pre_sleep_window_probas = int((pre_sleep_window - rnn_window) * 10 // window_for_averaging)\n","            plt.axvspan(pre_sleep_window_probas, 0, facecolor='r', alpha=0.2, label='Pre-sleep window')\n","\n","        plt.ylim(-0.1, 1.1)\n","        plt.legend(loc='upper right', fontsize=14)\n","        filename = f\"sleep_probabilities_{filename_suffix}_averaged_over_{window_for_averaging}.jpg\"\n","        if ifsaveplots:\n","            if output_path:\n","                filename = os.path.join(output_path, filename)\n","            plt.savefig(filename, format='jpg', dpi=200)\n","        if ifplot:\n","            plt.show()\n","        plt.close()\n","\n","    plot_graph(False, False, \"without_std\")\n","    plot_graph(True, False, \"with_std\")\n","    plot_graph(False, True, \"without_std_with_crossings\")\n","    plot_graph(True, True, \"with_std_with_crossings\")\n","\n","\n","\n","def calculate_metrics(y_pred, y_true, timegrid, indices):\n","    correct = y_true == y_pred\n","    accuracies = [np.mean(correct[indices[value]]) if not np.isnan(indices[value]).any() else np.nan for value in timegrid]\n","\n","    prediction_dict_by_time = {value: y_pred[indices[value]] if indices[value] is not np.nan else np.nan for value in timegrid}\n","    true_values_dict_by_time = {value: y_true[indices[value]] if indices[value] is not np.nan else np.nan for value in timegrid}\n","\n","    tp = {value: np.sum(np.logical_and(prediction_dict_by_time[value] == 1, true_values_dict_by_time[value] == 1)) if indices[value] is not np.nan else np.nan for value in timegrid}\n","    tn = {value: np.sum(np.logical_and(prediction_dict_by_time[value] == 0, true_values_dict_by_time[value] == 0)) if indices[value] is not np.nan else np.nan for value in timegrid}\n","    fp = {value: np.sum(np.logical_and(prediction_dict_by_time[value] == 1, true_values_dict_by_time[value] == 0)) if indices[value] is not np.nan else np.nan for value in timegrid}\n","    fn = {value: np.sum(np.logical_and(prediction_dict_by_time[value] == 0, true_values_dict_by_time[value] == 1)) if indices[value] is not np.nan else np.nan for value in timegrid}\n","\n","    recall = [tp[value] / (tp[value] + fn[value]) for value in timegrid]\n","    precision = [tp[value] / (tp[value] + fp[value]) for value in timegrid]\n","    f1 = [2 * (precision[i] * recall[i]) / (precision[i] + recall[i]) if precision[i] + recall[i] != 0 else np.nan for i in range(len(precision))]\n","\n","    return accuracies, recall, precision, f1\n","\n","def plot_metric(metric_values, metric_name, color, timegrid, window_for_averaging, rnn_window,\n","                pre_sleep_window=None, awake_window=None, ifsaveplots=False, output_path=None,\n","                ifplot = False):\n","\n","    plt.figure(figsize=(20,10))\n","    plt.xlabel('Minutes', fontsize=14)\n","    xticks = np.arange(0, len(metric_values), 10//window_for_averaging)\n","    plt.xticks(xticks, timegrid[xticks*window_for_averaging], fontsize=12)\n","    plt.yticks(fontsize=12)\n","\n","    plt.plot(metric_values, label=metric_name, color=color)\n","    #plt.hlines(0.5, linestyle='--', color='r', label='Threshold for classifying as pre-sleep onset', xmin=0, xmax=len(metric_values))\n","\n","    if awake_window is not None:\n","        awake_window_probas = int((awake_window - rnn_window)* 10//window_for_averaging)\n","        plt.axvspan(awake_window_probas, len(metric_values), facecolor='g', alpha=0.2, label='Awake window')\n","\n","    if pre_sleep_window is not None:\n","        pre_sleep_window_probas = int((pre_sleep_window - rnn_window) * 10//window_for_averaging)\n","        plt.axvspan(pre_sleep_window_probas, 0, facecolor='r', alpha=0.2, label='Pre-sleep window')\n","\n","    plt.ylim(-0.1, 1.1)\n","    plt.legend(loc='upper right', fontsize=12)\n","    if ifsaveplots:\n","        filename = f\"{metric_name}_metric_averaged_over_{window_for_averaging}.jpg\"\n","        if output_path:\n","            filename = os.path.join(output_path, filename)\n","        plt.savefig(filename, format='jpg', dpi=300)\n","\n","    if ifplot:\n","        plt.show()\n","    plt.close()\n","\n","\n","def plot_metrics(accuracies, #recall, precision, f1,\n","                 timegrid, window_for_averaging, rnn_window,\n","                 pre_sleep_window=None, awake_window=None,\n","                 ifsaveplots=False, output_path=None, ifplot = False):\n","\n","    plot_metric(accuracies, 'Accuracy', 'blue', timegrid, window_for_averaging, rnn_window,\n","                pre_sleep_window, awake_window, ifsaveplots, output_path, ifplot = ifplot)\n","\n","    #plot_metric(recall, 'Recall', 'green', timegrid, window_for_averaging, rnn_window,\n","                #pre_sleep_window, awake_window, ifsaveplots, output_path, ifplot = ifplot)\n","\n","    #plot_metric(precision, 'Precision', 'purple', timegrid, window_for_averaging, rnn_window,\n","                #pre_sleep_window, awake_window, ifsaveplots, output_path, ifplot = ifplot)\n","\n","    #plot_metric(f1, 'F1', 'red', timegrid, window_for_averaging, rnn_window,\n","                #pre_sleep_window, awake_window, ifsaveplots, output_path, ifplot = ifplot)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"myH7suxKVJpE"},"source":["### Define functions for probability change point analysis"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"h-NwT5xaVJpE","executionInfo":{"status":"ok","timestamp":1693956094715,"user_tz":-60,"elapsed":455,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["def nan_convolve(array, window, mode='valid'):\n","    if mode != 'valid':\n","        raise ValueError(\"Only 'valid' mode is currently supported\")\n","\n","    output = np.zeros(len(array) - len(window) + 1)\n","    for i in range(len(output)):\n","        current_slice = array[i: i + len(window)]\n","        valid_data = current_slice[~np.isnan(current_slice)]\n","        valid_window = window[:len(valid_data)]\n","        output[i] = np.sum(valid_data * valid_window)\n","    return output"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"NqTTQDPyVJpE","executionInfo":{"status":"ok","timestamp":1693956094716,"user_tz":-60,"elapsed":14,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["def threshold_consistency_change_point(data, times, threshold = 0.5, consistency_length=None, window_size=20, proportion=0.7, stay_above_proportion=0.8):\n","    #assert len(data) == len(times), \"Data and times must have the same length\"\n","\n","    if type(data) == list:\n","        data = np.array(data)\n","    if type(times) == list:\n","        times = np.array(times)\n","\n","\n","    data = data[::-1]\n","\n","    above = data > threshold\n","\n","\n","    change_points_consistent = np.array([])  # Initialize here\n","    # reverse data\n","\n","    #print(data)\n","    #print(times)\n","\n","    # Check for consistent crossing\n","    if consistency_length:\n","        consistent_above = nan_convolve(above, np.ones(consistency_length), 'valid') == consistency_length\n","        change_points_consistent = np.where(consistent_above)[0] + consistency_length - 1\n","\n","    # Check for proportion in a running window\n","    if not consistency_length or (window_size and proportion):\n","        within_window = nan_convolve(above, np.ones(window_size), 'valid') / window_size\n","        change_points_window = np.where(within_window > proportion)[0] + window_size - 1\n","        change_points_indices = np.sort(np.unique(np.concatenate([change_points_consistent, change_points_window])))\n","    else:\n","        change_points_indices = change_points_consistent\n","\n","    old_indices = change_points_indices.copy()\n","\n","    final_change_points = np.array([])  # Initialize here\n","    last_index = None\n","\n","\n","    # change to integer\n","    change_points_indices = change_points_indices.astype(int)\n","    #print(\"change_points_indices: \", change_points_indices)\n","\n","    for index in change_points_indices:\n","        #print(\"index: \", index)\n","        preceding_data = data[:index]\n","        # calculate how many values are above the threshold (>threshold) while omitting nans\n","        percentage =  np.nansum(preceding_data > threshold)/ np.sum(~np.isnan(preceding_data))\n","       #print(\"percentage: \", percentage)\n","        if percentage > stay_above_proportion:\n","           #print('Delete index: ', index)\n","            # delete the last entry from the final change points\n","            if len(final_change_points) > 0:\n","                final_change_points = np.delete(final_change_points, -1)\n","\n","            change_points_indices = np.delete(change_points_indices, np.where(change_points_indices == index)[0])\n","            #print(\"change_points_indices after deletion: \", change_points_indices)\n","            # add index to final change points\n","\n","            final_change_points = np.append(final_change_points, index)\n","            #print(\"final_change_points after deletion: \", final_change_points)\n","        else:\n","            #print('index that broke consistency: ', index)\n","            # add the remaining change points (change_points_indices) to the final change points\n","            final_change_points = np.concatenate([final_change_points, change_points_indices])\n","            #print(\"final_change_points after concatenation: \", final_change_points)\n","            break\n","\n","\n","    if len(final_change_points) == 0 and len(old_indices) > 0:\n","        final_change_points = np.array([old_indices[-1]])\n","\n","    # Ensure all indices are integers\n","    final_change_points = final_change_points.astype(int)\n","\n","    # Convert indices to associated times\n","    change_points_times = times[final_change_points].tolist()\n","\n","    # Check if the last timepoint is the largest one in the original time grid\n","    if final_change_points is not None and len(final_change_points) > 0:\n","        # the last index of data that is not nan\n","        last_index = np.where(~np.isnan(data))[0][-1]\n","        if final_change_points[0] == last_index:\n","            return np.array([]), np.array([])\n","        elif change_points_times[0] > 28:\n","           return np.array([]), np.array([])\n","\n","    del change_points_indices, change_points_consistent, change_points_window\n","\n","    return final_change_points, change_points_times\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"D62QL14zVJpE"},"source":["### Define the loop for probability analysis"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"Gpjyjq1FVJpE","executionInfo":{"status":"ok","timestamp":1693956094716,"user_tz":-60,"elapsed":14,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["def check_accuracy_timeline(y_pred, y_true, timeline, sleep_stage, window_for_averaging=1,\n","                            if_proba=False, classes=None, pre_sleep_window=None,\n","                            awake_window=None, rnn_window=None, ifsaveplots=False, output_path=None,\n","                            ifplot = False, ifsubjecttest = False, ifcrossings = False):\n","    y_pred, y_true, timeline, sleep_stage = preprocess_data(y_pred, y_true, timeline, sleep_stage)\n","\n","    indices, unique_values_sorted = get_indices_for_unique_values(timeline)\n","    indices, timegrid = update_indices_with_missing_values(indices, rnn_window)\n","\n","    if if_proba:\n","        if classes is not None:\n","            class_idx = np.where(classes != 'Awake')[0][0]\n","\n","\n","            y_pred = y_pred[:,class_idx]\n","\n","        y_pred_mean, y_pred_std = sort_predictions_by_time(y_pred, indices, timegrid)\n","        sleep_stage_mean, sleep_stage_std = sort_predictions_by_time(sleep_stage, indices, timegrid)\n","\n","\n","        if window_for_averaging > 1:\n","            y_pred_mean = downsample_data(y_pred_mean, window_for_averaging)\n","            y_pred_std = downsample_data(y_pred_std, window_for_averaging)\n","            sleep_stage_mean= downsample_data(sleep_stage_mean, window_for_averaging)\n","            sleep_stage_std = downsample_data(sleep_stage_std, window_for_averaging)\n","            downsampled_timegrid = downsample_data(timegrid, window_for_averaging)\n","\n","\n","        change_points_times = np.array([])\n","        final_change_points = np.array([])\n","\n","        if ifcrossings:\n","            window_size = int(20/window_for_averaging)\n","            if window_for_averaging > 1:\n","                dict_pred_time = {downsampled_timegrid[i]: y_pred_mean[i] for i in range(len(y_pred_mean))}\n","                # sort in descending order according to keys\n","                sorted_dict_pred_time = dict(sorted(dict_pred_time.items(), reverse=True))\n","                # get the values of sorted dictionary\n","                y_probas_sorted = list(sorted_dict_pred_time.values())\n","\n","\n","                final_change_points, change_points_times = threshold_consistency_change_point(data = y_probas_sorted,\n","                                                                        times = downsampled_timegrid,\n","                                                                        threshold=0.5,\n","                                                                        window_size=window_size, proportion=0.8, stay_above_proportion=0.6)\n","            else:\n","                dict_pred_time = {timegrid[i]: y_pred_mean[i] for i in range(len(y_pred_mean))}\n","                # sort in descending order according to keys\n","                sorted_dict_pred_time = dict(sorted(dict_pred_time.items(), reverse=True))\n","                # get the values of sorted dictionary\n","                y_probas_sorted = list(sorted_dict_pred_time.values())\n","\n","                final_change_points, change_points_times = threshold_consistency_change_point(data = y_probas_sorted,\n","                                                                        times = timegrid,\n","                                                                        threshold=0.5,\n","                                                                        window_size=window_size, proportion=0.8, stay_above_proportion=0.6)\n","\n","            num_change_points = len(final_change_points)\n","            if num_change_points == 1:\n","                time_of_prediction = change_points_times[0]\n","            else:\n","                time_of_prediction = np.nan\n","\n","\n","        plot_data(y_pred_mean, y_pred_std, sleep_stage_mean, sleep_stage_std,\n","                  timegrid, window_for_averaging, rnn_window,\n","                  pre_sleep_window, awake_window, ifsaveplots,\n","                  output_path, ifplot = ifplot, ifsubjecttest = ifsubjecttest,\n","                   change_points_times = final_change_points)\n","\n","        if window_for_averaging > 1:\n","            if ifcrossings:\n","\n","                del  dict_pred_time, sorted_dict_pred_time, y_probas_sorted, final_change_points, change_points_times\n","\n","                return num_change_points, time_of_prediction\n","            else:\n","\n","                dict_pred_time = {downsampled_timegrid[i]: y_pred_mean[i] for i in range(len(y_pred_mean))}\n","\n","                del downsampled_timegrid\n","\n","                return dict_pred_time\n","        elif ifcrossings:\n","            del dict_pred_time, sorted_dict_pred_time, y_probas_sorted, final_change_points, change_points_times\n","            return num_change_points, time_of_prediction\n","        else:\n","            del timegrid\n","            return y_pred_mean\n","\n","    else:\n","        accuracies, recall, precision, f1 = calculate_metrics(y_pred, y_true, timegrid, indices)\n","        if window_for_averaging > 1:\n","            accuracies = downsample_data(accuracies, window_for_averaging)\n","            #recall = downsample_data(recall, window_for_averaging)\n","            #precision = downsample_data(precision, window_for_averaging)\n","            #f1 = downsample_data(f1, window_for_averaging)\n","\n","\n","        plot_metrics(accuracies, #recall, precision, f1,\n","                     timegrid, window_for_averaging, rnn_window,\n","                     pre_sleep_window, awake_window, ifsaveplots,\n","                     output_path, ifplot = ifplot)\n","\n","        return accuracies, recall, precision, f1\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Rt9SL2MVVJpF"},"source":["### Define the training and evaluation functions for regression"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"OKcKss-pVJpF","executionInfo":{"status":"ok","timestamp":1693956094716,"user_tz":-60,"elapsed":14,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["import torch\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","def train_regression(model, train_loader, num_epochs, criterion,\n","                     optimizer, device, ifplot=False, ifvalidation = True, val_loader = None):\n","\n","    train_losses = []\n","    val_losses = []\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n","\n","        # Train the model\n","        model.train()\n","        running_train_loss = 0.0\n","        train_predictions, train_actuals = [], []\n","        for i, (inputs, labels_and_endpoints) in enumerate(train_loader):\n","            inputs = inputs.to(device)\n","            labels = labels_and_endpoints[:, 1]\n","            labels= labels.to(device)\n","\n","            # Forward pass\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels.view(-1, 1))\n","\n","            # Backward and optimize\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_train_loss += loss.item()\n","            outputs_detached = outputs.detach()\n","            train_predictions.extend(outputs_detached.cpu().numpy().flatten())\n","            train_actuals.extend(labels.cpu().numpy())\n","\n","        avg_train_loss = running_train_loss / len(train_loader)\n","        train_losses.append(avg_train_loss)\n","\n","\n","\n","        if ifvalidation:\n","            # Validate the model\n","            model.eval()\n","            running_val_loss = 0.0\n","            val_predictions, val_actuals = [], []\n","            with torch.no_grad():\n","                for inputs, labels_and_endpoints in val_loader:\n","                    inputs = inputs.to(device)\n","                    labels = labels_and_endpoints[:, 1]\n","                    labels = labels.to(device)\n","\n","                    outputs = model(inputs)\n","                    running_val_loss += criterion(outputs, labels.view(-1, 1)).item()\n","                    val_predictions.extend(outputs.cpu().numpy().flatten())\n","                    val_actuals.extend(labels.cpu().numpy())\n","\n","\n","\n","                avg_val_loss = running_val_loss / len(val_loader)\n","                val_losses.append(avg_val_loss)\n","\n","        # Calculate metrics\n","        if ifvalidation:\n","            mse = mean_squared_error(val_actuals, val_predictions)\n","            r2 = r2_score(val_actuals, val_predictions)\n","            mae = mean_absolute_error(val_actuals, val_predictions)\n","        else:\n","            mse = mean_squared_error(train_actuals, train_predictions)\n","            r2 = r2_score(train_actuals, train_predictions)\n","            mae = mean_absolute_error(train_actuals, train_predictions)\n","\n","        if ifplot:\n","            print('-'*100)\n","            if ifvalidation:\n","                print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}')\n","            else:\n","                print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss}')\n","            print(f'Mean Squared Error: {mse}, R2 Score: {r2}')\n","            print(f'Mean Absolute Error: {mae}')\n","    if ifvalidation:\n","        del train_predictions, train_actuals\n","        return  val_predictions, val_actuals, train_losses, val_losses, model, optimizer\n","    else:\n","        return  train_predictions, train_actuals, train_losses, model, optimizer\n","\n","\n","# Test function\n","def test_regression(model, test_loader, device, criterion, ifplot=False):\n","    model.eval()\n","    predictions, actuals = [], []\n","\n","    with torch.no_grad():\n","        test_loss = 0\n","        for inputs, labels_and_endpoints in test_loader:\n","            inputs = inputs.to(device)\n","            labels = labels_and_endpoints[:, 1].to(device)\n","\n","            outputs = model(inputs)  # these are outputs now\n","            test_loss += criterion(outputs, labels.view(-1, 1)).item()\n","\n","            predictions.extend(outputs.cpu().numpy().flatten())\n","            actuals.extend(labels.cpu().numpy())\n","        if ifplot:\n","            print(f'Test Loss: {test_loss/len(test_loader)}')\n","\n","    # Calculate metrics\n","    mse = mean_squared_error(actuals, predictions)\n","    r2 = r2_score(actuals, predictions)\n","    mae = mean_absolute_error(actuals, predictions)\n","    if ifplot:\n","        print('Mean Squared Error:', mse)\n","        print('R2 Score:', r2)\n","        print('Mean Absolute Error:', mae)\n","    del mse, r2, mae\n","    return predictions, actuals"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"SwDmktEyVJpF","executionInfo":{"status":"ok","timestamp":1693956094716,"user_tz":-60,"elapsed":13,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["def evaluate_regression(predictions, y_test, ifsaveplots = False, savepath = None, ifplot = False, iftest = False):\n","\n","    # check if the predictions are in the correct format\n","\n","    predictions = np.array(predictions)\n","\n","    # check if the y_test is in the correct format\n","\n","    y_test = np.array(y_test)\n","\n","    errors = abs(y_test - predictions)\n","    mape = np.where(y_test != 0, 100 * (errors / y_test), 0)\n","    accuracy = 100 - np.mean(mape)\n","    mse = sklearn.metrics.mean_squared_error(y_test, predictions)\n","    rmse = np.sqrt(mse)\n","    r2 = sklearn.metrics.r2_score(y_test, predictions)\n","    squared_errors = np.square(y_test - predictions)\n","    weights = 1.0 / (abs(y_test) + 0.1)\n","    custom_mse = np.mean(weights * squared_errors)\n","\n","    if ifplot:\n","\n","        print('Mean Absolute Error:', round(np.mean(errors), 2))\n","        print('Accuracy:', round(accuracy, 2), '%.')\n","        print('Mean Squared Error:', round(mse, 2))\n","        print('Root Mean Squared Error:', round(rmse, 2))\n","        print('R2:', round(r2, 2))\n","        print('Custom MSE:', round(custom_mse, 2))\n","\n","\n","\n","\n","    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=1)\n","    plt.scatter(y_test, predictions, alpha=0.2)\n","    plt.xlabel('Actual time to sleep onset')\n","    plt.ylabel('Predicted time to sleep onset')\n","    plt.title('Actual vs Predicted time to sleep onset')\n","    if ifsaveplots:\n","        if iftest:\n","            plt.savefig(os.path.join(savepath, 'test_actual_vs_predicted.png'))\n","        else:\n","            plt.savefig(os.path.join(savepath, 'crossval_actual_vs_predicted.png'))\n","    if ifplot:\n","        plt.show()\n","    plt.close()\n","\n","    residuals = y_test - predictions\n","    plt.scatter(y_test, residuals, alpha=0.2)\n","    plt.plot([y_test.min(), y_test.max()], [0, 0], 'k--', lw=1)\n","    plt.xlabel('Actual time to sleep onset')\n","    plt.ylabel('Residuals')\n","    plt.title('Actual vs Residuals for predicted time to sleep onset')\n","    if ifsaveplots:\n","        if iftest:\n","            plt.savefig(os.path.join(savepath, 'test_actual_vs_residuals.png'))\n","        else:\n","            plt.savefig(os.path.join(savepath, 'crossval_actual_vs_residuals.png'))\n","    if ifplot:\n","        plt.show()\n","    plt.close()\n","\n","    plt.hist(residuals)\n","    plt.xlabel('Residuals')\n","    plt.ylabel('Frequency')\n","    plt.title('Residuals distribution')\n","    if ifsaveplots:\n","        if iftest:\n","            plt.savefig(os.path.join(savepath, 'test_residuals_distribution.png'))\n","        else:\n","            plt.savefig(os.path.join(savepath, 'crossval_residuals_distribution.png'))\n","    if ifplot:\n","        plt.show()\n","    plt.close()\n","\n","    # for each actual, plot the mean residual\n","    # round y_test to 0.1 decimal place\n","    y_test = np.round(y_test, 1)\n","\n","    predictions_residuals_dict = {}\n","    for i in range(len(y_test)):\n","        if y_test[i] not in predictions_residuals_dict:\n","            predictions_residuals_dict[y_test[i]] = []\n","        predictions_residuals_dict[y_test[i]].append(residuals[i])\n","\n","    # sort it in ascending order of keys\n","    predictions_residuals_dict = dict(sorted(predictions_residuals_dict.items()))\n","\n","\n","    actual_predictions_dict = {}\n","    for i in range(len(y_test)):\n","        if y_test[i] not in actual_predictions_dict:\n","            actual_predictions_dict[y_test[i]] = []\n","        actual_predictions_dict[y_test[i]].append(predictions[i])\n","\n","    # sort it in ascending order of keys\n","    actual_predictions_dict = dict(sorted(actual_predictions_dict.items()))\n","\n","\n","    predictions_residuals_mean_dict = {}\n","    for key in predictions_residuals_dict:\n","        predictions_residuals_mean_dict[key] = np.mean(predictions_residuals_dict[key])\n","\n","    # sort it in ascending order of keys\n","    predictions_residuals_mean_dict = dict(sorted(predictions_residuals_mean_dict.items()))\n","\n","\n","    residuals_std_dict = {}\n","    for key in predictions_residuals_dict:\n","        residuals_std_dict[key] = np.std(predictions_residuals_dict[key])\n","    # sort it in ascending order of keys\n","    residuals_std_dict = dict(sorted(residuals_std_dict.items()))\n","\n","\n","    actuals_predictions_mean_dict = {}\n","    for key in actual_predictions_dict:\n","        actuals_predictions_mean_dict[key] = np.mean(actual_predictions_dict[key])\n","\n","    # sort it in ascending order of keys\n","    actuals_predictions_mean_dict = dict(sorted(actuals_predictions_mean_dict.items()))\n","\n","    predictions_std_dict = {}\n","    for key in actual_predictions_dict:\n","        predictions_std_dict[key] = np.std(actual_predictions_dict[key])\n","    # sort it in ascending order of keys\n","    predictions_std_dict = dict(sorted(predictions_std_dict.items()))\n","\n","    plt.plot(list(predictions_residuals_mean_dict.keys()), list(predictions_residuals_mean_dict.values()))\n","    # create std shading\n","    plt.fill_between(list(predictions_residuals_mean_dict.keys()),\n","                        np.array(list(predictions_residuals_mean_dict.values())) - np.array(list(residuals_std_dict.values())),\n","                        np.array(list(predictions_residuals_mean_dict.values())) + np.array(list(residuals_std_dict.values())),\n","                        alpha=0.2)\n","    # plot ideal line (x = 0)\n","    plt.plot([y_test.min(), y_test.max()], [0, 0], 'k--', lw=1, color='red')\n","    plt.legend(['Mean of residuals', 'STD of residuals', 'Ideal residuals line'])\n","    plt.xlabel('Actual time to sleep onset')\n","    plt.ylabel('Residuals for this actual')\n","    plt.title('Mean residual of predictions for each actual time to sleep onset')\n","    if ifsaveplots:\n","        if iftest:\n","            plt.savefig(os.path.join(savepath, 'test_mean_residuals_across_time.png'))\n","        else:\n","            plt.savefig(os.path.join(savepath, 'crossval_mean_residuals_across_time.png'))\n","    if ifplot:\n","        plt.show()\n","    plt.close()\n","\n","    plt.plot(list(actuals_predictions_mean_dict.keys()), list(actuals_predictions_mean_dict.values()))\n","    # create std shading\n","    plt.fill_between(list(actuals_predictions_mean_dict.keys()),\n","                        np.array(list(actuals_predictions_mean_dict.values())) - np.array(list(predictions_std_dict.values())),\n","                        np.array(list(actuals_predictions_mean_dict.values())) + np.array(list(predictions_std_dict.values())),\n","                        alpha=0.2)\n","    # plot ideal line (x = y)\n","    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=1, color='red')\n","    plt.legend(['Mean of predictions', 'STD of predictions', 'Ideal actual = prediction line (x = y)'])\n","    plt.xlabel('Actual time to sleep onset')\n","    plt.ylabel('Mean prediction for this actual')\n","    plt.title('Mean prediction for each actual time to sleep onset')\n","    if ifsaveplots:\n","        if iftest:\n","            plt.savefig(os.path.join(savepath, 'test_mean_predictions_across_time.png'))\n","        else:\n","             plt.savefig(os.path.join(savepath, 'crossval_mean_predictions_across_time.png'))\n","    if ifplot:\n","        plt.show()\n","    plt.close()\n","\n","    test_metrics = {'MAE': round(np.mean(errors), 2), 'Accuracy': round(accuracy, 2),\n","    'MSE': round(mse, 2), 'RMSE': round(rmse, 2), 'R2': round(r2, 2), 'custom_MSE': round(custom_mse, 2)}\n","\n","\n","    del predictions, predictions_residuals_dict\n","    del actual_predictions_dict, predictions_residuals_mean_dict\n","    del actuals_predictions_mean_dict, predictions_std_dict, residuals, residuals_std_dict, y_test\n","    del mse, rmse, r2, custom_mse\n","\n","    return test_metrics"]},{"cell_type":"markdown","metadata":{"id":"HO7xWkvYVJpF"},"source":["### Define functions for assessing the model's performance on a single participant"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"N7p64dcGVJpF","executionInfo":{"status":"ok","timestamp":1693956094716,"user_tz":-60,"elapsed":13,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["def select_random_participants(df, n_participants):\n","    \"\"\"Selects a random sample of participants from the dataset.\n","\n","    Args:\n","        df (pandas.DataFrame): The dataset.\n","        n_participants (int): The number of participants to select.\n","\n","    Returns:\n","        pandas.DataFrame: A random sample of participants.\n","    \"\"\"\n","    # Select a random sample of participants\n","    participants = df['Sbj_ID'].unique()\n","    np.random.seed(42)\n","    selected_participants = np.random.choice(participants, n_participants, replace=False)\n","\n","    for participant in selected_participants:\n","        # check how many nan values there are in the dataset for this participant\n","        n_nans = df[df['Sbj_ID'] == participant].isna().sum().sum()\n","        # if there are more than 20 nan values, select another participant\n","        while n_nans > 50:\n","            selected_participants = np.delete(selected_participants, np.where(selected_participants == participant))\n","            new_participant = np.random.choice(participants, 1, replace=False)\n","            # check if the new participant is already in the list\n","            if new_participant not in selected_participants:\n","                selected_participants = np.append(selected_participants, new_participant)\n","            else:\n","                continue\n","            participant = selected_participants[-1]\n","            n_nans = df[df['Sbj_ID'] == participant].isna().sum().sum()\n","\n","    # Filter the dataset\n","    df = df[df['Sbj_ID'].isin(selected_participants)].copy()\n","\n","    return df, selected_participants"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"Wt0QI743VJpG","executionInfo":{"status":"ok","timestamp":1693956094716,"user_tz":-60,"elapsed":13,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["def test_on_random_participants(data, model, criterion, random_seed, ifplot,\n","                                device, output_path_new, index, convergence_point,\n","                                awake_window, pre_sleep_window, ifsaveplots = True,\n","                                window_size_minutes = 1, ifmissing = True,\n","                                method = None, resampling_method = None, filling_limit = 0):\n","\n","    \"\"\"\n","    Test the model on a random subset of participants.\n","    \"\"\"\n","    # create a new folder for the results\n","    output_path_new = os.path.join(output_path_new, 'random_participants')\n","\n","    if not os.path.exists(output_path_new):\n","        os.makedirs(output_path_new)\n","\n","    # intialise empty dataframe to store results\n","    df_all = pd.DataFrame()\n","\n","    participant_dataloaders = {}\n","\n","    for i, sbj in enumerate(data['Sbj_ID'].unique()):\n","\n","        print('Participant {} of {}'.format(i + 1, len(data['Sbj_ID'].unique())))\n","\n","        # create a folder for each participant\n","        output_path_participant = os.path.join(output_path_new, str(sbj))\n","        if ifsaveplots:\n","            if not os.path.exists(output_path_participant):\n","                os.makedirs(output_path_participant)\n","\n","        participant_data = data[data['Sbj_ID'] == sbj].copy()\n","\n","        test_sequences_sbj, test_labels_sbj, test_end_points_sbj, test_sleep_stages_sbj, index, label = prepare_classification_data(mydata_train = participant_data,\n","                                                                                                        sleep_onset_threshold = convergence_point,\n","                                                                                                        awake_threshold = convergence_point,\n","                                                                                                        method = method, filling_limit = filling_limit,\n","                                                                                                        window_size_minutes = window_size_minutes,\n","                                                                                                        ifmissing = ifmissing, random_seed = random_seed,\n","                                                                                                        resampling_method = None)\n","\n","        # Get the tuple of test labels and test end points\n","        test_labels_and_end_points_sbj = [(test_labels_sbj[i], test_end_points_sbj[i]) for i in range(len(test_labels_sbj))]\n","\n","        # Create the test dataset and dataloader with labels and end points (for the regression head)\n","        test_dataset_sbj = SleepDataset(test_sequences_sbj, test_labels_and_end_points_sbj)\n","        test_loader_sbj = DataLoader(test_dataset_sbj, batch_size=1, shuffle=False)\n","\n","        predictions, actuals, probas = test_classification(model, test_loader_sbj, device, criterion, index, threshold=0.5, ifprobabilities=True)\n","        timeline = test_end_points_sbj #participant_data['Old_label']\n","        participant_dataloaders[sbj] = test_loader_sbj\n","\n","        # get the accuracy of each timepoint\n","        if ifplot:\n","            print('Predict for participant: ', sbj)\n","\n","            print('--------------------------------------Evaluate classificaiton ----------------------------------------')\n","\n","        test_metrics = evaluate_classification(actuals, predictions, probas, index = index, iftest=True, columns = label,\n","                                ifplot=ifplot, output_path=output_path_participant, ifsaveplots = ifsaveplots, classes = None, ifvocal = False)\n","        test_metrics['Sbj_ID'] = sbj\n","\n","\n","        if ifplot:\n","                print('------------------------------------------- No averaging ----------------------------------------')\n","                print('Accuracy')\n","        _, _, _, _ = check_accuracy_timeline(predictions, test_labels_sbj, timeline,\n","                                            test_sleep_stages_sbj, window_for_averaging= 1, awake_window = awake_window,\n","                                            pre_sleep_window = pre_sleep_window, rnn_window = window_size_minutes,\n","                                            ifsaveplots = ifsaveplots, output_path= output_path_participant, ifplot = ifplot)\n","        if ifplot:\n","            print('Probability of sleep as predicted by the model')\n","        num_crossings, time_of_prediction = check_accuracy_timeline(probas, test_labels_sbj, timeline,\n","                                                test_sleep_stages_sbj, window_for_averaging= 1, if_proba = True,\n","                                                classes = index, awake_window = awake_window, pre_sleep_window = pre_sleep_window,\n","                                                rnn_window = window_size_minutes, ifsaveplots = ifsaveplots, output_path= output_path_participant,\n","                                                ifplot = ifplot, ifsubjecttest = True, ifcrossings = True)\n","        test_metrics['num_crossings'] = num_crossings\n","        test_metrics['time_of_prediction'] = time_of_prediction\n","\n","        if ifplot:\n","            print('-------------------------------Averaging of accuracy over 30 seconds---------------------------------')\n","            print('Accuracy')\n","        _, _, _, _ = check_accuracy_timeline(predictions, test_labels_sbj, timeline,\n","                                            test_sleep_stages_sbj, window_for_averaging= 5,\n","                                            awake_window = awake_window, pre_sleep_window = pre_sleep_window,\n","                                            rnn_window = window_size_minutes, ifsaveplots = ifsaveplots,\n","                                            output_path= output_path_participant, ifplot = ifplot)\n","        if ifplot:\n","            print('Probability of sleep as predicted by the model')\n","        num_crossings_smoothed, time_of_prediction_smoothed = check_accuracy_timeline(probas, test_labels_sbj, timeline,\n","                                                test_sleep_stages_sbj, window_for_averaging= 5, if_proba = True,\n","                                                classes = index, awake_window = awake_window, pre_sleep_window = pre_sleep_window,\n","                                                rnn_window = window_size_minutes, ifsaveplots = ifsaveplots, output_path= output_path_participant,\n","                                                ifplot = ifplot, ifsubjecttest = True, ifcrossings = True)\n","\n","        test_metrics['num_crossings_smoothed'] = num_crossings_smoothed\n","        test_metrics['time_of_prediction_smoothed'] = time_of_prediction_smoothed\n","\n","        del test_dataset_sbj, test_loader_sbj, test_labels_and_end_points_sbj, test_sequences_sbj, test_labels_sbj, test_end_points_sbj, test_sleep_stages_sbj\n","        del predictions, actuals, probas\n","\n","\n","        next_index = len(df_all)\n","        df = pd.DataFrame(test_metrics, index=[next_index])\n","        df_all = pd.concat([df_all, df])\n","\n","        del test_metrics\n","\n","    df_all.to_csv(os.path.join(output_path_new, 'classification_results_random_participants.csv'))\n","\n","    # calculate how many participants have only one crossing in df_all['num_crossings'] out of all the participants\n","    num_participants_one_crossing = len(df_all[df_all['num_crossings'] == 1])\n","    num_participants = len(df_all)\n","    one_crossing_percentage = num_participants_one_crossing/num_participants * 100\n","    mean_time_of_prediction = np.mean(df_all['time_of_prediction'][df_all['num_crossings'] == 1])\n","    std_time_of_prediction = np.std(df_all['time_of_prediction'][df_all['num_crossings'] == 1])\n","\n","\n","    num_participants_one_crossing_smoothed = len(df_all[df_all['num_crossings_smoothed'] == 1])\n","    one_crossing_percentage_smoothed = num_participants_one_crossing_smoothed/num_participants * 100\n","    mean_time_of_prediction_smoothed = np.mean(df_all['time_of_prediction_smoothed'][df_all['num_crossings_smoothed'] == 1])\n","    std_time_of_prediction_smoothed = np.std(df_all['time_of_prediction_smoothed'][df_all['num_crossings_smoothed'] == 1])\n","\n","\n","    print(f'Percentage of participants with only one crossing: {one_crossing_percentage}%')\n","    print(f'Mean time of prediction for participants with only one crossing: {mean_time_of_prediction} minutes')\n","    print(f'STD of time of prediction for participants with only one crossing: {std_time_of_prediction} minutes')\n","\n","\n","    print(f'Percentage of participants with only one crossing (smoothed): {one_crossing_percentage_smoothed}%')\n","    print(f'Mean time of prediction for participants with only one crossing (smoothed): {mean_time_of_prediction_smoothed} minutes')\n","    print(f'STD of time of prediction for participants with only one crossing (smoothed): {std_time_of_prediction_smoothed} minutes')\n","\n","    output_dict = {'one_crossing_percentage': one_crossing_percentage, 'mean_time_of_prediction': mean_time_of_prediction,\n","                    'std_time_of_prediction': std_time_of_prediction, 'one_crossing_percentage_smoothed': one_crossing_percentage_smoothed,\n","                    'mean_time_of_prediction_smoothed': mean_time_of_prediction_smoothed, 'std_time_of_prediction_smoothed': std_time_of_prediction_smoothed}\n","\n","    del one_crossing_percentage, mean_time_of_prediction, std_time_of_prediction, one_crossing_percentage_smoothed, mean_time_of_prediction_smoothed, std_time_of_prediction_smoothed\n","\n","    return participant_dataloaders, output_dict"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"ZmAJNSoSVJpG","executionInfo":{"status":"ok","timestamp":1693956094716,"user_tz":-60,"elapsed":13,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["def test_reg_on_random_participants(participant_dataloaders, model, criterion, device, random_seed, ifplot, output_path_new, index):\n","\n","    \"\"\"\n","    Test the model on a random subset of participants.\n","    \"\"\"\n","    # Set random seed\n","\n","    # create a new folder for the results\n","    output_path_new = os.path.join(output_path_new, 'random_participants')\n","    if not os.path.exists(output_path_new):\n","        os.makedirs(output_path_new)\n","\n","    # intialise empty dataframe to store results\n","    df_all = pd.DataFrame()\n","\n","\n","    for sbj in list(participant_dataloaders.keys()):\n","        # create a folder for each participant\n","\n","        output_path_participant = os.path.join(output_path_new, str(sbj))\n","        if not os.path.exists(output_path_participant):\n","            os.makedirs(output_path_participant)\n","\n","        test_dataloader = participant_dataloaders[sbj]\n","\n","        predictions, actuals = test_regression(model, test_dataloader, device, criterion)\n","\n","        # get the accuracy of each timepoint\n","        if ifplot:\n","            print('Predict for participant: ', sbj)\n","\n","            print('--------------------------------------Evaluate regression ----------------------------------------')\n","\n","        test_metrics = evaluate_regression(predictions, actuals, iftest=True,\n","                                ifplot=ifplot, savepath =output_path_participant, ifsaveplots= True)\n","\n","\n","        del test_dataloader, predictions, actuals\n","        df = pd.DataFrame(test_metrics, index = [sbj])\n","        df_all = pd.concat([df_all, df])\n","        del test_metrics\n","\n","    df_all.to_csv(os.path.join(output_path_new, 'regression_results_random_participants.csv'))\n","\n"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"z-ZZn62iVJpG","executionInfo":{"status":"ok","timestamp":1693956094717,"user_tz":-60,"elapsed":13,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["\n","def train_test_split_sbjs(mydata_no_nan, test_size = 0.2, random_seed = 42):\n","\n","    # assert that the data has no NaNs\n","    assert mydata_no_nan.isnull().sum().sum() == 0, 'There are NaNs in the data'\n","\n","    # get the unique subjects\n","    unique_subjects = mydata_no_nan['Sbj_ID'].unique()\n","\n","    # define the train size\n","    train_size = 1 - test_size\n","\n","    # define the number of subjects in the test set\n","\n","    test_size_subjects = int(test_size * len(unique_subjects))\n","\n","    # Select the unique subjects for the test set\n","\n","    test_subjects = random.sample(list(unique_subjects), test_size_subjects)\n","\n","    # Select the unique subjects for the training set\n","    train_subjects = [x for x in unique_subjects if x not in test_subjects]\n","\n","    # Create the test and training sets\n","    test_set = mydata_no_nan[mydata_no_nan['Sbj_ID'].isin(test_subjects)]\n","    train_set = mydata_no_nan[mydata_no_nan['Sbj_ID'].isin(train_subjects)]\n","\n","    # Create X and y from dataframes to use in scikit-learn (drop Label, SleepStage, Sbj_ID and ifCleanOnset columns)\n","\n","    X_train = train_set.drop(['Label', 'SleepStage', 'Sbj_ID', 'ifCleanOnset', 'Time2Sleep'], axis=1)\n","    y_train = train_set['Label']\n","    X_test = test_set.drop(['Label', 'SleepStage', 'Sbj_ID', 'ifCleanOnset', 'Time2Sleep'], axis=1)\n","    y_test = test_set['Label']\n","\n","\n","    # Create list of sbj_IDs for each sample in the training and test sets\n","    sbj_train_set = train_set['Sbj_ID'].copy()\n","    sbj_test_set = test_set['Sbj_ID'].copy()\n","\n","    # Turn the sbj_IDs into a list\n","    sbj_train_set = sbj_train_set.tolist()\n","    sbj_test_set = sbj_test_set.tolist()\n","\n","\n","\n","    return X_train, y_train, X_test, y_test, sbj_train_set, sbj_test_set"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"LVU3ERUdVJpG","executionInfo":{"status":"ok","timestamp":1693956094717,"user_tz":-60,"elapsed":13,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["def get_rid_of_nans(probabilities, y_times2sleep = None):\n","    if np.isnan(probabilities).any():\n","        for i in range(len(probabilities)):\n","            if np.isnan(probabilities[i]):\n","                # omit the corresponding y_times2sleep\n","                if y_times2sleep is not None:\n","                    y_times2sleep.pop(i)\n","                probabilities.pop(i)\n","    return probabilities, y_times2sleep"]},{"cell_type":"markdown","metadata":{"id":"rib1PnKaVJpH"},"source":["### Define the function for schedule learning"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"jw1pMV1_VJpH","executionInfo":{"status":"ok","timestamp":1693956094717,"user_tz":-60,"elapsed":13,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["def update_thresholds(epoch, initial_sleep_onset_threshold, initial_awake_threshold, awake_delta, sleep_delta, schedule_period=1):\n","    \"\"\"\n","    Updates the thresholds based on a target convergence point.\n","\n","    Parameters:\n","    - epoch: current epoch number\n","    - initial_awake_threshold: initial awake threshold\n","    - initial_sleep_onset_threshold: initial sleep onset threshold\n","    - convergence_point: point at which the thresholds should converge\n","    - total_epochs: total epochs for which the training is scheduled\n","    - schedule_period: epochs after which thresholds are adjusted. Default is 10 epochs.\n","\n","    Returns:\n","    - New awake and sleep_onset thresholds\n","    \"\"\"\n","\n","    # Adjusting both thresholds based on the epoch\n","    if epoch % schedule_period == 0 :\n","        initial_awake_threshold -= awake_delta\n","        initial_sleep_onset_threshold += sleep_delta\n","\n","    return round(initial_sleep_onset_threshold, 1), round(initial_awake_threshold,1)"]},{"cell_type":"markdown","metadata":{"id":"8qjH2rzuVJpH"},"source":["### Define helper functions for reducing the length of schedule learning loop"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"V7DWQO-CVJpH","executionInfo":{"status":"ok","timestamp":1693956094717,"user_tz":-60,"elapsed":13,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["def prepare_classification_data(mydata_train, sleep_onset_threshold,\n","                                awake_threshold, method, filling_limit, window_size_minutes,\n","                                ifmissing, random_seed, resampling_method,\n","                                ):\n","\n","    mydata_train_class, index, label = regression_to_classification(mydata_train, sleep_onset_threshold, awake_threshold)\n","\n","    mydata_imputed_class = impute_missing_values(mydata_train_class, method=method, limit=filling_limit)\n","\n","\n","    train_sequences, train_labels, _, _, _, _, train_end_points, train_sleep_stages = create_sliding_windows(data=mydata_imputed_class, window_size_minutes=window_size_minutes,\n","                                                                                                        ifmissing=ifmissing, random_seed=random_seed, train_proportion=1,\n","                                                                                                        if_stratified_sampling=0, resampling_method=resampling_method,\n","                                                                                                          iftest=0, ifoutput_end_points=1)\n","    del mydata_imputed_class\n","    del mydata_train\n","\n","    return train_sequences, train_labels, train_end_points, train_sleep_stages, index, label\n","\n","def define_loss_function(ifclassweights, train_labels, device, ifplot = False):\n","    if ifclassweights:\n","        class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n","        class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n","        pos_class_idx = np.where(np.unique(train_labels) == 1)[0][0]\n","        pos_weight = torch.tensor([class_weights[pos_class_idx]]).to(device)\n","        if ifplot:\n","            print('Positive class weights: ', pos_weight)\n","        return nn.BCEWithLogitsLoss(pos_weight=pos_weight), pos_weight\n","    else:\n","        return nn.BCEWithLogitsLoss(), None\n","\n","def train_and_evaluate_classification_model(model, train_loader, label,  test_labels, test_end_points,\n","                                            test_sleep_stages,\n","                                            test_loader, device, criterion,\n","                                            optimizer, num_epochs, index, ifprobabilities,\n","                                            ifplot, ifsaveplots, save_directory_epoch,\n","                                            awake_threshold, sleep_onset_threshold, window_size_minutes):\n","    if ifprobabilities:\n","        train_losses, test_losses, model, optimizer, test_predictions, test_actuals, test_prediction_probabilities = train_classification(model = model, train_loader=train_loader,\n","                                                                                                                                          val_loader=test_loader,\n","                                                                                                                                          device = device,\n","                                                                                                                                          criterion=criterion, optimizer=optimizer,\n","                                                                                                                                          num_epochs=num_epochs, index=index,\n","                                                                                                                                          ifprobabilities=ifprobabilities,\n","                                                                                                                                          ifplot = ifplot)\n","\n","        test_metrics = evaluate_classification(true_labels=test_actuals, predictions=test_predictions,\n","                                               prediction_probabilities=test_prediction_probabilities,\n","                                               index=index, columns = label, ifsaveplots=ifsaveplots,\n","                                               output_path=save_directory_epoch, ifplot=ifplot)\n","\n","        if ifplot:\n","                print('------------------------------------------- No averaging ----------------------------------------')\n","                print('Accuracy')\n","        accuracies, precision, recall, f1 = check_accuracy_timeline(test_predictions, test_labels, test_end_points,\n","                                                                    test_sleep_stages, window_for_averaging= 1, awake_window=awake_threshold,\n","                                                                    pre_sleep_window=sleep_onset_threshold, rnn_window = window_size_minutes,\n","                                                                    ifsaveplots = ifsaveplots, output_path= save_directory_epoch, ifplot = ifplot)\n","        if ifplot:\n","            print('Probability of sleep as predicted by the model')\n","        y_pred_sorted = check_accuracy_timeline(test_prediction_probabilities, test_labels, test_end_points,\n","                                                test_sleep_stages, window_for_averaging= 1, if_proba = True,\n","                                                classes = index, awake_window=awake_threshold, pre_sleep_window=sleep_onset_threshold,\n","                                                rnn_window = window_size_minutes, ifsaveplots = ifsaveplots, output_path= save_directory_epoch,\n","                                                ifplot = ifplot)\n","        if ifplot:\n","            print('-------------------------------Averaging of accuracy over 30 seconds---------------------------------')\n","            print('Accuracy')\n","        accuracies, precision, recall, f1 = check_accuracy_timeline(test_predictions, test_labels, test_end_points,\n","                                                                    test_sleep_stages, window_for_averaging= 5,\n","                                                                    awake_window=awake_threshold, pre_sleep_window=sleep_onset_threshold,\n","                                                                    rnn_window = window_size_minutes, ifsaveplots = ifsaveplots,\n","                                                                    output_path= save_directory_epoch, ifplot = ifplot)\n","        if ifplot:\n","            print('Probability of sleep as predicted by the model')\n","        y_pred_sorted = check_accuracy_timeline(test_prediction_probabilities, test_labels, test_end_points,\n","                                                test_sleep_stages, window_for_averaging= 5, if_proba = True,\n","                                                classes = index,  awake_window=awake_threshold,\n","                                                pre_sleep_window=sleep_onset_threshold, rnn_window = window_size_minutes,\n","                                                ifsaveplots = ifsaveplots, output_path= save_directory_epoch, ifplot = ifplot)\n","\n","        return train_losses, test_losses, model, optimizer, test_predictions, test_actuals, test_prediction_probabilities, test_metrics\n","\n","    else:\n","        train_losses, test_losses, model, optimizer, test_predictions, test_actuals, test_prediction_probabilities = train_classification(model = model, train_loader=train_loader,\n","                                                                                                                                          val_loader=test_loader,\n","                                                                                                                                          device = device,\n","                                                                                                                                          criterion=criterion, optimizer=optimizer,\n","                                                                                                                                          num_epochs=num_epochs, index=index,\n","                                                                                                                                          ifprobabilities=ifprobabilities,\n","                                                                                                                                          ifplot = ifplot)\n","\n","        test_metrics = evaluate_classification(true_labels=test_actuals, predictions=test_predictions,\n","                                               prediction_probabilities=None,\n","                                               index=index, columns = label, ifsaveplots=ifsaveplots,\n","                                               output_path=save_directory_epoch, ifplot=ifplot)\n","\n","        return train_losses, test_losses, model, optimizer, test_predictions, test_actuals, None, test_metrics\n","\n","def train_and_evaluate_regression_model(model, train_loader, test_loader, criterion, optimizer, device, num_epochs, ifplot, ifsaveplots, output_path):\n","    test_predictions, test_actuals, train_losses, test_losses, model, optimizer = train_regression(\n","        model = model, train_loader = train_loader, val_loader = test_loader, num_epochs = num_epochs, criterion = criterion, optimizer =optimizer, device = device, ifplot = ifplot, ifvalidation = True)\n","\n","    test_metrics = evaluate_regression(test_predictions, test_actuals, ifsaveplots=ifsaveplots, savepath=output_path, ifplot = ifplot)\n","\n","    return train_losses, test_losses, model, optimizer, test_predictions, test_actuals, test_metrics"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"dwm0rzq3VJpH","executionInfo":{"status":"ok","timestamp":1693956094717,"user_tz":-60,"elapsed":12,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["def save_models(model_classification, model_regression, save_directory_epoch, sleep_onset_threshold, awake_threshold, window_size_minutes):\n","    model_path_classification = os.path.join(save_directory_epoch, f\"model_classification_presleep_{sleep_onset_threshold}_awake_{awake_threshold}_window_{window_size_minutes}.pt\")\n","    model_regression_path = os.path.join(save_directory_epoch, f\"model_regression_presleep_{sleep_onset_threshold}_awake_{awake_threshold}_window_{window_size_minutes}.pt\")\n","    torch.save(model_classification.state_dict(), model_path_classification)\n","    torch.save(model_regression.state_dict(), model_regression_path)\n","\n","def plot_losses(train_losses, test_losses, title, ifplot, ifsaveplots, save_directory, filename):\n","    plt.plot(train_losses, label='Training loss')\n","    plt.plot(test_losses, label='Validation loss')\n","    plt.title(title)\n","    plt.legend()\n","    if ifplot:\n","        plt.show()\n","    if ifsaveplots:\n","        plt.savefig(os.path.join(save_directory, filename))\n","    plt.close()"]},{"cell_type":"markdown","metadata":{"id":"_99nmyw3VJpH"},"source":["### Define custom MSE loss function"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"jYPyf4ZFVJpH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693956094717,"user_tz":-60,"elapsed":12,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}},"outputId":"c80ec1fc-d460-4139-858e-a1415430fb3c"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.1099)\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","class CustomMSELoss(nn.Module):\n","    def __init__(self, epsilon=0.1):\n","        super(CustomMSELoss, self).__init__()\n","        self.epsilon = epsilon  # Small value to prevent division by zero\n","\n","    def forward(self, predictions, targets):\n","        # Compute error\n","        errors = (predictions - targets)**2\n","\n","        # Compute weights inversely proportional to target values\n","        weights = 1.0 / (torch.abs(targets) + self.epsilon)\n","\n","        # Weighted sum of errors\n","        loss = torch.sum(weights * errors) / torch.sum(weights)\n","        return loss\n","\n","# Test the custom loss\n","predictions = torch.tensor([0.5, 0.2, 0.7])\n","targets = torch.tensor([0.1, 0.4, 0.8])\n","criterion = CustomMSELoss()\n","loss = criterion(predictions, targets)\n","print(loss)\n"]},{"cell_type":"markdown","metadata":{"id":"cTy5nGqzVJpI"},"source":["### Definte the loop for schedule learning"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"WCAn4qU4VJpI","executionInfo":{"status":"ok","timestamp":1693956094717,"user_tz":-60,"elapsed":11,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["from sklearn.utils.class_weight import compute_class_weight\n","import warnings\n","def training_loop_RNN_regclass_schedule(mydata, convergence_point, total_epochs,  filling_limit, method, ifclassweights, input_size,\n","                                        num_epochs, hidden_size, num_layers, dropout, l2, lr, weight_decay, batch_size,\n","                                        device, random_seed = 42, window_size_minutes = 1, ifprobabilitiesanalysis = 0,\n","                                        if_stratified_sampling = 1, initial_awake_threshold = 30, initial_sleep_onset_threshold = 2,\n","                                        convergence_sensitivity = 0.1, resampling_method = None, ifplot = False, ifsaveplots = True,\n","                                        output_path=None, shuffle_train_loader = True, results_saving_dir = None, selected_participants_data = None,\n","                                        selected_participants = None, ifresetresults = False):\n","\n","    ################## INITIALISATION OF RESULTS STORAGE ##################\n","    warnings.filterwarnings('ignore', category=RuntimeWarning)\n","    warnings.filterwarnings('ignore', category=UserWarning)\n","\n","    if results_saving_dir is None:\n","        results_saving_dir = output_path\n","\n","    save_directory = output_path\n","\n","    if not os.path.exists(save_directory):\n","        os.makedirs(save_directory)\n","\n","    # Initialize a DataFrame to store results\n","    columns = ['convergence_point', 'initial_sleep_onset_threshold','ifconverged', 'pre_sleep_window', 'awake_window', 'pos_class_weight',\n","                'sliding_window_size', 'Accuracy', 'precision', 'recall', 'f1_weighted',\n","                'f1_macro', 'f1_pre_sleep', 'f1_awake', 'auc',\n","                'one_crossing_percentage', 'mean_time_of_prediction',\n","                'std_time_of_prediction', 'one_crossing_percentage_smoothed',\n","                'mean_time_of_prediction_smoothed', 'std_time_of_prediction_smoothed',\n","                'MAE', 'MSE', 'RMSE', 'R2', 'custom_MSE']\n","\n","    if ifresetresults:\n","        results_df = pd.DataFrame(columns=columns)\n","        results_df.to_csv(os.path.join(results_saving_dir, 'results.csv'))\n","\n","    ################## PREPARE DATA FOR TRAINING AND TESTING ##################\n","    # Firstly, get the datasets for a classification and regressions problem\n","    # Create a new column 'old_label' to store the original label\n","    mydata_bothlabels = mydata.copy()\n","\n","\n","    mydata_bothlabels['Old_label'] = mydata_bothlabels['Label'].copy()\n","\n","    # The split we would get after the schedule learning convergence\n","    mydata_final_split, index, label = regression_to_classification(mydata, convergence_point, convergence_point)\n","\n","    del mydata\n","    # Fill in the missing values\n","    mydata_imputed = impute_missing_values(mydata_final_split, method = method, limit = filling_limit)\n","    del mydata_final_split\n","    # Cheсk if the dataset still has any missing values\n","    ifmissing = mydata_imputed.isnull().sum().sum()\n","\n","    # Split the data into train, validation and test sets\n","    _, _, _, _, _, _,  train_subjects, test_subjects, _ = create_sliding_windows(data = mydata_imputed,\n","                                                                        window_size_minutes=window_size_minutes,\n","                                                                        ifmissing=ifmissing, random_seed=random_seed,\n","                                                                        ifoutputsubjects = 1,\n","                                                                        if_stratified_sampling = if_stratified_sampling,\n","                                                                        resampling_method = None,\n","                                                                        train_proportion = 0.8, iftest = 0, ifoutput_end_points= 0)\n","    del mydata_imputed\n","    mydata_test = mydata_bothlabels[mydata_bothlabels['Sbj_ID'].isin(test_subjects)]\n","    mydata_train = mydata_bothlabels[~mydata_bothlabels['Sbj_ID'].isin(test_subjects)]\n","\n","    ########################## PREPARE THE TESTING DATA  #############################\n","    # Get the sequences (windows) and labels (categorical 1 or 0 for classification) for the test set,\n","    # also get the end points time of the sequences (in minutes) and their corresponding sleep stages (at the end of the sequence)\n","    test_sequences, test_labels, test_end_points, test_sleep_stages, _, _,= prepare_classification_data(mydata_train = mydata_test, sleep_onset_threshold = convergence_point,\n","                                                                                                        awake_threshold = convergence_point, method = method,\n","                                                                                                        filling_limit = filling_limit,\n","                                                                                                        window_size_minutes = window_size_minutes,\n","                                                                                                        ifmissing = ifmissing, random_seed = random_seed,\n","                                                                                                        resampling_method = None,\n","                                                                                                        )\n","\n","    # Get the tuple of test labels and test end points\n","    test_labels_and_end_points = [(test_labels[i], test_end_points[i]) for i in range(len(test_labels))]\n","\n","    # Create the test dataset and dataloader with labels and end points (for the regression head)\n","    test_dataset = SleepDataset(test_sequences, test_labels_and_end_points )\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    ########################## PREPARE INDIVIDUAL SUBJECTS TO TEST ON  #############################\n","    if selected_participants is not None:\n","        selected_participant_bothlabels = selected_participants_data.copy()\n","        selected_participant_bothlabels['Old_label'] = selected_participant_bothlabels['Label'].copy()\n","\n","\n","    ############################ INITIALISE THE SCHEDULED LEARNING  ################################\n","    # set the sleep onset and awake thresholds to initial values\n","    sleep_onset_threshold = initial_sleep_onset_threshold\n","    awake_threshold = initial_awake_threshold\n","\n","    # Calculate the deltas for the thresholds (how much the thresholds should change each iteration of the schedule until convergence)\n","    distance_to_convergence_sleep_onset = convergence_point - sleep_onset_threshold\n","    sleep_onset_delta = distance_to_convergence_sleep_onset / total_epochs\n","    distance_to_convergence_awake = awake_threshold - convergence_point\n","    awake_delta = distance_to_convergence_awake / total_epochs\n","\n","    ################################## INITIALISE MODELS  #########################################\n","    # Initialize the classification model\n","    model_classification = SleepOnsetRNNClassifier(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, l2=l2).to(device)\n","\n","    # Define the optimizer\n","    optimizer_class = torch.optim.AdamW(model_classification.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","    criterion_regression = CustomMSELoss()\n","\n","    # Empty lists to store training and validation losses\n","    train_losses_reg_all, test_losses_reg_all = [], []\n","\n","\n","    ############################# BEGIN THE SCHEDULED LEARNING  ###################################\n","\n","    for epoch in range(total_epochs):\n","\n","\n","        train_losses_class, test_losses_class, train_losses_reg, test_losses_reg = [], [], [], []\n","\n","        # Check if the thresholds have converged\n","        if abs(sleep_onset_threshold - convergence_point) < convergence_sensitivity:\n","            if ifplot:\n","                print('Sleep onset threshold has converged')\n","            break\n","        if abs(awake_threshold - convergence_point) < convergence_sensitivity:\n","            if ifplot:\n","                print('Awake threshold has converged')\n","            break\n","\n","        # Reinitilize the optimizer\n","        #optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","        # save each epoch in a separate folder\n","        save_directory_epoch = save_directory + '/epoch_' + str(epoch)\n","        if not os.path.exists(save_directory_epoch):\n","            os.makedirs(save_directory_epoch)\n","\n","\n","        sleep_onset_threshold, awake_threshold = update_thresholds(epoch, sleep_onset_threshold, awake_threshold, awake_delta, sleep_onset_delta)\n","\n","        if epoch == total_epochs - 1:\n","          sleep_onset_threshold = convergence_point\n","          awake_threshold = convergence_point\n","\n","\n","        #if ifplot:\n","        print('New sleep onset threshold: ', sleep_onset_threshold)\n","        print('New awake threshold: ', awake_threshold)\n","        model_parameters = {'pre_sleep_window': sleep_onset_threshold,\n","                            'awake_window': awake_threshold,\n","                            'convergence_point': convergence_point,\n","                            'sliding_window_size': window_size_minutes}\n","\n","\n","        # Prepare the data\n","        train_sequences, train_labels, train_end_points, train_sleep_stages, index, label = prepare_classification_data(mydata_train = mydata_train, sleep_onset_threshold = sleep_onset_threshold,\n","                                                                                                                        awake_threshold = awake_threshold ,\n","                                                                                                                        method = method, filling_limit = filling_limit,\n","                                                                                                                        window_size_minutes = window_size_minutes, ifmissing = ifmissing,\n","                                                                                                                        random_seed = random_seed, resampling_method = resampling_method)\n","        #print('Train sequences: ', train_sequences.shape)\n","        #print('Train labels: ', train_labels.shape)\n","        # adjust the classification training loss accordidng to the class proportion in the training set to prevent imbalance\n","        # only works if ifclassweights = 1\n","        criterion_class, pos_weight = define_loss_function(ifclassweights, train_labels, device)\n","\n","        # Create the training set and dataloader\n","        train_labels_and_end_points = [(train_labels[i], train_end_points[i]) for i in range(len(train_labels))]\n","\n","        train_dataset = SleepDataset(train_sequences, train_labels_and_end_points)\n","        if shuffle_train_loader:\n","            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","        else:\n","            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n","\n","        print('--------------------------------------Evaluate classification ----------------------------------------')\n","\n","        ########################## TRAIN AND EVALUATE  #############################\n","        train_losses_class, test_losses_class, model_classification, optimizer_class, test_predictions_class, test_actuals_class, test_prediction_probabilities_class, test_metrics_class = train_and_evaluate_classification_model(model = model_classification, train_loader = train_loader,\n","                                                                                                                                                                                                                                    label = label, test_labels = test_labels,\n","                                                                                                                                                                                                                                    test_end_points = test_end_points,\n","                                                                                                                                                                                                                                    test_sleep_stages= test_sleep_stages,\n","                                                                                                                                                                                                                                    test_loader = test_loader, device = device,\n","                                                                                                                                                                                                                                    criterion = criterion_class,\n","                                                                                                                                                                                                                                    optimizer = optimizer_class,\n","                                                                                                                                                                                                                                    num_epochs = num_epochs,\n","                                                                                                                                                                                                                                    index = index,\n","                                                                                                                                                                                                                                    ifprobabilities = ifprobabilitiesanalysis,\n","                                                                                                                                                                                                                                    ifplot = ifplot,\n","                                                                                                                                                                                                                                    ifsaveplots=ifsaveplots,\n","                                                                                                                                                                                                                                    save_directory_epoch = save_directory_epoch,\n","                                                                                                                                                                                                                                    awake_threshold= awake_threshold,\n","                                                                                                                                                                                                                                    sleep_onset_threshold=sleep_onset_threshold,\n","                                                                                                                                                                                                                                    window_size_minutes=window_size_minutes)\n","\n","        # plot the losses for the classification model\n","        title = 'Training and validation losses during classification with window size of ' + str(window_size_minutes) + ' minutes'\n","        filename = 'classification_losses.png'\n","        plot_losses(train_losses_class, test_losses_class, title, ifplot, ifsaveplots,\n","                    save_directory = save_directory_epoch, filename = filename)\n","\n","        # Save the pos class weight in the test metrics dictionary\n","        if ifclassweights:\n","            test_metrics_class['pos_class_weight'] = pos_weight.cpu().detach().numpy()[0]\n","\n","\n","        # Get the stats for probability change points and mean prediction times in participants on the validaiton set\n","        _,  participant_dict = test_on_random_participants(data = mydata_test, model =  model_classification, criterion = criterion_class, random_seed = random_seed,\n","                                                                                            ifplot = False, device = device, output_path_new =  save_directory_epoch, index = index,\n","                                                                                            convergence_point = convergence_point,\n","                                                                                            awake_window = awake_threshold, pre_sleep_window = sleep_onset_threshold,\n","                                                                                            ifsaveplots = False, window_size_minutes = window_size_minutes, ifmissing = True,\n","                                                                                            filling_limit = filling_limit, method = method, resampling_method = None)\n","\n","\n","        if selected_participants is not None:\n","            participant_dataloaders, _ = test_on_random_participants(data = selected_participant_bothlabels, model = model_classification, criterion = criterion_class,\n","                                                                                                random_seed = random_seed, ifplot = ifplot, device = device, output_path_new =  save_directory_epoch, index = index,\n","                                                                                                convergence_point = convergence_point,\n","                                                                                                awake_window = awake_threshold, pre_sleep_window = sleep_onset_threshold, ifsaveplots = True,\n","                                                                                                window_size_minutes = window_size_minutes, ifmissing = True,\n","                                                                                                filling_limit = filling_limit, method = method, resampling_method = None)\n","\n","\n","\n","        # From the pretrained classification model, create a new model for regression and train it\n","        model_regression = PretrainedModelForRegression(model_classification).to(device)\n","        optimizer_reg = torch.optim.AdamW(model_regression.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","        print('--------------------------------------Evaluate regression ----------------------------------------')\n","\n","        train_losses_reg, test_losses_reg, model_regression, optimizer_reg, test_predictions_red, test_actuals, test_metrics_reg = train_and_evaluate_regression_model(model = model_regression, train_loader =train_loader,\n","                                                                                                                                        test_loader = test_loader, criterion = criterion_regression,\n","                                                                                                                                        optimizer = optimizer_reg, device = device, num_epochs = num_epochs, ifplot = ifplot,\n","                                                                                                                                        ifsaveplots = ifsaveplots, output_path = save_directory_epoch)\n","        if selected_participants is not None:\n","            test_reg_on_random_participants(participant_dataloaders = participant_dataloaders, model = model_regression,  criterion = criterion_regression,\n","                                                 device = device, random_seed = random_seed, ifplot = ifplot, output_path_new =save_directory_epoch, index = index)\n","\n","\n","\n","        # After each epoch, save the model\n","        save_models(model_classification, model_regression, save_directory_epoch, sleep_onset_threshold, awake_threshold, window_size_minutes)\n","\n","        # plot the losses for the regression model\n","        filename = 'regression_losses.png'\n","        title = 'Training and validation losses during regression with window size of ' + str(window_size_minutes) + ' minutes'\n","        plot_losses(train_losses_reg, test_losses_reg, title,\n","                    ifplot, ifsaveplots,\n","                    save_directory = save_directory_epoch, filename = filename)\n","\n","        # Add results to dataframe\n","        ifconverged = 0\n","        if epoch + 1 == total_epochs:\n","            ifconverged = 1\n","\n","        model_parameters['ifconverged'] = ifconverged\n","        model_parameters['initial_sleep_onset_threshold'] = initial_sleep_onset_threshold\n","\n","        # joing two dictionaries\n","        test_metrics = {**model_parameters, **test_metrics_class,  **participant_dict, **test_metrics_reg}\n","        # populate the results_df dataframe with the results from the current epoch\n","        new_row = pd.DataFrame(test_metrics, index = [epoch])\n","\n","        # Open the results dataframe and add the new results\n","        results_df_all = pd.read_csv(os.path.join(results_saving_dir, 'results.csv'))\n","        results_df_all = pd.concat([results_df_all, new_row], ignore_index=True)\n","        results_df_all.to_csv(os.path.join(results_saving_dir, 'results.csv'), index=False)\n","        del results_df_all\n","        del new_row\n","        del test_metrics\n","        del train_loader\n","\n","    # flatten the list of lists\n","    test_losses_reg_all = [item for sublist in test_losses_reg_all for item in sublist]\n","    train_losses_reg_all = [item for sublist in train_losses_reg_all for item in sublist]\n","\n","    # plot the losses across all epochs\n","    filename = 'all_regression_losses.png'\n","    title = 'Training and validation losses during all regression with window size of ' + str(window_size_minutes) + ' minutes'\n","    plot_losses(train_losses_reg_all, test_losses_reg_all, title,\n","                ifplot, ifsaveplots,\n","                save_directory = save_directory, filename = filename)\n","\n","\n","    # Clean up the cuda memory\n","    torch.cuda.empty_cache()\n","    # close all the plots\n","    plt.close('all')\n","    # clean up the memory\n","    gc.collect()\n","\n","    del mydata_train, mydata_test\n","\n","\n","\n"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"8PMRL5JIVJpI","executionInfo":{"status":"ok","timestamp":1693956094717,"user_tz":-60,"elapsed":10,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["selected_participants = np.array(selected_participants)"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"Xc85VmglVJpI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693956094717,"user_tz":-60,"elapsed":10,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}},"outputId":"0fbb9f31-8b71-452d-f7fc-68fd9f7c87df"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 110,  188,  912,  933, 1077, 2019, 2251, 2443, 2574, 2780, 3689,\n","       4512, 4618, 4998, 5255, 5568, 5801, 6279, 6321, 6475])"]},"metadata":{},"execution_count":43}],"source":["selected_participants"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"cX1eWnpgVJpJ","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1693956094718,"user_tz":-60,"elapsed":11,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}},"outputId":"ecbe1fd3-1ae1-4403-94de-3caa35936868"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["      Label  Sbj_ID  Age  Gender  Race1  Race2  Race3  Race4  ifCleanOnset  \\\n","0      29.9     110   60       1    1.0    0.0    0.0    0.0             1   \n","1      29.8     110   60       1    1.0    0.0    0.0    0.0             1   \n","2      29.7     110   60       1    1.0    0.0    0.0    0.0             1   \n","3      29.6     110   60       1    1.0    0.0    0.0    0.0             1   \n","4      29.5     110   60       1    1.0    0.0    0.0    0.0             1   \n","...     ...     ...  ...     ...    ...    ...    ...    ...           ...   \n","5995    0.4    6475   63       1    0.0    1.0    0.0    0.0             1   \n","5996    0.3    6475   63       1    0.0    1.0    0.0    0.0             1   \n","5997    0.2    6475   63       1    0.0    1.0    0.0    0.0             1   \n","5998    0.1    6475   63       1    0.0    1.0    0.0    0.0             1   \n","5999    0.0    6475   63       1    0.0    1.0    0.0    0.0             1   \n","\n","      Time2Sleep  ...  wrseltr5  slpapnea5  cpap5  dntaldv5  uvula5  insmnia5  \\\n","0           93.0  ...       1.0        0.0    0.0       0.0     0.0       0.0   \n","1           93.0  ...       1.0        0.0    0.0       0.0     0.0       0.0   \n","2           93.0  ...       1.0        0.0    0.0       0.0     0.0       0.0   \n","3           93.0  ...       1.0        0.0    0.0       0.0     0.0       0.0   \n","4           93.0  ...       1.0        0.0    0.0       0.0     0.0       0.0   \n","...          ...  ...       ...        ...    ...       ...     ...       ...   \n","5995        77.5  ...      -1.0        0.0    0.0       0.0     0.0       0.0   \n","5996        77.5  ...      -1.0        0.0    0.0       0.0     0.0       0.0   \n","5997        77.5  ...      -1.0        0.0    0.0       0.0     0.0       0.0   \n","5998        77.5  ...      -1.0        0.0    0.0       0.0     0.0       0.0   \n","5999        77.5  ...      -1.0        0.0    0.0       0.0     0.0       0.0   \n","\n","      rstlesslgs5  whiirs5c  epslpscl5c  hoostmeq5c  \n","0             0.0       6.0         4.0        11.0  \n","1             0.0       6.0         4.0        11.0  \n","2             0.0       6.0         4.0        11.0  \n","3             0.0       6.0         4.0        11.0  \n","4             0.0       6.0         4.0        11.0  \n","...           ...       ...         ...         ...  \n","5995          0.0       5.0         4.0        16.0  \n","5996          0.0       5.0         4.0        16.0  \n","5997          0.0       5.0         4.0        16.0  \n","5998          0.0       5.0         4.0        16.0  \n","5999          0.0       5.0         4.0        16.0  \n","\n","[6000 rows x 90 columns]"],"text/html":["\n","  <div id=\"df-f6a17eb9-d4a0-4909-a897-fbe9072b14c5\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Label</th>\n","      <th>Sbj_ID</th>\n","      <th>Age</th>\n","      <th>Gender</th>\n","      <th>Race1</th>\n","      <th>Race2</th>\n","      <th>Race3</th>\n","      <th>Race4</th>\n","      <th>ifCleanOnset</th>\n","      <th>Time2Sleep</th>\n","      <th>...</th>\n","      <th>wrseltr5</th>\n","      <th>slpapnea5</th>\n","      <th>cpap5</th>\n","      <th>dntaldv5</th>\n","      <th>uvula5</th>\n","      <th>insmnia5</th>\n","      <th>rstlesslgs5</th>\n","      <th>whiirs5c</th>\n","      <th>epslpscl5c</th>\n","      <th>hoostmeq5c</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>29.9</td>\n","      <td>110</td>\n","      <td>60</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>93.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>6.0</td>\n","      <td>4.0</td>\n","      <td>11.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>29.8</td>\n","      <td>110</td>\n","      <td>60</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>93.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>6.0</td>\n","      <td>4.0</td>\n","      <td>11.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>29.7</td>\n","      <td>110</td>\n","      <td>60</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>93.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>6.0</td>\n","      <td>4.0</td>\n","      <td>11.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>29.6</td>\n","      <td>110</td>\n","      <td>60</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>93.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>6.0</td>\n","      <td>4.0</td>\n","      <td>11.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>29.5</td>\n","      <td>110</td>\n","      <td>60</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>93.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>6.0</td>\n","      <td>4.0</td>\n","      <td>11.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5995</th>\n","      <td>0.4</td>\n","      <td>6475</td>\n","      <td>63</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>77.5</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.0</td>\n","      <td>4.0</td>\n","      <td>16.0</td>\n","    </tr>\n","    <tr>\n","      <th>5996</th>\n","      <td>0.3</td>\n","      <td>6475</td>\n","      <td>63</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>77.5</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.0</td>\n","      <td>4.0</td>\n","      <td>16.0</td>\n","    </tr>\n","    <tr>\n","      <th>5997</th>\n","      <td>0.2</td>\n","      <td>6475</td>\n","      <td>63</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>77.5</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.0</td>\n","      <td>4.0</td>\n","      <td>16.0</td>\n","    </tr>\n","    <tr>\n","      <th>5998</th>\n","      <td>0.1</td>\n","      <td>6475</td>\n","      <td>63</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>77.5</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.0</td>\n","      <td>4.0</td>\n","      <td>16.0</td>\n","    </tr>\n","    <tr>\n","      <th>5999</th>\n","      <td>0.0</td>\n","      <td>6475</td>\n","      <td>63</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>77.5</td>\n","      <td>...</td>\n","      <td>-1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.0</td>\n","      <td>4.0</td>\n","      <td>16.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>6000 rows × 90 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f6a17eb9-d4a0-4909-a897-fbe9072b14c5')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-f6a17eb9-d4a0-4909-a897-fbe9072b14c5 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-f6a17eb9-d4a0-4909-a897-fbe9072b14c5');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-bede6d49-81e8-4eb8-875b-0b605b4c7a0b\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bede6d49-81e8-4eb8-875b-0b605b4c7a0b')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-bede6d49-81e8-4eb8-875b-0b605b4c7a0b button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":44}],"source":["selected_participants_data"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"Us8gQFtbVJpJ","executionInfo":{"status":"ok","timestamp":1693956094718,"user_tz":-60,"elapsed":9,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["convergence_points =[4]"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"kG0FK-IhVJpJ","executionInfo":{"status":"ok","timestamp":1693956094718,"user_tz":-60,"elapsed":9,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["convergence_points = [4, 6, 8, 10, 12, 14]\n","convergence_starting_points4 = [2]\n","convergence_starting_points6 = [2, 4]\n","convergence_starting_points8 = [2, 4, 6]\n","convergence_starting_points10_14 = [2, 4, 6, 8]\n","convergence_starting_points = {4: convergence_starting_points4, 6: convergence_starting_points6, 8: convergence_starting_points8, 10: convergence_starting_points10_14, 12: convergence_starting_points10_14, 14: convergence_starting_points10_14}\n","\n","window_size2 = [0.5, 1, 1.5]\n","window_size4 = [0.5, 1, 2, 3]\n","window_size6_8= [0.5, 1, 2, 3, 4, 5]\n","starting_window_size = {4: window_size4, 6: window_size6_8, 8: window_size6_8}"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"f41rOmEXVJpJ","executionInfo":{"status":"ok","timestamp":1693956094718,"user_tz":-60,"elapsed":9,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["def hyperparameter_tuning_loop(mydata, convergence_starting_points_dict, starting_window_size_dict,\n","                            resampling_method = 'undersampling', results_saving_dir = 'regression_head_rnn_schedule_learning',\n","                            selected_participants = None, selected_participants_data = None, ifresetresults = False):\n","\n","    if not os.path.exists(results_saving_dir):\n","        os.makedirs(results_saving_dir)\n","\n","    # Initialize a DataFrame to store results\n","        # Initialize a DataFrame to store results\n","    columns = ['convergence_point', 'initial_sleep_onset_threshold','ifconverged', 'pre_sleep_window', 'awake_window', 'pos_class_weight',\n","                'sliding_window_size', 'Accuracy', 'precision', 'recall', 'f1_weighted',\n","                'f1_macro', 'f1_pre_sleep', 'f1_awake', 'auc',\n","                'one_crossing_percentage', 'mean_time_of_prediction',\n","                'std_time_of_prediction', 'one_crossing_percentage_smoothed',\n","                'mean_time_of_prediction_smoothed', 'std_time_of_prediction_smoothed',\n","                'MAE', 'MSE', 'RMSE', 'R2', 'custom_MSE']\n","\n","    if ifresetresults:\n","        results_df_all = pd.DataFrame(columns=columns)\n","        results_df_all.to_csv(os.path.join(results_saving_dir, 'results.csv'))\n","\n","\n","    if torch.cuda.is_available():\n","        device = torch.device(\"cuda\")\n","    else:\n","        device = torch.device('cpu')\n","    print(\"Using {}.\".format(device))\n","    for convergence_point in list(convergence_starting_points_dict.keys()):\n","        for starting_point in convergence_starting_points_dict[convergence_point]:\n","            window_sizes = starting_window_size_dict[starting_point]\n","            for window_size in window_sizes:\n","\n","                gc.collect()\n","                torch.cuda.empty_cache()\n","\n","                experiment_dir = os.path.join(results_saving_dir, f'convergence_point_{convergence_point}_starting_point_{starting_point}_window_size_{window_size}')\n","                if not os.path.exists(experiment_dir):\n","                    os.makedirs(experiment_dir)\n","\n","                print('Convergence point: ', convergence_point)\n","                print('Starting point: ', starting_point)\n","                print('Window size: ', window_size)\n","                training_loop_RNN_regclass_schedule(mydata, convergence_point, total_epochs = 3, filling_limit = 40, method = 'LOCF', ifclassweights = 1, input_size = 86,\n","                                            num_epochs = 10, hidden_size = 128, num_layers = 2, dropout = 0, l2 = 0, lr = 0.001, weight_decay = 0.001, batch_size = 64,\n","                                            device = device, random_seed = 42, window_size_minutes = window_size, ifprobabilitiesanalysis = 1,\n","                                            if_stratified_sampling = 1, initial_awake_threshold = 30, initial_sleep_onset_threshold = starting_point,\n","                                            convergence_sensitivity = 0.05, resampling_method = resampling_method, ifplot = False, ifsaveplots = True,\n","                                            output_path = experiment_dir, shuffle_train_loader = True, results_saving_dir = results_saving_dir, selected_participants_data = selected_participants_data,\n","                                            selected_participants = selected_participants, ifresetresults = False)\n","\n"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"8qZ5bEfgVJpJ","executionInfo":{"status":"ok","timestamp":1693956094718,"user_tz":-60,"elapsed":9,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"outputs":[],"source":["import gc\n","import torch\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"vsFbyf8xai7t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693956094718,"user_tz":-60,"elapsed":9,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}},"outputId":"7b461672-60f7-486a-e489-985f6645110c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":49}],"source":["torch.cuda.is_available()"]},{"cell_type":"markdown","metadata":{"id":"WrpCngQKVJpJ"},"source":["## Try and see if the loop works"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1204338,"status":"error","timestamp":1692964298325,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"},"user_tz":-60},"id":"Qt2augi5VJpJ","outputId":"81d7361d-caa9-4791-a411-43e20a6141c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda.\n","Convergence point:  4\n","Starting point:  2\n","Window size:  0.5\n","New sleep onset threshold:  2.7\n","New awake threshold:  21.3\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 58.58585858585859%\n","Mean time of prediction for participants with only one crossing: 11.408620689655171 minutes\n","STD of time of prediction for participants with only one crossing: 8.345993969497119 minutes\n","Percentage of participants with only one crossing (smoothed): 57.57575757575758%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.82280701754386 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.243168730357503 minutes\n","Percentage of participants with only one crossing: 50.0%\n","Mean time of prediction for participants with only one crossing: 12.69 minutes\n","STD of time of prediction for participants with only one crossing: 6.817690224702205 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.381818181818181 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.11640518073434 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  3.4\n","New awake threshold:  12.6\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 61.61616161616161%\n","Mean time of prediction for participants with only one crossing: 9.501639344262294 minutes\n","STD of time of prediction for participants with only one crossing: 7.314268377815673 minutes\n","Percentage of participants with only one crossing (smoothed): 58.58585858585859%\n","Mean time of prediction for participants with only one crossing (smoothed): 9.372413793103451 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.245421696003152 minutes\n","Percentage of participants with only one crossing: 50.0%\n","Mean time of prediction for participants with only one crossing: 11.09 minutes\n","STD of time of prediction for participants with only one crossing: 7.894358745332012 minutes\n","Percentage of participants with only one crossing (smoothed): 50.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.099999999999998 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.489325737341113 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  4.1\n","New awake threshold:  3.9\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 60.60606060606061%\n","Mean time of prediction for participants with only one crossing: 9.096666666666668 minutes\n","STD of time of prediction for participants with only one crossing: 7.194719514261059 minutes\n","Percentage of participants with only one crossing (smoothed): 63.63636363636363%\n","Mean time of prediction for participants with only one crossing (smoothed): 9.088888888888892 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.373507082924653 minutes\n","Percentage of participants with only one crossing: 45.0%\n","Mean time of prediction for participants with only one crossing: 11.666666666666664 minutes\n","STD of time of prediction for participants with only one crossing: 8.502940667792526 minutes\n","Percentage of participants with only one crossing (smoothed): 45.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.144444444444444 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 9.748060902457713 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  4\n","Starting point:  2\n","Window size:  1\n","New sleep onset threshold:  2.7\n","New awake threshold:  21.3\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 66.66666666666666%\n","Mean time of prediction for participants with only one crossing: 11.272727272727273 minutes\n","STD of time of prediction for participants with only one crossing: 7.513917298456517 minutes\n","Percentage of participants with only one crossing (smoothed): 66.66666666666666%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.82878787878788 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.460659755363972 minutes\n","Percentage of participants with only one crossing: 45.0%\n","Mean time of prediction for participants with only one crossing: 14.577777777777776 minutes\n","STD of time of prediction for participants with only one crossing: 8.404598976192574 minutes\n","Percentage of participants with only one crossing (smoothed): 60.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.616666666666665 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.1004972412535 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-86-796abfc392c9>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstarting_window_size_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfolder_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/sleep onset datasets/regression_head_rnn_schedule_learning_test'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mhyperparameter_tuning_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvergence_starting_points_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarting_window_size_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresampling_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'undersampling'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_saving_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfolder_name\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mselected_participants\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselected_participants\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_participants_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselected_participants_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mifresetresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-83-12ee4c6d0fc9>\u001b[0m in \u001b[0;36mhyperparameter_tuning_loop\u001b[0;34m(mydata, convergence_starting_points_dict, starting_window_size_dict, resampling_method, results_saving_dir, selected_participants, selected_participants_data, ifresetresults)\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Starting point: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarting_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Window size: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 training_loop_RNN_regclass_schedule(mydata, convergence_point, total_epochs = 3, filling_limit = 40, method = 'LOCF', ifclassweights = 1, input_size = 86,\n\u001b[0m\u001b[1;32m     44\u001b[0m                                             \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                                             \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size_minutes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mifprobabilitiesanalysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-84-d272c58df5c6>\u001b[0m in \u001b[0;36mtraining_loop_RNN_regclass_schedule\u001b[0;34m(mydata, convergence_point, total_epochs, filling_limit, method, ifclassweights, input_size, num_epochs, hidden_size, num_layers, dropout, l2, lr, weight_decay, batch_size, device, random_seed, window_size_minutes, ifprobabilitiesanalysis, if_stratified_sampling, initial_awake_threshold, initial_sleep_onset_threshold, convergence_sensitivity, resampling_method, ifplot, ifsaveplots, output_path, shuffle_train_loader, results_saving_dir, selected_participants_data, selected_participants, ifresetresults)\u001b[0m\n\u001b[1;32m    223\u001b[0m                                                                                                                                         ifsaveplots = ifsaveplots, output_path = save_directory_epoch)\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mselected_participants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             test_reg_on_random_participants(participant_dataloaders = participant_dataloaders, model = model_regression,  criterion = criterion_regression,\n\u001b[0m\u001b[1;32m    226\u001b[0m                                                  device = device, random_seed = random_seed, ifplot = ifplot, output_path_new =save_directory_epoch, index = index)\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-66-8ef5c9572579>\u001b[0m in \u001b[0;36mtest_reg_on_random_participants\u001b[0;34m(participant_dataloaders, model, criterion, device, random_seed, ifplot, output_path_new, index)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparticipant_dataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msbj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactuals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# get the accuracy of each timepoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-62-366d826ae075>\u001b[0m in \u001b[0;36mtest_regression\u001b[0;34m(model, test_loader, device, criterion, ifplot)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_and_endpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# these are outputs now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-54-9f6a14fd8fbb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Note: Here we're not initializing h0 and c0 as the original model does.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# This means they will default to 0s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregression_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    813\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    814\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import gc\n","import torch\n","gc.collect()\n","torch.cuda.empty_cache()\n","convergence_starting_points_test = {4:[2]}\n","starting_window_size_test = {2:[0.5, 1]}\n","folder_name = '/content/drive/MyDrive/sleep onset datasets/regression_head_rnn_schedule_learning_test'\n","hyperparameter_tuning_loop(data_train, convergence_starting_points_test, starting_window_size_test, resampling_method = 'undersampling', results_saving_dir = folder_name , selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = True)\n"]},{"cell_type":"markdown","metadata":{"id":"4PFX09CVVJpJ"},"source":["## Hypereparameter tune for window selection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_-aZwUg7VJpJ"},"outputs":[],"source":["convergence_points = [4, 6, 8, 10, 12, 14]\n","convergence_starting_points4 = [2]\n","convergence_starting_points6 = [2, 4]\n","convergence_starting_points8 = [2, 4, 6]\n","convergence_starting_points10_14 = [2, 4, 6, 8]\n","convergence_starting_points = {4: convergence_starting_points4, 6: convergence_starting_points6, 8: convergence_starting_points8, 10: convergence_starting_points10_14, 12: convergence_starting_points10_14, 14: convergence_starting_points10_14}\n","\n","window_size2 = [0.5, 1, 1.5]\n","window_size4 = [0.5, 1, 2, 3]\n","window_size6_8= [0.5, 1, 2, 3, 4, 5]\n","starting_window_size = {2:window_size2, 4: window_size4, 6: window_size6_8, 8: window_size6_8}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IP_E8tWeTLr6"},"outputs":[],"source":["folder_name = '/content/drive/MyDrive/sleep onset datasets/FIXED_regression_head_rnn_schedule_learning_mixed_onset'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16179690,"status":"ok","timestamp":1693496653273,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"},"user_tz":-60},"id":"N9eMy08NVJpJ","outputId":"5e1e10e5-56fa-4450-bb5d-1049c0274f5a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda.\n","Convergence point:  4\n","Starting point:  2\n","Window size:  0.5\n","New sleep onset threshold:  2.7\n","New awake threshold:  21.3\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 74.74747474747475%\n","Mean time of prediction for participants with only one crossing: 11.347297297297297 minutes\n","STD of time of prediction for participants with only one crossing: 7.576175726667383 minutes\n","Percentage of participants with only one crossing (smoothed): 73.73737373737373%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.549315068493152 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.360429619825399 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 13.718181818181819 minutes\n","STD of time of prediction for participants with only one crossing: 7.317204903739254 minutes\n","Percentage of participants with only one crossing (smoothed): 70.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.09285714285714 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.056006125486595 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  3.4\n","New awake threshold:  12.6\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 65.65656565656566%\n","Mean time of prediction for participants with only one crossing: 9.821538461538461 minutes\n","STD of time of prediction for participants with only one crossing: 6.851849661732389 minutes\n","Percentage of participants with only one crossing (smoothed): 66.66666666666666%\n","Mean time of prediction for participants with only one crossing (smoothed): 8.972727272727274 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.592110653274395 minutes\n","Percentage of participants with only one crossing: 40.0%\n","Mean time of prediction for participants with only one crossing: 14.162500000000001 minutes\n","STD of time of prediction for participants with only one crossing: 6.438931879589968 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 9.700000000000001 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.990252954195317 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  4\n","New awake threshold:  4\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 39.39393939393939%\n","Mean time of prediction for participants with only one crossing: 7.44102564102564 minutes\n","STD of time of prediction for participants with only one crossing: 5.9485278484214135 minutes\n","Percentage of participants with only one crossing (smoothed): 53.535353535353536%\n","Mean time of prediction for participants with only one crossing (smoothed): 6.945283018867924 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.019326297090374 minutes\n","Percentage of participants with only one crossing: 25.0%\n","Mean time of prediction for participants with only one crossing: 9.66 minutes\n","STD of time of prediction for participants with only one crossing: 9.451899280038907 minutes\n","Percentage of participants with only one crossing (smoothed): 40.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 9.95 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.246211251235321 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  4\n","Starting point:  2\n","Window size:  1\n","New sleep onset threshold:  2.7\n","New awake threshold:  21.3\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 59.59595959595959%\n","Mean time of prediction for participants with only one crossing: 7.277966101694915 minutes\n","STD of time of prediction for participants with only one crossing: 6.471006203596663 minutes\n","Percentage of participants with only one crossing (smoothed): 72.72727272727273%\n","Mean time of prediction for participants with only one crossing (smoothed): 7.241666666666668 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.904770905331082 minutes\n","Percentage of participants with only one crossing: 35.0%\n","Mean time of prediction for participants with only one crossing: 12.385714285714286 minutes\n","STD of time of prediction for participants with only one crossing: 7.4734632575014714 minutes\n","Percentage of participants with only one crossing (smoothed): 50.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.15 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.55116951065759 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  3.4\n","New awake threshold:  12.6\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 55.55555555555556%\n","Mean time of prediction for participants with only one crossing: 8.118181818181817 minutes\n","STD of time of prediction for participants with only one crossing: 6.695509781911265 minutes\n","Percentage of participants with only one crossing (smoothed): 69.6969696969697%\n","Mean time of prediction for participants with only one crossing (smoothed): 7.627536231884059 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.131573882591057 minutes\n","Percentage of participants with only one crossing: 35.0%\n","Mean time of prediction for participants with only one crossing: 10.500000000000002 minutes\n","STD of time of prediction for participants with only one crossing: 8.652827778907227 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 7.427272727272728 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.046480863664037 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  4\n","New awake threshold:  4\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 42.42424242424242%\n","Mean time of prediction for participants with only one crossing: 6.771428571428572 minutes\n","STD of time of prediction for participants with only one crossing: 6.430259037746225 minutes\n","Percentage of participants with only one crossing (smoothed): 65.65656565656566%\n","Mean time of prediction for participants with only one crossing (smoothed): 5.807692307692308 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 4.678504287361923 minutes\n","Percentage of participants with only one crossing: 25.0%\n","Mean time of prediction for participants with only one crossing: 10.580000000000002 minutes\n","STD of time of prediction for participants with only one crossing: 6.70325294166944 minutes\n","Percentage of participants with only one crossing (smoothed): 45.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.2 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.459051693633013 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  4\n","Starting point:  2\n","Window size:  1.5\n","New sleep onset threshold:  2.7\n","New awake threshold:  21.3\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 19.19191919191919%\n","Mean time of prediction for participants with only one crossing: 13.452631578947368 minutes\n","STD of time of prediction for participants with only one crossing: 6.4776838307866145 minutes\n","Percentage of participants with only one crossing (smoothed): 65.65656565656566%\n","Mean time of prediction for participants with only one crossing (smoothed): 5.784615384615384 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.835283415942939 minutes\n","Percentage of participants with only one crossing: 0.0%\n","Mean time of prediction for participants with only one crossing: nan minutes\n","STD of time of prediction for participants with only one crossing: nan minutes\n","Percentage of participants with only one crossing (smoothed): 30.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 2.533333333333333 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 0.372677996249965 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  3.4\n","New awake threshold:  12.6\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 22.22222222222222%\n","Mean time of prediction for participants with only one crossing: 12.563636363636363 minutes\n","STD of time of prediction for participants with only one crossing: 6.741967979114478 minutes\n","Percentage of participants with only one crossing (smoothed): 67.67676767676768%\n","Mean time of prediction for participants with only one crossing (smoothed): 6.095522388059702 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.736461877879468 minutes\n","Percentage of participants with only one crossing: 10.0%\n","Mean time of prediction for participants with only one crossing: 13.350000000000001 minutes\n","STD of time of prediction for participants with only one crossing: 2.05 minutes\n","Percentage of participants with only one crossing (smoothed): 45.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 5.144444444444444 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 4.2716406192757965 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  4\n","New awake threshold:  4\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 13.131313131313133%\n","Mean time of prediction for participants with only one crossing: 11.415384615384614 minutes\n","STD of time of prediction for participants with only one crossing: 4.79741054413671 minutes\n","Percentage of participants with only one crossing (smoothed): 56.56565656565656%\n","Mean time of prediction for participants with only one crossing (smoothed): 4.941071428571428 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 4.571838360539221 minutes\n","Percentage of participants with only one crossing: 15.0%\n","Mean time of prediction for participants with only one crossing: 10.5 minutes\n","STD of time of prediction for participants with only one crossing: 2.619160170741759 minutes\n","Percentage of participants with only one crossing (smoothed): 30.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 4.533333333333333 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 2.656229575084871 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n"]}],"source":["\n","convergence_starting_points = {4: [2]}\n","window_size2 = [0.5,1, 1.5]\n","starting_window_size = {2:window_size2}\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17195590,"status":"ok","timestamp":1693513848860,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"},"user_tz":-60},"id":"Lo-4wccK2L9y","outputId":"aecca6a6-344a-4c5b-a800-f5907352c5ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda.\n","Convergence point:  6\n","Starting point:  2\n","Window size:  0.5\n","New sleep onset threshold:  3.3\n","New awake threshold:  22.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 63.63636363636363%\n","Mean time of prediction for participants with only one crossing: 12.352380952380953 minutes\n","STD of time of prediction for participants with only one crossing: 7.388727088061882 minutes\n","Percentage of participants with only one crossing (smoothed): 70.70707070707071%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.042857142857146 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.724054661132535 minutes\n","Percentage of participants with only one crossing: 40.0%\n","Mean time of prediction for participants with only one crossing: 12.45 minutes\n","STD of time of prediction for participants with only one crossing: 7.846018098373212 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 9.563636363636364 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.034275746877219 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  4.6\n","New awake threshold:  14.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 51.515151515151516%\n","Mean time of prediction for participants with only one crossing: 10.094117647058823 minutes\n","STD of time of prediction for participants with only one crossing: 6.106551023116394 minutes\n","Percentage of participants with only one crossing (smoothed): 60.60606060606061%\n","Mean time of prediction for participants with only one crossing (smoothed): 8.691666666666668 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.28655421426467 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 12.990909090909092 minutes\n","STD of time of prediction for participants with only one crossing: 8.60247898155521 minutes\n","Percentage of participants with only one crossing (smoothed): 60.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.158333333333331 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.625201285896011 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  6\n","New awake threshold:  6\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 48.484848484848484%\n","Mean time of prediction for participants with only one crossing: 10.88125 minutes\n","STD of time of prediction for participants with only one crossing: 6.963316267232159 minutes\n","Percentage of participants with only one crossing (smoothed): 60.60606060606061%\n","Mean time of prediction for participants with only one crossing (smoothed): 8.558333333333334 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.587869522394866 minutes\n","Percentage of participants with only one crossing: 30.0%\n","Mean time of prediction for participants with only one crossing: 9.066666666666666 minutes\n","STD of time of prediction for participants with only one crossing: 4.552532872539808 minutes\n","Percentage of participants with only one crossing (smoothed): 50.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 7.7 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.168468205316454 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  6\n","Starting point:  2\n","Window size:  1\n","New sleep onset threshold:  3.3\n","New awake threshold:  22.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 69.6969696969697%\n","Mean time of prediction for participants with only one crossing: 10.892753623188407 minutes\n","STD of time of prediction for participants with only one crossing: 6.939580216193169 minutes\n","Percentage of participants with only one crossing (smoothed): 70.70707070707071%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.278571428571434 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.315168055039431 minutes\n","Percentage of participants with only one crossing: 50.0%\n","Mean time of prediction for participants with only one crossing: 15.59 minutes\n","STD of time of prediction for participants with only one crossing: 9.38301124373194 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.881818181818181 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.528755443878525 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  4.6\n","New awake threshold:  14.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 65.65656565656566%\n","Mean time of prediction for participants with only one crossing: 10.52153846153846 minutes\n","STD of time of prediction for participants with only one crossing: 7.301790977512979 minutes\n","Percentage of participants with only one crossing (smoothed): 71.71717171717171%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.467605633802822 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.0689914942574195 minutes\n","Percentage of participants with only one crossing: 40.0%\n","Mean time of prediction for participants with only one crossing: 11.0 minutes\n","STD of time of prediction for participants with only one crossing: 4.476047363467013 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 9.972727272727273 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 4.821182598999187 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  6\n","New awake threshold:  6\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 56.56565656565656%\n","Mean time of prediction for participants with only one crossing: 9.846428571428572 minutes\n","STD of time of prediction for participants with only one crossing: 7.250885765736149 minutes\n","Percentage of participants with only one crossing (smoothed): 52.52525252525253%\n","Mean time of prediction for participants with only one crossing (smoothed): 8.834615384615384 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.074544959962623 minutes\n","Percentage of participants with only one crossing: 30.0%\n","Mean time of prediction for participants with only one crossing: 12.166666666666666 minutes\n","STD of time of prediction for participants with only one crossing: 6.937979212678254 minutes\n","Percentage of participants with only one crossing (smoothed): 40.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.5125 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.367363761661771 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  6\n","Starting point:  2\n","Window size:  1.5\n","New sleep onset threshold:  3.3\n","New awake threshold:  22.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 63.63636363636363%\n","Mean time of prediction for participants with only one crossing: 10.80952380952381 minutes\n","STD of time of prediction for participants with only one crossing: 7.5231012324835165 minutes\n","Percentage of participants with only one crossing (smoothed): 72.72727272727273%\n","Mean time of prediction for participants with only one crossing (smoothed): 9.255555555555556 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.101719058104658 minutes\n","Percentage of participants with only one crossing: 30.0%\n","Mean time of prediction for participants with only one crossing: 16.666666666666668 minutes\n","STD of time of prediction for participants with only one crossing: 10.313367808604305 minutes\n","Percentage of participants with only one crossing (smoothed): 45.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.366666666666667 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 10.44030650891055 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  4.6\n","New awake threshold:  14.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 68.68686868686868%\n","Mean time of prediction for participants with only one crossing: 11.332352941176469 minutes\n","STD of time of prediction for participants with only one crossing: 8.054400353341215 minutes\n","Percentage of participants with only one crossing (smoothed): 78.78787878787878%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.232051282051287 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.090965103438677 minutes\n","Percentage of participants with only one crossing: 35.0%\n","Mean time of prediction for participants with only one crossing: 13.242857142857144 minutes\n","STD of time of prediction for participants with only one crossing: 7.251178651552772 minutes\n","Percentage of participants with only one crossing (smoothed): 35.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.842857142857143 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 9.959612319313457 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  6\n","New awake threshold:  6\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 40.4040404040404%\n","Mean time of prediction for participants with only one crossing: 9.2975 minutes\n","STD of time of prediction for participants with only one crossing: 5.620252996974425 minutes\n","Percentage of participants with only one crossing (smoothed): 55.55555555555556%\n","Mean time of prediction for participants with only one crossing (smoothed): 7.518181818181817 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.914822395215443 minutes\n","Percentage of participants with only one crossing: 40.0%\n","Mean time of prediction for participants with only one crossing: 13.775 minutes\n","STD of time of prediction for participants with only one crossing: 8.267821659905346 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.472727272727273 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.645449765795446 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n"]}],"source":["\n","convergence_starting_points = {6: [2]}\n","window_size2 = [0.5, 1, 1.5]\n","starting_window_size = {2:window_size2}\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O_E2aGee2Syv","outputId":"729f71e5-e930-451c-b846-f326189a9e21"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda.\n","Convergence point:  6\n","Starting point:  4\n","Window size:  0.5\n","New sleep onset threshold:  4.7\n","New awake threshold:  22.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 60.60606060606061%\n","Mean time of prediction for participants with only one crossing: 12.141666666666664 minutes\n","STD of time of prediction for participants with only one crossing: 6.627978366155266 minutes\n","Percentage of participants with only one crossing (smoothed): 64.64646464646465%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.098437500000001 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.602070986334042 minutes\n","Percentage of participants with only one crossing: 45.0%\n","Mean time of prediction for participants with only one crossing: 15.122222222222222 minutes\n","STD of time of prediction for participants with only one crossing: 7.337086581618949 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.109090909090908 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.991961515755417 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  5.4\n","New awake threshold:  14.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 54.54545454545454%\n","Mean time of prediction for participants with only one crossing: 11.14814814814815 minutes\n","STD of time of prediction for participants with only one crossing: 6.963974512811571 minutes\n","Percentage of participants with only one crossing (smoothed): 62.62626262626263%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.43387096774194 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.154244026887389 minutes\n","Percentage of participants with only one crossing: 50.0%\n","Mean time of prediction for participants with only one crossing: 10.589999999999998 minutes\n","STD of time of prediction for participants with only one crossing: 6.526630064589229 minutes\n","Percentage of participants with only one crossing (smoothed): 65.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.66153846153846 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.16526776248863 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  6\n","New awake threshold:  6\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 52.52525252525253%\n","Mean time of prediction for participants with only one crossing: 9.959615384615384 minutes\n","STD of time of prediction for participants with only one crossing: 6.096728618863645 minutes\n","Percentage of participants with only one crossing (smoothed): 62.62626262626263%\n","Mean time of prediction for participants with only one crossing (smoothed): 9.68387096774194 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.888494230058136 minutes\n","Percentage of participants with only one crossing: 20.0%\n","Mean time of prediction for participants with only one crossing: 16.175 minutes\n","STD of time of prediction for participants with only one crossing: 8.434564304100123 minutes\n","Percentage of participants with only one crossing (smoothed): 45.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.088888888888889 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 9.749010712710454 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  6\n","Starting point:  4\n","Window size:  1\n","New sleep onset threshold:  4.7\n","New awake threshold:  22.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 64.64646464646465%\n","Mean time of prediction for participants with only one crossing: 10.6671875 minutes\n","STD of time of prediction for participants with only one crossing: 7.238656096945326 minutes\n","Percentage of participants with only one crossing (smoothed): 65.65656565656566%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.753846153846155 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.662156292666865 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 13.845454545454546 minutes\n","STD of time of prediction for participants with only one crossing: 8.029532267184189 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.336363636363636 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.944883365376101 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  5.4\n","New awake threshold:  14.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 60.60606060606061%\n","Mean time of prediction for participants with only one crossing: 10.921666666666665 minutes\n","STD of time of prediction for participants with only one crossing: 6.990591097245561 minutes\n","Percentage of participants with only one crossing (smoothed): 59.59595959595959%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.386440677966105 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.432767220324988 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 15.245454545454544 minutes\n","STD of time of prediction for participants with only one crossing: 9.018200146608942 minutes\n","Percentage of participants with only one crossing (smoothed): 65.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.507692307692302 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.15366472947656 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  6\n","New awake threshold:  6\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 61.61616161616161%\n","Mean time of prediction for participants with only one crossing: 10.199999999999998 minutes\n","STD of time of prediction for participants with only one crossing: 7.277407212579554 minutes\n","Percentage of participants with only one crossing (smoothed): 60.60606060606061%\n","Mean time of prediction for participants with only one crossing (smoothed): 9.575000000000005 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.025919275558656 minutes\n","Percentage of participants with only one crossing: 40.0%\n","Mean time of prediction for participants with only one crossing: 13.975 minutes\n","STD of time of prediction for participants with only one crossing: 9.40196123157291 minutes\n","Percentage of participants with only one crossing (smoothed): 45.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.144444444444444 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 9.032178414622923 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  6\n","Starting point:  4\n","Window size:  2\n","New sleep onset threshold:  4.7\n","New awake threshold:  22.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 43.43434343434344%\n","Mean time of prediction for participants with only one crossing: 19.469767441860466 minutes\n","STD of time of prediction for participants with only one crossing: 8.866622410431862 minutes\n","Percentage of participants with only one crossing (smoothed): 52.52525252525253%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.67115384615385 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 9.706962935845064 minutes\n","Percentage of participants with only one crossing: 40.0%\n","Mean time of prediction for participants with only one crossing: 23.7 minutes\n","STD of time of prediction for participants with only one crossing: 7.028869041318098 minutes\n","Percentage of participants with only one crossing (smoothed): 70.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.23571428571428 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 10.860255662792865 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  5.4\n","New awake threshold:  14.0\n","--------------------------------------Evaluate classification ----------------------------------------\n"]}],"source":["\n","convergence_starting_points = {6: [4]}\n","window_size4 = [0.5, 1, 2, 3]\n","starting_window_size = {4:window_size4}\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7841152,"status":"ok","timestamp":1693532881396,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"},"user_tz":-60},"id":"KUmnywNj9NvB","outputId":"49d17553-9d69-4d40-c3bf-ee2362dfee75"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda.\n","Convergence point:  6\n","Starting point:  4\n","Window size:  2\n","New sleep onset threshold:  4.7\n","New awake threshold:  22.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 41.41414141414141%\n","Mean time of prediction for participants with only one crossing: 19.704878048780486 minutes\n","STD of time of prediction for participants with only one crossing: 8.809389425964698 minutes\n","Percentage of participants with only one crossing (smoothed): 46.464646464646464%\n","Mean time of prediction for participants with only one crossing (smoothed): 16.428260869565225 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 9.04079313189314 minutes\n","Percentage of participants with only one crossing: 45.0%\n","Mean time of prediction for participants with only one crossing: 21.66666666666667 minutes\n","STD of time of prediction for participants with only one crossing: 8.606586624982828 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 16.154545454545453 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.884241783411554 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  5.4\n","New awake threshold:  14.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 44.44444444444444%\n","Mean time of prediction for participants with only one crossing: 15.988636363636363 minutes\n","STD of time of prediction for participants with only one crossing: 9.28388672302449 minutes\n","Percentage of participants with only one crossing (smoothed): 52.52525252525253%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.373076923076926 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.816987872733465 minutes\n","Percentage of participants with only one crossing: 25.0%\n","Mean time of prediction for participants with only one crossing: 18.759999999999998 minutes\n","STD of time of prediction for participants with only one crossing: 10.344776459643775 minutes\n","Percentage of participants with only one crossing (smoothed): 35.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.057142857142859 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 10.676122509447644 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  6\n","New awake threshold:  6\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 29.292929292929294%\n","Mean time of prediction for participants with only one crossing: 12.710344827586209 minutes\n","STD of time of prediction for participants with only one crossing: 8.285647949922772 minutes\n","Percentage of participants with only one crossing (smoothed): 42.42424242424242%\n","Mean time of prediction for participants with only one crossing (smoothed): 9.985714285714286 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.170899916299113 minutes\n","Percentage of participants with only one crossing: 25.0%\n","Mean time of prediction for participants with only one crossing: 19.96 minutes\n","STD of time of prediction for participants with only one crossing: 9.32772212279075 minutes\n","Percentage of participants with only one crossing (smoothed): 45.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.588888888888889 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 10.56928931116988 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  6\n","Starting point:  4\n","Window size:  3\n","New sleep onset threshold:  4.7\n","New awake threshold:  22.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 25.252525252525253%\n","Mean time of prediction for participants with only one crossing: 15.847999999999999 minutes\n","STD of time of prediction for participants with only one crossing: 9.213169704287445 minutes\n","Percentage of participants with only one crossing (smoothed): 39.39393939393939%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.584615384615386 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.310066137585512 minutes\n","Percentage of participants with only one crossing: 15.0%\n","Mean time of prediction for participants with only one crossing: 18.2 minutes\n","STD of time of prediction for participants with only one crossing: 8.595735376724127 minutes\n","Percentage of participants with only one crossing (smoothed): 15.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 17.366666666666667 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.498365855987975 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  5.4\n","New awake threshold:  14.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 30.303030303030305%\n","Mean time of prediction for participants with only one crossing: 11.673333333333336 minutes\n","STD of time of prediction for participants with only one crossing: 8.347891283964405 minutes\n","Percentage of participants with only one crossing (smoothed): 38.38383838383838%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.252631578947366 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.727930779155816 minutes\n","Percentage of participants with only one crossing: 30.0%\n","Mean time of prediction for participants with only one crossing: 10.083333333333332 minutes\n","STD of time of prediction for participants with only one crossing: 5.812749397277983 minutes\n","Percentage of participants with only one crossing (smoothed): 20.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.7 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.363961030678928 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  6\n","New awake threshold:  6\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 28.28282828282828%\n","Mean time of prediction for participants with only one crossing: 13.278571428571428 minutes\n","STD of time of prediction for participants with only one crossing: 8.807762693980202 minutes\n","Percentage of participants with only one crossing (smoothed): 33.33333333333333%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.109090909090908 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.7454332052629615 minutes\n","Percentage of participants with only one crossing: 20.0%\n","Mean time of prediction for participants with only one crossing: 12.2 minutes\n","STD of time of prediction for participants with only one crossing: 6.14288205975013 minutes\n","Percentage of participants with only one crossing (smoothed): 25.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.5 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.1773780845922 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n"]}],"source":["\n","convergence_starting_points = {6: [4]}\n","window_size4 = [2, 3]\n","starting_window_size = {4:window_size4}\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17201259,"status":"ok","timestamp":1693550082650,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"},"user_tz":-60},"id":"OcBTSiV07afE","outputId":"1a4c31c2-a0e0-4978-af0d-2d530d950a9e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda.\n","Convergence point:  8\n","Starting point:  2\n","Window size:  0.5\n","New sleep onset threshold:  4.0\n","New awake threshold:  22.7\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 70.70707070707071%\n","Mean time of prediction for participants with only one crossing: 11.467142857142857 minutes\n","STD of time of prediction for participants with only one crossing: 6.905768218951281 minutes\n","Percentage of participants with only one crossing (smoothed): 66.66666666666666%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.268181818181821 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.966307640978851 minutes\n","Percentage of participants with only one crossing: 60.0%\n","Mean time of prediction for participants with only one crossing: 11.4 minutes\n","STD of time of prediction for participants with only one crossing: 7.98373346248483 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.063636363636364 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.71683787421114 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  6.0\n","New awake threshold:  15.4\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 67.67676767676768%\n","Mean time of prediction for participants with only one crossing: 10.782089552238803 minutes\n","STD of time of prediction for participants with only one crossing: 7.037976417386029 minutes\n","Percentage of participants with only one crossing (smoothed): 70.70707070707071%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.421428571428574 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.107659708414031 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 13.509090909090908 minutes\n","STD of time of prediction for participants with only one crossing: 9.134893594987057 minutes\n","Percentage of participants with only one crossing (smoothed): 60.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.283333333333331 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 9.451263877857231 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  8\n","New awake threshold:  8\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 63.63636363636363%\n","Mean time of prediction for participants with only one crossing: 10.855555555555558 minutes\n","STD of time of prediction for participants with only one crossing: 6.644744379279101 minutes\n","Percentage of participants with only one crossing (smoothed): 59.59595959595959%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.428813559322037 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.714089684184579 minutes\n","Percentage of participants with only one crossing: 35.0%\n","Mean time of prediction for participants with only one crossing: 9.457142857142857 minutes\n","STD of time of prediction for participants with only one crossing: 6.084372756228427 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.927272727272726 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.42443317856847 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  8\n","Starting point:  2\n","Window size:  1\n","New sleep onset threshold:  4.0\n","New awake threshold:  22.7\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 69.6969696969697%\n","Mean time of prediction for participants with only one crossing: 11.273913043478263 minutes\n","STD of time of prediction for participants with only one crossing: 7.421388052791724 minutes\n","Percentage of participants with only one crossing (smoothed): 74.74747474747475%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.929729729729733 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.488364482959994 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 13.736363636363636 minutes\n","STD of time of prediction for participants with only one crossing: 7.991617509191937 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.336363636363636 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.644732787002598 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  6.0\n","New awake threshold:  15.4\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 71.71717171717171%\n","Mean time of prediction for participants with only one crossing: 11.595774647887325 minutes\n","STD of time of prediction for participants with only one crossing: 7.185224209106625 minutes\n","Percentage of participants with only one crossing (smoothed): 70.70707070707071%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.78571428571429 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.453652713637133 minutes\n","Percentage of participants with only one crossing: 50.0%\n","Mean time of prediction for participants with only one crossing: 15.259999999999996 minutes\n","STD of time of prediction for participants with only one crossing: 9.085945190237501 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.245454545454544 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 9.71281848771028 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  8\n","New awake threshold:  8\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 71.71717171717171%\n","Mean time of prediction for participants with only one crossing: 10.3887323943662 minutes\n","STD of time of prediction for participants with only one crossing: 6.959531902985208 minutes\n","Percentage of participants with only one crossing (smoothed): 66.66666666666666%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.881818181818183 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.345188264000373 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 14.554545454545455 minutes\n","STD of time of prediction for participants with only one crossing: 10.069518688178212 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.427272727272724 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 9.891143045361423 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  8\n","Starting point:  2\n","Window size:  1.5\n","New sleep onset threshold:  4.0\n","New awake threshold:  22.7\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 71.71717171717171%\n","Mean time of prediction for participants with only one crossing: 11.923943661971828 minutes\n","STD of time of prediction for participants with only one crossing: 8.342179685928551 minutes\n","Percentage of participants with only one crossing (smoothed): 66.66666666666666%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.033333333333335 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.424950918539404 minutes\n","Percentage of participants with only one crossing: 60.0%\n","Mean time of prediction for participants with only one crossing: 15.191666666666665 minutes\n","STD of time of prediction for participants with only one crossing: 9.906938505691631 minutes\n","Percentage of participants with only one crossing (smoothed): 65.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.661538461538457 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 9.540322399127291 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  6.0\n","New awake threshold:  15.4\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 74.74747474747475%\n","Mean time of prediction for participants with only one crossing: 11.285135135135135 minutes\n","STD of time of prediction for participants with only one crossing: 8.112364310820134 minutes\n","Percentage of participants with only one crossing (smoothed): 73.73737373737373%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.02191780821918 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.968417910303432 minutes\n","Percentage of participants with only one crossing: 40.0%\n","Mean time of prediction for participants with only one crossing: 14.325 minutes\n","STD of time of prediction for participants with only one crossing: 9.115337349763857 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 16.018181818181816 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.92970712965927 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  8\n","New awake threshold:  8\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 61.61616161616161%\n","Mean time of prediction for participants with only one crossing: 12.001639344262296 minutes\n","STD of time of prediction for participants with only one crossing: 8.277373698543352 minutes\n","Percentage of participants with only one crossing (smoothed): 63.63636363636363%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.430158730158736 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.126395067937661 minutes\n","Percentage of participants with only one crossing: 30.0%\n","Mean time of prediction for participants with only one crossing: 17.1 minutes\n","STD of time of prediction for participants with only one crossing: 8.717797887081346 minutes\n","Percentage of participants with only one crossing (smoothed): 40.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.5125 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 9.198972429027059 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n"]}],"source":["convergence_starting_points = {8: [2]}\n","window_size2 = [1, 1.5]\n","starting_window_size = {2:window_size2}\n","older_name = '/content/drive/MyDrive/sleep onset datasets/regression_head_rnn_schedule_learning_mixed_onset'\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":414,"status":"ok","timestamp":1693686293227,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"},"user_tz":-60},"id":"xK3lam2CkQ0i","outputId":"73eb40a0-aa54-44b0-b27c-f79885115339"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/sleep onset datasets/FIXED_regression_head_rnn_schedule_learning_mixed_onset'"]},"execution_count":87,"metadata":{},"output_type":"execute_result"}],"source":["folder_name"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tR96qZ5h7riA","outputId":"64a522ac-7395-4110-b069-364edbda8bd8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda.\n","Convergence point:  8\n","Starting point:  4\n","Window size:  0.5\n","New sleep onset threshold:  5.3\n","New awake threshold:  22.7\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 71.71717171717171%\n","Mean time of prediction for participants with only one crossing: 12.590140845070422 minutes\n","STD of time of prediction for participants with only one crossing: 7.483712924417413 minutes\n","Percentage of participants with only one crossing (smoothed): 66.66666666666666%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.374242424242425 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.658176668151789 minutes\n","Percentage of participants with only one crossing: 60.0%\n","Mean time of prediction for participants with only one crossing: 14.308333333333332 minutes\n","STD of time of prediction for participants with only one crossing: 9.139881320649385 minutes\n","Percentage of participants with only one crossing (smoothed): 70.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.985714285714282 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 9.361231385946214 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  6.6\n","New awake threshold:  15.4\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 69.6969696969697%\n","Mean time of prediction for participants with only one crossing: 11.06521739130435 minutes\n","STD of time of prediction for participants with only one crossing: 7.014763439501101 minutes\n","Percentage of participants with only one crossing (smoothed): 66.66666666666666%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.17727272727273 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.8763052855559925 minutes\n","Percentage of participants with only one crossing: 30.0%\n","Mean time of prediction for participants with only one crossing: 12.766666666666666 minutes\n","STD of time of prediction for participants with only one crossing: 8.110624034081608 minutes\n","Percentage of participants with only one crossing (smoothed): 35.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.771428571428572 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.226388599364554 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  8\n","New awake threshold:  8\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 57.57575757575758%\n","Mean time of prediction for participants with only one crossing: 9.752631578947367 minutes\n","STD of time of prediction for participants with only one crossing: 6.291489803852917 minutes\n","Percentage of participants with only one crossing (smoothed): 58.58585858585859%\n","Mean time of prediction for participants with only one crossing (smoothed): 9.01896551724138 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.903460612700612 minutes\n","Percentage of participants with only one crossing: 25.0%\n","Mean time of prediction for participants with only one crossing: 15.280000000000001 minutes\n","STD of time of prediction for participants with only one crossing: 7.617453642786415 minutes\n","Percentage of participants with only one crossing (smoothed): 30.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.033333333333333 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.924062353485036 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  8\n","Starting point:  4\n","Window size:  1\n","New sleep onset threshold:  5.3\n","New awake threshold:  22.7\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 71.71717171717171%\n","Mean time of prediction for participants with only one crossing: 13.225352112676054 minutes\n","STD of time of prediction for participants with only one crossing: 7.638739633334535 minutes\n","Percentage of participants with only one crossing (smoothed): 68.68686868686868%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.405882352941179 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.70276654564757 minutes\n","Percentage of participants with only one crossing: 60.0%\n","Mean time of prediction for participants with only one crossing: 14.699999999999998 minutes\n","STD of time of prediction for participants with only one crossing: 9.086436778701172 minutes\n","Percentage of participants with only one crossing (smoothed): 50.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.949999999999998 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.947765084086639 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  6.6\n","New awake threshold:  15.4\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 69.6969696969697%\n","Mean time of prediction for participants with only one crossing: 12.544927536231883 minutes\n","STD of time of prediction for participants with only one crossing: 8.03240784070298 minutes\n","Percentage of participants with only one crossing (smoothed): 65.65656565656566%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.823076923076924 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.857397675111783 minutes\n","Percentage of participants with only one crossing: 65.0%\n","Mean time of prediction for participants with only one crossing: 17.569230769230767 minutes\n","STD of time of prediction for participants with only one crossing: 10.025381988915367 minutes\n","Percentage of participants with only one crossing (smoothed): 50.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.55 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.905195112966362 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  8\n","New awake threshold:  8\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 61.61616161616161%\n","Mean time of prediction for participants with only one crossing: 11.511475409836066 minutes\n","STD of time of prediction for participants with only one crossing: 7.355963307672324 minutes\n","Percentage of participants with only one crossing (smoothed): 62.62626262626263%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.296774193548393 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.5550873213274246 minutes\n","Percentage of participants with only one crossing: 50.0%\n","Mean time of prediction for participants with only one crossing: 12.370000000000001 minutes\n","STD of time of prediction for participants with only one crossing: 8.939244934556832 minutes\n","Percentage of participants with only one crossing (smoothed): 50.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 9.05 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.741846927956759 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  8\n","Starting point:  4\n","Window size:  2\n","New sleep onset threshold:  5.3\n","New awake threshold:  22.7\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 70.70707070707071%\n","Mean time of prediction for participants with only one crossing: 14.398571428571428 minutes\n","STD of time of prediction for participants with only one crossing: 8.848655956974367 minutes\n","Percentage of participants with only one crossing (smoothed): 67.67676767676768%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.274626865671642 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.778473387214003 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 17.463636363636365 minutes\n","STD of time of prediction for participants with only one crossing: 9.44556391571993 minutes\n","Percentage of participants with only one crossing (smoothed): 60.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 16.074999999999996 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 9.920779959257235 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  6.6\n","New awake threshold:  15.4\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 71.71717171717171%\n","Mean time of prediction for participants with only one crossing: 11.80281690140845 minutes\n","STD of time of prediction for participants with only one crossing: 8.384525851543861 minutes\n","Percentage of participants with only one crossing (smoothed): 68.68686868686868%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.266176470588237 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.34147346086229 minutes\n","Percentage of participants with only one crossing: 45.0%\n","Mean time of prediction for participants with only one crossing: 10.777777777777779 minutes\n","STD of time of prediction for participants with only one crossing: 8.530830072647976 minutes\n","Percentage of participants with only one crossing (smoothed): 45.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.81111111111111 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.395692605198672 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  8\n","New awake threshold:  8\n","--------------------------------------Evaluate classification ----------------------------------------\n"]}],"source":["\n","convergence_starting_points = {8: [4]}\n","window_size4 = [0.5, 1, 2, 3]\n","starting_window_size = {4:window_size4}\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2804772,"status":"ok","timestamp":1693602190257,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"},"user_tz":-60},"id":"g3Y4Y5PWY07M","outputId":"0d949743-45b2-48c7-8650-f70e5978446a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda.\n","Convergence point:  8\n","Starting point:  4\n","Window size:  3\n","New sleep onset threshold:  5.3\n","New awake threshold:  22.7\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 52.52525252525253%\n","Mean time of prediction for participants with only one crossing: 14.44230769230769 minutes\n","STD of time of prediction for participants with only one crossing: 8.256683683161599 minutes\n","Percentage of participants with only one crossing (smoothed): 51.515151515151516%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.631372549019611 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.314893092260238 minutes\n","Percentage of participants with only one crossing: 50.0%\n","Mean time of prediction for participants with only one crossing: 14.690000000000001 minutes\n","STD of time of prediction for participants with only one crossing: 8.65383729914077 minutes\n","Percentage of participants with only one crossing (smoothed): 45.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.977777777777778 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.6416661617997335 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  6.6\n","New awake threshold:  15.4\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 58.58585858585859%\n","Mean time of prediction for participants with only one crossing: 14.737931034482758 minutes\n","STD of time of prediction for participants with only one crossing: 8.538711595983418 minutes\n","Percentage of participants with only one crossing (smoothed): 62.62626262626263%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.030645161290328 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.258887465295313 minutes\n","Percentage of participants with only one crossing: 45.0%\n","Mean time of prediction for participants with only one crossing: 13.2 minutes\n","STD of time of prediction for participants with only one crossing: 8.303011501858828 minutes\n","Percentage of participants with only one crossing (smoothed): 50.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.0 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.075270893288968 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  8\n","New awake threshold:  8\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 40.4040404040404%\n","Mean time of prediction for participants with only one crossing: 10.98 minutes\n","STD of time of prediction for participants with only one crossing: 7.337274698414936 minutes\n","Percentage of participants with only one crossing (smoothed): 45.45454545454545%\n","Mean time of prediction for participants with only one crossing (smoothed): 9.444444444444443 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.902191457325726 minutes\n","Percentage of participants with only one crossing: 25.0%\n","Mean time of prediction for participants with only one crossing: 12.24 minutes\n","STD of time of prediction for participants with only one crossing: 7.012160865239758 minutes\n","Percentage of participants with only one crossing (smoothed): 40.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.5125 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.933069629720894 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n"]}],"source":["\n","convergence_starting_points = {8: [4]}\n","window_size4 = [3]\n","starting_window_size = {4:window_size4}\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":524},"executionInfo":{"elapsed":3561,"status":"error","timestamp":1693739952852,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"},"user_tz":-60},"id":"iL6zuFs47yvn","outputId":"b6a2c697-b16d-471d-c56a-a2336e41073a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda.\n","Convergence point:  8\n","Starting point:  6\n","Window size:  0.5\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-46-ecb9cb3484ee>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwindow_size6_8\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstarting_window_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mwindow_size6_8\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhyperparameter_tuning_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvergence_starting_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarting_window_size\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mresampling_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'undersampling'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_saving_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfolder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_participants\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselected_participants\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_participants_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselected_participants_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mifresetresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-42-cdb3c3a52791>\u001b[0m in \u001b[0;36mhyperparameter_tuning_loop\u001b[0;34m(mydata, convergence_starting_points_dict, starting_window_size_dict, resampling_method, results_saving_dir, selected_participants, selected_participants_data, ifresetresults)\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Starting point: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarting_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Window size: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 training_loop_RNN_regclass_schedule(mydata, convergence_point, total_epochs = 3, filling_limit = 40, method = 'LOCF', ifclassweights = 1, input_size = 86,\n\u001b[0m\u001b[1;32m     44\u001b[0m                                             \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                                             \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size_minutes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mifprobabilitiesanalysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-36-3f3f7e4ffd02>\u001b[0m in \u001b[0;36mtraining_loop_RNN_regclass_schedule\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Split the data into train, validation and test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     _, _, _, _, _, _,  train_subjects, test_subjects, _ = create_sliding_windows(data = mydata_imputed,\n\u001b[0m\u001b[1;32m     56\u001b[0m                                                                         \u001b[0mwindow_size_minutes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_size_minutes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                                                                         \u001b[0mifmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mifmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-88b0b0947e74>\u001b[0m in \u001b[0;36mcreate_sliding_windows\u001b[0;34m(data, window_size_minutes, ifmissing, random_seed, ifoutputsubjects, train_proportion, ifoutput_end_points, if_stratified_sampling, resampling_method, iftest, ifvocal)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;31m# If there are any NaNs in the window, skip it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mifmissing_column\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\n","convergence_starting_points = {8: [6]}\n","window_size6_8= [0.5, 1, 2, 3, 4, 5]\n","starting_window_size = {6:window_size6_8}\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12572932,"status":"ok","timestamp":1693756598520,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"},"user_tz":-60},"id":"DYod5XtVjv3P","outputId":"63f6a917-6115-40b0-d946-68858658ae5d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda.\n","Convergence point:  8\n","Starting point:  6\n","Window size:  3\n","New sleep onset threshold:  6.7\n","New awake threshold:  22.7\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 46.464646464646464%\n","Mean time of prediction for participants with only one crossing: 15.319565217391306 minutes\n","STD of time of prediction for participants with only one crossing: 8.875922966464742 minutes\n","Percentage of participants with only one crossing (smoothed): 45.45454545454545%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.377777777777784 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.997009105225256 minutes\n","Percentage of participants with only one crossing: 40.0%\n","Mean time of prediction for participants with only one crossing: 14.2625 minutes\n","STD of time of prediction for participants with only one crossing: 7.25257497375932 minutes\n","Percentage of participants with only one crossing (smoothed): 40.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.8875 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.703252475477855 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  7.4\n","New awake threshold:  15.4\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 53.535353535353536%\n","Mean time of prediction for participants with only one crossing: 13.264150943396226 minutes\n","STD of time of prediction for participants with only one crossing: 8.291598255921079 minutes\n","Percentage of participants with only one crossing (smoothed): 59.59595959595959%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.674576271186444 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.555612554129954 minutes\n","Percentage of participants with only one crossing: 25.0%\n","Mean time of prediction for participants with only one crossing: 11.639999999999999 minutes\n","STD of time of prediction for participants with only one crossing: 6.761242489365397 minutes\n","Percentage of participants with only one crossing (smoothed): 30.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.200000000000001 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.819090848492928 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  8\n","New awake threshold:  8\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 52.52525252525253%\n","Mean time of prediction for participants with only one crossing: 11.525000000000002 minutes\n","STD of time of prediction for participants with only one crossing: 7.3148130792669885 minutes\n","Percentage of participants with only one crossing (smoothed): 56.56565656565656%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.807142857142859 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.265816666881617 minutes\n","Percentage of participants with only one crossing: 30.0%\n","Mean time of prediction for participants with only one crossing: 16.983333333333334 minutes\n","STD of time of prediction for participants with only one crossing: 9.078439048402295 minutes\n","Percentage of participants with only one crossing (smoothed): 40.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.2625 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 9.142748150857049 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  8\n","Starting point:  6\n","Window size:  4\n","New sleep onset threshold:  6.7\n","New awake threshold:  22.7\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 46.464646464646464%\n","Mean time of prediction for participants with only one crossing: 15.278260869565221 minutes\n","STD of time of prediction for participants with only one crossing: 7.876379466300774 minutes\n","Percentage of participants with only one crossing (smoothed): 47.474747474747474%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.965957446808517 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.885383100172565 minutes\n","Percentage of participants with only one crossing: 30.0%\n","Mean time of prediction for participants with only one crossing: 19.766666666666666 minutes\n","STD of time of prediction for participants with only one crossing: 6.245442782986292 minutes\n","Percentage of participants with only one crossing (smoothed): 35.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 19.485714285714288 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.453940198968322 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  7.4\n","New awake threshold:  15.4\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 43.43434343434344%\n","Mean time of prediction for participants with only one crossing: 11.569767441860465 minutes\n","STD of time of prediction for participants with only one crossing: 6.670606641765989 minutes\n","Percentage of participants with only one crossing (smoothed): 44.44444444444444%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.91590909090909 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.084231631582734 minutes\n","Percentage of participants with only one crossing: 50.0%\n","Mean time of prediction for participants with only one crossing: 10.47 minutes\n","STD of time of prediction for participants with only one crossing: 5.603222287220095 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 9.290909090909091 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 4.837440071595822 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  8\n","New awake threshold:  8\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 48.484848484848484%\n","Mean time of prediction for participants with only one crossing: 10.295833333333333 minutes\n","STD of time of prediction for participants with only one crossing: 6.102866755786898 minutes\n","Percentage of participants with only one crossing (smoothed): 49.494949494949495%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.393877551020408 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.251380522251531 minutes\n","Percentage of participants with only one crossing: 50.0%\n","Mean time of prediction for participants with only one crossing: 10.16 minutes\n","STD of time of prediction for participants with only one crossing: 5.499309047507696 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.745454545454546 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.917825711950699 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  8\n","Starting point:  6\n","Window size:  5\n","New sleep onset threshold:  6.7\n","New awake threshold:  22.7\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 39.39393939393939%\n","Mean time of prediction for participants with only one crossing: 11.807692307692307 minutes\n","STD of time of prediction for participants with only one crossing: 6.184114662725435 minutes\n","Percentage of participants with only one crossing (smoothed): 41.41414141414141%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.395121951219512 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.189958478110952 minutes\n","Percentage of participants with only one crossing: 25.0%\n","Mean time of prediction for participants with only one crossing: 13.62 minutes\n","STD of time of prediction for participants with only one crossing: 5.3745325378120095 minutes\n","Percentage of participants with only one crossing (smoothed): 30.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.366666666666667 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 4.5338235029118135 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  7.4\n","New awake threshold:  15.4\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 47.474747474747474%\n","Mean time of prediction for participants with only one crossing: 13.278723404255322 minutes\n","STD of time of prediction for participants with only one crossing: 7.134630443253012 minutes\n","Percentage of participants with only one crossing (smoothed): 47.474747474747474%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.327659574468091 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.482225807408288 minutes\n","Percentage of participants with only one crossing: 25.0%\n","Mean time of prediction for participants with only one crossing: 8.459999999999999 minutes\n","STD of time of prediction for participants with only one crossing: 0.45431266766402195 minutes\n","Percentage of participants with only one crossing (smoothed): 40.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.3875 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.963913680539126 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  8\n","New awake threshold:  8\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 41.41414141414141%\n","Mean time of prediction for participants with only one crossing: 10.990243902439024 minutes\n","STD of time of prediction for participants with only one crossing: 5.981426735668517 minutes\n","Percentage of participants with only one crossing (smoothed): 45.45454545454545%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.455555555555554 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.800979057425683 minutes\n","Percentage of participants with only one crossing: 20.0%\n","Mean time of prediction for participants with only one crossing: 13.95 minutes\n","STD of time of prediction for participants with only one crossing: 6.128825336065631 minutes\n","Percentage of participants with only one crossing (smoothed): 30.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.950000000000001 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.244250593861773 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n"]}],"source":["\n","convergence_starting_points = {8: [6]}\n","window_size6_8= [3, 4, 5]\n","starting_window_size = {6:window_size6_8}\n","\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YAmt5rQQXeHW","outputId":"1fb104b3-96ec-40ca-bd7a-076a78f6c79f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda.\n","Convergence point:  12\n","Starting point:  2\n","Window size:  0.5\n","New sleep onset threshold:  5.3\n","New awake threshold:  24.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 64.64646464646465%\n","Mean time of prediction for participants with only one crossing: 13.628125 minutes\n","STD of time of prediction for participants with only one crossing: 6.930107970614528 minutes\n","Percentage of participants with only one crossing (smoothed): 64.64646464646465%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.121875 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.701571851536217 minutes\n","Percentage of participants with only one crossing: 45.0%\n","Mean time of prediction for participants with only one crossing: 17.877777777777776 minutes\n","STD of time of prediction for participants with only one crossing: 9.298599390084721 minutes\n","Percentage of participants with only one crossing (smoothed): 50.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 16.15 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 9.94346519076725 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  8.6\n","New awake threshold:  18.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 64.64646464646465%\n","Mean time of prediction for participants with only one crossing: 12.415625 minutes\n","STD of time of prediction for participants with only one crossing: 6.952275228971808 minutes\n","Percentage of participants with only one crossing (smoothed): 59.59595959595959%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.767796610169494 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.284449205658518 minutes\n","Percentage of participants with only one crossing: 50.0%\n","Mean time of prediction for participants with only one crossing: 10.47 minutes\n","STD of time of prediction for participants with only one crossing: 4.951575506846281 minutes\n","Percentage of participants with only one crossing (smoothed): 45.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.533333333333333 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.820912137766711 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  12\n","New awake threshold:  12\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 59.59595959595959%\n","Mean time of prediction for participants with only one crossing: 11.713559322033898 minutes\n","STD of time of prediction for participants with only one crossing: 6.830153921066782 minutes\n","Percentage of participants with only one crossing (smoothed): 62.62626262626263%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.917741935483877 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.181192375462636 minutes\n","Percentage of participants with only one crossing: 35.0%\n","Mean time of prediction for participants with only one crossing: 9.642857142857142 minutes\n","STD of time of prediction for participants with only one crossing: 4.396380700679381 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.790909090909091 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.899419373997116 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  12\n","Starting point:  2\n","Window size:  1\n","New sleep onset threshold:  5.3\n","New awake threshold:  24.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 69.6969696969697%\n","Mean time of prediction for participants with only one crossing: 11.544927536231883 minutes\n","STD of time of prediction for participants with only one crossing: 6.419931288446333 minutes\n","Percentage of participants with only one crossing (smoothed): 61.61616161616161%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.150819672131153 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.048780474589646 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 14.390909090909092 minutes\n","STD of time of prediction for participants with only one crossing: 9.331418536365543 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.609090909090908 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 9.445581414844868 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  8.6\n","New awake threshold:  18.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 64.64646464646465%\n","Mean time of prediction for participants with only one crossing: 12.454687499999999 minutes\n","STD of time of prediction for participants with only one crossing: 7.0732756221812645 minutes\n","Percentage of participants with only one crossing (smoothed): 63.63636363636363%\n","Mean time of prediction for participants with only one crossing (smoothed): 11.826984126984133 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.201723919482506 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 13.018181818181818 minutes\n","STD of time of prediction for participants with only one crossing: 7.5534240497957645 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.654545454545454 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.732006292044087 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  12\n","New awake threshold:  12\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 62.62626262626263%\n","Mean time of prediction for participants with only one crossing: 14.096774193548386 minutes\n","STD of time of prediction for participants with only one crossing: 6.835839324588613 minutes\n","Percentage of participants with only one crossing (smoothed): 62.62626262626263%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.224193548387102 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.070741291700037 minutes\n","Percentage of participants with only one crossing: 35.0%\n","Mean time of prediction for participants with only one crossing: 14.928571428571429 minutes\n","STD of time of prediction for participants with only one crossing: 8.539273017855171 minutes\n","Percentage of participants with only one crossing (smoothed): 35.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.12857142857143 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 9.00566714998542 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  12\n","Starting point:  2\n","Window size:  2\n","New sleep onset threshold:  5.3\n","New awake threshold:  24.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 71.71717171717171%\n","Mean time of prediction for participants with only one crossing: 15.032394366197183 minutes\n","STD of time of prediction for participants with only one crossing: 7.420295249580988 minutes\n","Percentage of participants with only one crossing (smoothed): 65.65656565656566%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.346153846153847 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.378098085614392 minutes\n","Percentage of participants with only one crossing: 45.0%\n","Mean time of prediction for participants with only one crossing: 14.877777777777775 minutes\n","STD of time of prediction for participants with only one crossing: 6.72454505322401 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.427272727272724 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.262492798818637 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  8.6\n","New awake threshold:  18.0\n","--------------------------------------Evaluate classification ----------------------------------------\n"]}],"source":["\n","convergence_starting_points = {12: [2]}\n","window_size2 = [0.5, 1, 1.5]\n","starting_window_size = {2: window_size4}\n","\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"piBTKR5tOntb","outputId":"e8c43202-d401-4e33-d3d9-a9f2186e49a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda.\n","Convergence point:  12\n","Starting point:  4\n","Window size:  0.5\n","New sleep onset threshold:  6.7\n","New awake threshold:  24.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 68.68686868686868%\n","Mean time of prediction for participants with only one crossing: 13.286764705882353 minutes\n","STD of time of prediction for participants with only one crossing: 7.4055206833615355 minutes\n","Percentage of participants with only one crossing (smoothed): 61.61616161616161%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.167213114754103 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.273277415609704 minutes\n","Percentage of participants with only one crossing: 45.0%\n","Mean time of prediction for participants with only one crossing: 15.166666666666666 minutes\n","STD of time of prediction for participants with only one crossing: 8.331333093275715 minutes\n","Percentage of participants with only one crossing (smoothed): 50.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.599999999999998 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.93812060782355 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  9.4\n","New awake threshold:  18.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 62.62626262626263%\n","Mean time of prediction for participants with only one crossing: 15.061290322580646 minutes\n","STD of time of prediction for participants with only one crossing: 7.148953337303204 minutes\n","Percentage of participants with only one crossing (smoothed): 63.63636363636363%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.160317460317467 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.83930956796538 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 14.872727272727273 minutes\n","STD of time of prediction for participants with only one crossing: 8.50530218682771 minutes\n","Percentage of participants with only one crossing (smoothed): 60.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.366666666666665 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.476274076634274 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  12\n","New awake threshold:  12\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 63.63636363636363%\n","Mean time of prediction for participants with only one crossing: 13.917460317460318 minutes\n","STD of time of prediction for participants with only one crossing: 7.093648840157857 minutes\n","Percentage of participants with only one crossing (smoothed): 55.55555555555556%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.509090909090915 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.636326276358102 minutes\n","Percentage of participants with only one crossing: 40.0%\n","Mean time of prediction for participants with only one crossing: 14.6125 minutes\n","STD of time of prediction for participants with only one crossing: 8.347070968309783 minutes\n","Percentage of participants with only one crossing (smoothed): 50.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.1 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 9.221713506718803 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  12\n","Starting point:  4\n","Window size:  1\n","New sleep onset threshold:  6.7\n","New awake threshold:  24.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 63.63636363636363%\n","Mean time of prediction for participants with only one crossing: 13.147619047619047 minutes\n","STD of time of prediction for participants with only one crossing: 6.4904534814262265 minutes\n","Percentage of participants with only one crossing (smoothed): 56.56565656565656%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.673214285714286 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.61466150390362 minutes\n","Percentage of participants with only one crossing: 35.0%\n","Mean time of prediction for participants with only one crossing: 12.585714285714285 minutes\n","STD of time of prediction for participants with only one crossing: 7.123889100650525 minutes\n","Percentage of participants with only one crossing (smoothed): 60.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 16.158333333333328 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 9.777649541457064 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  9.4\n","New awake threshold:  18.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 68.68686868686868%\n","Mean time of prediction for participants with only one crossing: 12.691176470588237 minutes\n","STD of time of prediction for participants with only one crossing: 6.878990505926203 minutes\n","Percentage of participants with only one crossing (smoothed): 67.67676767676768%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.274626865671644 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.180640292482593 minutes\n","Percentage of participants with only one crossing: 45.0%\n","Mean time of prediction for participants with only one crossing: 14.68888888888889 minutes\n","STD of time of prediction for participants with only one crossing: 7.9889429761729565 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.699999999999996 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.787904911141945 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  12\n","New awake threshold:  12\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 64.64646464646465%\n","Mean time of prediction for participants with only one crossing: 13.6859375 minutes\n","STD of time of prediction for participants with only one crossing: 6.553025713065207 minutes\n","Percentage of participants with only one crossing (smoothed): 67.67676767676768%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.207462686567165 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.888500333572077 minutes\n","Percentage of participants with only one crossing: 45.0%\n","Mean time of prediction for participants with only one crossing: 15.266666666666667 minutes\n","STD of time of prediction for participants with only one crossing: 6.140937315354319 minutes\n","Percentage of participants with only one crossing (smoothed): 50.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 17.099999999999998 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.101851640211638 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  12\n","Starting point:  4\n","Window size:  2\n","New sleep onset threshold:  6.7\n","New awake threshold:  24.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 65.65656565656566%\n","Mean time of prediction for participants with only one crossing: 14.875384615384615 minutes\n","STD of time of prediction for participants with only one crossing: 6.132336253744904 minutes\n","Percentage of participants with only one crossing (smoothed): 63.63636363636363%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.461904761904767 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.595968671930487 minutes\n","Percentage of participants with only one crossing: 45.0%\n","Mean time of prediction for participants with only one crossing: 15.811111111111112 minutes\n","STD of time of prediction for participants with only one crossing: 6.919769495911204 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.336363636363632 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.9164290100875006 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  9.4\n","New awake threshold:  18.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 70.70707070707071%\n","Mean time of prediction for participants with only one crossing: 15.78857142857143 minutes\n","STD of time of prediction for participants with only one crossing: 6.460883240308398 minutes\n","Percentage of participants with only one crossing (smoothed): 67.67676767676768%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.050746268656718 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.1529218147453015 minutes\n","Percentage of participants with only one crossing: 60.0%\n","Mean time of prediction for participants with only one crossing: 15.358333333333334 minutes\n","STD of time of prediction for participants with only one crossing: 8.029576403660545 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.927272727272724 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.955585186686754 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  12\n","New awake threshold:  12\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 63.63636363636363%\n","Mean time of prediction for participants with only one crossing: 14.7015873015873 minutes\n","STD of time of prediction for participants with only one crossing: 6.716586719682353 minutes\n","Percentage of participants with only one crossing (smoothed): 67.67676767676768%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.774626865671642 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.259709777587573 minutes\n","Percentage of participants with only one crossing: 40.0%\n","Mean time of prediction for participants with only one crossing: 15.25 minutes\n","STD of time of prediction for participants with only one crossing: 6.971549325652082 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.654545454545454 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.331298240032082 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  12\n","Starting point:  4\n","Window size:  3\n","New sleep onset threshold:  6.7\n","New awake threshold:  24.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 60.60606060606061%\n","Mean time of prediction for participants with only one crossing: 15.606666666666664 minutes\n","STD of time of prediction for participants with only one crossing: 5.750416410042745 minutes\n","Percentage of participants with only one crossing (smoothed): 55.55555555555556%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.536363636363642 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.457317477441347 minutes\n","Percentage of participants with only one crossing: 40.0%\n","Mean time of prediction for participants with only one crossing: 18.875 minutes\n","STD of time of prediction for participants with only one crossing: 6.383915334651611 minutes\n","Percentage of participants with only one crossing (smoothed): 30.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 16.45 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.677073776750366 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  9.4\n","New awake threshold:  18.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 58.58585858585859%\n","Mean time of prediction for participants with only one crossing: 14.24310344827586 minutes\n","STD of time of prediction for participants with only one crossing: 6.185525709237921 minutes\n","Percentage of participants with only one crossing (smoothed): 63.63636363636363%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.731746031746038 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.028215818135765 minutes\n","Percentage of participants with only one crossing: 60.0%\n","Mean time of prediction for participants with only one crossing: 16.566666666666666 minutes\n","STD of time of prediction for participants with only one crossing: 5.854532906693373 minutes\n","Percentage of participants with only one crossing (smoothed): 65.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.353846153846149 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.3105059390749805 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  12\n","New awake threshold:  12\n"]}],"source":["\n","convergence_starting_points = {12: [4]}\n","window_size4 = [0.5, 1, 2, 3]\n","starting_window_size = {4: window_size4}\n","\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2680502,"status":"ok","timestamp":1693825257999,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"},"user_tz":-60},"id":"Ycf3trvxsKmJ","outputId":"b57fc317-f44f-49cf-fe51-710e6869e093"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda.\n","Convergence point:  12\n","Starting point:  4\n","Window size:  3\n","New sleep onset threshold:  6.7\n","New awake threshold:  24.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 55.55555555555556%\n","Mean time of prediction for participants with only one crossing: 15.36181818181818 minutes\n","STD of time of prediction for participants with only one crossing: 6.098181276088229 minutes\n","Percentage of participants with only one crossing (smoothed): 55.55555555555556%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.009090909090915 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.203731215303863 minutes\n","Percentage of participants with only one crossing: 35.0%\n","Mean time of prediction for participants with only one crossing: 15.814285714285715 minutes\n","STD of time of prediction for participants with only one crossing: 5.344270248574254 minutes\n","Percentage of participants with only one crossing (smoothed): 35.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 18.557142857142857 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.859755514855338 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  9.4\n","New awake threshold:  18.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 66.66666666666666%\n","Mean time of prediction for participants with only one crossing: 14.083333333333334 minutes\n","STD of time of prediction for participants with only one crossing: 5.1918812884282435 minutes\n","Percentage of participants with only one crossing (smoothed): 68.68686868686868%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.42794117647059 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.870811287463631 minutes\n","Percentage of participants with only one crossing: 50.0%\n","Mean time of prediction for participants with only one crossing: 16.0 minutes\n","STD of time of prediction for participants with only one crossing: 5.462050896870148 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.699999999999996 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.074281203447779 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  12\n","New awake threshold:  12\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 63.63636363636363%\n","Mean time of prediction for participants with only one crossing: 13.084126984126984 minutes\n","STD of time of prediction for participants with only one crossing: 5.478593445639602 minutes\n","Percentage of participants with only one crossing (smoothed): 64.64646464646465%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.020312500000001 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.430215801636593 minutes\n","Percentage of participants with only one crossing: 50.0%\n","Mean time of prediction for participants with only one crossing: 14.11 minutes\n","STD of time of prediction for participants with only one crossing: 5.575383394888642 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.654545454545454 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.495678241931834 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n"]}],"source":["\n","convergence_starting_points = {12: [4]}\n","window_size4 = [3]\n","starting_window_size = {4: window_size4}\n","\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EWTuIOJq2SbV","outputId":"8432a82f-7853-48f1-c53f-ee715a292add"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Using cuda.\n","Convergence point:  12\n","Starting point:  6\n","Window size:  0.5\n","New sleep onset threshold:  8.0\n","New awake threshold:  24.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 66.66666666666666%\n","Mean time of prediction for participants with only one crossing: 14.434848484848487 minutes\n","STD of time of prediction for participants with only one crossing: 7.21354980121569 minutes\n","Percentage of participants with only one crossing (smoothed): 58.58585858585859%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.65689655172414 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.926513579616284 minutes\n","Percentage of participants with only one crossing: 50.0%\n","Mean time of prediction for participants with only one crossing: 13.580000000000002 minutes\n","STD of time of prediction for participants with only one crossing: 8.067440734210571 minutes\n","Percentage of participants with only one crossing (smoothed): 50.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.7 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.842193570679061 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10.0\n","New awake threshold:  18.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 64.64646464646465%\n","Mean time of prediction for participants with only one crossing: 13.765625 minutes\n","STD of time of prediction for participants with only one crossing: 7.607977284362447 minutes\n","Percentage of participants with only one crossing (smoothed): 66.66666666666666%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.351515151515152 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.7469298400553575 minutes\n","Percentage of participants with only one crossing: 65.0%\n","Mean time of prediction for participants with only one crossing: 15.515384615384619 minutes\n","STD of time of prediction for participants with only one crossing: 9.21894755830839 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.2 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.79860127318879 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  12\n","New awake threshold:  12\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 63.63636363636363%\n","Mean time of prediction for participants with only one crossing: 13.566666666666666 minutes\n","STD of time of prediction for participants with only one crossing: 7.291765305455276 minutes\n","Percentage of participants with only one crossing (smoothed): 63.63636363636363%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.596825396825402 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.710104033514008 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 16.863636363636363 minutes\n","STD of time of prediction for participants with only one crossing: 7.956503655075083 minutes\n","Percentage of participants with only one crossing (smoothed): 40.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.8875 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.981840283907961 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  12\n","Starting point:  6\n","Window size:  1\n","New sleep onset threshold:  8.0\n","New awake threshold:  24.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 67.67676767676768%\n","Mean time of prediction for participants with only one crossing: 14.701492537313431 minutes\n","STD of time of prediction for participants with only one crossing: 6.7197524060933995 minutes\n","Percentage of participants with only one crossing (smoothed): 64.64646464646465%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.684375 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.125531093144917 minutes\n","Percentage of participants with only one crossing: 70.0%\n","Mean time of prediction for participants with only one crossing: 19.3 minutes\n","STD of time of prediction for participants with only one crossing: 7.51455730090412 minutes\n","Percentage of participants with only one crossing (smoothed): 65.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 17.930769230769226 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.420705672547204 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10.0\n","New awake threshold:  18.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 68.68686868686868%\n","Mean time of prediction for participants with only one crossing: 14.133823529411766 minutes\n","STD of time of prediction for participants with only one crossing: 6.884842906792505 minutes\n","Percentage of participants with only one crossing (smoothed): 66.66666666666666%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.063636363636364 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.525484709318918 minutes\n","Percentage of participants with only one crossing: 60.0%\n","Mean time of prediction for participants with only one crossing: 14.316666666666665 minutes\n","STD of time of prediction for participants with only one crossing: 7.72947101826653 minutes\n","Percentage of participants with only one crossing (smoothed): 65.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.276923076923074 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.39378468283379 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  12\n","New awake threshold:  12\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 72.72727272727273%\n","Mean time of prediction for participants with only one crossing: 13.784722222222221 minutes\n","STD of time of prediction for participants with only one crossing: 6.967616755833578 minutes\n","Percentage of participants with only one crossing (smoothed): 71.71717171717171%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.178873239436623 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.559359751802476 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 14.736363636363636 minutes\n","STD of time of prediction for participants with only one crossing: 9.936733753399576 minutes\n","Percentage of participants with only one crossing (smoothed): 65.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.16153846153846 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 9.359202666691912 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  12\n","Starting point:  6\n","Window size:  2\n","New sleep onset threshold:  8.0\n","New awake threshold:  24.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 67.67676767676768%\n","Mean time of prediction for participants with only one crossing: 14.902985074626864 minutes\n","STD of time of prediction for participants with only one crossing: 6.377267700836733 minutes\n","Percentage of participants with only one crossing (smoothed): 62.62626262626263%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.498387096774199 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.728885541034981 minutes\n","Percentage of participants with only one crossing: 35.0%\n","Mean time of prediction for participants with only one crossing: 17.085714285714285 minutes\n","STD of time of prediction for participants with only one crossing: 6.710637306636717 minutes\n","Percentage of participants with only one crossing (smoothed): 40.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 19.2625 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.492483634942794 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10.0\n","New awake threshold:  18.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 66.66666666666666%\n","Mean time of prediction for participants with only one crossing: 15.607575757575756 minutes\n","STD of time of prediction for participants with only one crossing: 6.714193708357818 minutes\n","Percentage of participants with only one crossing (smoothed): 59.59595959595959%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.80169491525424 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.757782494717208 minutes\n","Percentage of participants with only one crossing: 60.0%\n","Mean time of prediction for participants with only one crossing: 17.058333333333334 minutes\n","STD of time of prediction for participants with only one crossing: 7.258610786889978 minutes\n","Percentage of participants with only one crossing (smoothed): 60.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 16.699999999999996 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.530493565054905 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  12\n","New awake threshold:  12\n","--------------------------------------Evaluate classification ----------------------------------------\n"]}],"source":["\n","convergence_starting_points = {12: [6]}\n","window_size6_8= [0.5, 1, 2, 3, 4, 5]\n","starting_window_size = {6:window_size6_8}\n","\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y26dkQ5cdJwx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1ca662b1-67ae-4f98-e9a4-dabfcd803327"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda.\n","Convergence point:  12\n","Starting point:  6\n","Window size:  2\n","New sleep onset threshold:  8.0\n","New awake threshold:  24.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 65.65656565656566%\n","Mean time of prediction for participants with only one crossing: 14.738461538461538 minutes\n","STD of time of prediction for participants with only one crossing: 6.318000957159991 minutes\n","Percentage of participants with only one crossing (smoothed): 62.62626262626263%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.224193548387102 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.999670164668613 minutes\n","Percentage of participants with only one crossing: 30.0%\n","Mean time of prediction for participants with only one crossing: 14.166666666666666 minutes\n","STD of time of prediction for participants with only one crossing: 4.646025780767424 minutes\n","Percentage of participants with only one crossing (smoothed): 35.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.485714285714286 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.289771417928947 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10.0\n","New awake threshold:  18.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 62.62626262626263%\n","Mean time of prediction for participants with only one crossing: 15.191935483870967 minutes\n","STD of time of prediction for participants with only one crossing: 6.237769927887451 minutes\n","Percentage of participants with only one crossing (smoothed): 57.57575757575758%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.094736842105265 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.215245010840925 minutes\n","Percentage of participants with only one crossing: 60.0%\n","Mean time of prediction for participants with only one crossing: 15.808333333333335 minutes\n","STD of time of prediction for participants with only one crossing: 7.577099085768612 minutes\n","Percentage of participants with only one crossing (smoothed): 60.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.658333333333331 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 7.534527007199737 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  12\n","New awake threshold:  12\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 65.65656565656566%\n","Mean time of prediction for participants with only one crossing: 12.973846153846154 minutes\n","STD of time of prediction for participants with only one crossing: 6.1301092203364895 minutes\n","Percentage of participants with only one crossing (smoothed): 65.65656565656566%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.223076923076922 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.392562246158102 minutes\n","Percentage of participants with only one crossing: 55.00000000000001%\n","Mean time of prediction for participants with only one crossing: 14.372727272727273 minutes\n","STD of time of prediction for participants with only one crossing: 6.269854414590924 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.518181818181814 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.43948306533687 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  12\n","Starting point:  6\n","Window size:  3\n","New sleep onset threshold:  8.0\n","New awake threshold:  24.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 59.59595959595959%\n","Mean time of prediction for participants with only one crossing: 14.162711864406779 minutes\n","STD of time of prediction for participants with only one crossing: 5.7240171476689685 minutes\n","Percentage of participants with only one crossing (smoothed): 61.61616161616161%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.667213114754103 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.771365005414292 minutes\n","Percentage of participants with only one crossing: 25.0%\n","Mean time of prediction for participants with only one crossing: 9.440000000000001 minutes\n","STD of time of prediction for participants with only one crossing: 1.8194504664870659 minutes\n","Percentage of participants with only one crossing (smoothed): 25.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 10.9 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 4.319722213291035 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10.0\n","New awake threshold:  18.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 70.70707070707071%\n","Mean time of prediction for participants with only one crossing: 15.512857142857142 minutes\n","STD of time of prediction for participants with only one crossing: 5.980561880639918 minutes\n","Percentage of participants with only one crossing (smoothed): 69.6969696969697%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.00434782608696 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.29827293517624 minutes\n","Percentage of participants with only one crossing: 35.0%\n","Mean time of prediction for participants with only one crossing: 14.357142857142858 minutes\n","STD of time of prediction for participants with only one crossing: 5.777260137286319 minutes\n","Percentage of participants with only one crossing (smoothed): 40.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.575 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.435982830928 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  12\n","New awake threshold:  12\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 71.71717171717171%\n","Mean time of prediction for participants with only one crossing: 13.470422535211267 minutes\n","STD of time of prediction for participants with only one crossing: 5.118631924568368 minutes\n","Percentage of participants with only one crossing (smoothed): 70.70707070707071%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.128571428571432 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.147174562515758 minutes\n","Percentage of participants with only one crossing: 60.0%\n","Mean time of prediction for participants with only one crossing: 13.858333333333334 minutes\n","STD of time of prediction for participants with only one crossing: 5.805522992997463 minutes\n","Percentage of participants with only one crossing (smoothed): 55.00000000000001%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.154545454545454 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.634530493687786 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  12\n","Starting point:  6\n","Window size:  4\n","New sleep onset threshold:  8.0\n","New awake threshold:  24.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 59.59595959595959%\n","Mean time of prediction for participants with only one crossing: 14.683050847457626 minutes\n","STD of time of prediction for participants with only one crossing: 5.596220048204235 minutes\n","Percentage of participants with only one crossing (smoothed): 56.56565656565656%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.833928571428572 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.104699402991126 minutes\n","Percentage of participants with only one crossing: 40.0%\n","Mean time of prediction for participants with only one crossing: 14.4875 minutes\n","STD of time of prediction for participants with only one crossing: 5.73442183223383 minutes\n","Percentage of participants with only one crossing (smoothed): 45.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 12.81111111111111 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.430253614794401 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10.0\n","New awake threshold:  18.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 61.61616161616161%\n","Mean time of prediction for participants with only one crossing: 15.563934426229507 minutes\n","STD of time of prediction for participants with only one crossing: 5.859027860584886 minutes\n","Percentage of participants with only one crossing (smoothed): 58.58585858585859%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.05344827586207 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.999287759945755 minutes\n","Percentage of participants with only one crossing: 45.0%\n","Mean time of prediction for participants with only one crossing: 17.977777777777774 minutes\n","STD of time of prediction for participants with only one crossing: 6.491152573187219 minutes\n","Percentage of participants with only one crossing (smoothed): 35.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 16.12857142857143 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.971516392889464 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  12\n","New awake threshold:  12\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 57.57575757575758%\n","Mean time of prediction for participants with only one crossing: 14.863157894736842 minutes\n","STD of time of prediction for participants with only one crossing: 5.260766123212419 minutes\n","Percentage of participants with only one crossing (smoothed): 60.60606060606061%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.200000000000005 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.278933030380241 minutes\n","Percentage of participants with only one crossing: 40.0%\n","Mean time of prediction for participants with only one crossing: 15.7375 minutes\n","STD of time of prediction for participants with only one crossing: 5.5787403372087505 minutes\n","Percentage of participants with only one crossing (smoothed): 45.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.477777777777778 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.178417233092374 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","Convergence point:  12\n","Starting point:  6\n","Window size:  5\n","New sleep onset threshold:  8.0\n","New awake threshold:  24.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 60.60606060606061%\n","Mean time of prediction for participants with only one crossing: 15.908333333333335 minutes\n","STD of time of prediction for participants with only one crossing: 5.109706177680104 minutes\n","Percentage of participants with only one crossing (smoothed): 61.61616161616161%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.159016393442627 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.273887950505334 minutes\n","Percentage of participants with only one crossing: 35.0%\n","Mean time of prediction for participants with only one crossing: 16.942857142857143 minutes\n","STD of time of prediction for participants with only one crossing: 5.752231155413187 minutes\n","Percentage of participants with only one crossing (smoothed): 35.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 17.557142857142857 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.104531605491856 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10.0\n","New awake threshold:  18.0\n"]}],"source":["\n","convergence_starting_points = {12: [6]}\n","window_size6_8= [2, 3, 4, 5]\n","starting_window_size = {6:window_size6_8}\n","\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"]},{"cell_type":"code","source":["\n","convergence_starting_points = {12: [6]}\n","window_size6_8= [5]\n","starting_window_size = {6:window_size6_8}\n","\n","hyperparameter_tuning_loop(data_train, convergence_starting_points, starting_window_size , resampling_method = 'undersampling', results_saving_dir = folder_name, selected_participants = selected_participants, selected_participants_data = selected_participants_data, ifresetresults = False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jz_A5lVaOKuA","executionInfo":{"status":"ok","timestamp":1693901192593,"user_tz":-60,"elapsed":2587369,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}},"outputId":"e5822c6c-6066-49ca-eb97-9cfb681d511f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda.\n","Convergence point:  12\n","Starting point:  6\n","Window size:  5\n","New sleep onset threshold:  8.0\n","New awake threshold:  24.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 60.60606060606061%\n","Mean time of prediction for participants with only one crossing: 15.303333333333331 minutes\n","STD of time of prediction for participants with only one crossing: 5.90939835253039 minutes\n","Percentage of participants with only one crossing (smoothed): 61.61616161616161%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.536065573770497 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.850403735175252 minutes\n","Percentage of participants with only one crossing: 40.0%\n","Mean time of prediction for participants with only one crossing: 15.1375 minutes\n","STD of time of prediction for participants with only one crossing: 6.4909432095805615 minutes\n","Percentage of participants with only one crossing (smoothed): 45.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 14.255555555555553 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 6.193385559720792 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  10.0\n","New awake threshold:  18.0\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 63.63636363636363%\n","Mean time of prediction for participants with only one crossing: 16.79047619047619 minutes\n","STD of time of prediction for participants with only one crossing: 5.825360927184923 minutes\n","Percentage of participants with only one crossing (smoothed): 59.59595959595959%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.776271186440681 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.781922118472804 minutes\n","Percentage of participants with only one crossing: 50.0%\n","Mean time of prediction for participants with only one crossing: 17.18 minutes\n","STD of time of prediction for participants with only one crossing: 6.0047980815344655 minutes\n","Percentage of participants with only one crossing (smoothed): 45.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.92222222222222 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.97267440148656 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n","New sleep onset threshold:  12\n","New awake threshold:  12\n","--------------------------------------Evaluate classification ----------------------------------------\n","Percentage of participants with only one crossing: 65.65656565656566%\n","Mean time of prediction for participants with only one crossing: 16.243076923076924 minutes\n","STD of time of prediction for participants with only one crossing: 5.409335782650903 minutes\n","Percentage of participants with only one crossing (smoothed): 62.62626262626263%\n","Mean time of prediction for participants with only one crossing (smoothed): 15.022580645161295 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.4536882860056926 minutes\n","Percentage of participants with only one crossing: 60.0%\n","Mean time of prediction for participants with only one crossing: 13.666666666666666 minutes\n","STD of time of prediction for participants with only one crossing: 4.9676173586763115 minutes\n","Percentage of participants with only one crossing (smoothed): 70.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 13.557142857142852 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 5.689679163150751 minutes\n","--------------------------------------Evaluate regression ----------------------------------------\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n"]}]},{"cell_type":"markdown","source":["## Analysing Hyperparameter Tuning results"],"metadata":{"id":"iIWgUh2omdID"}},{"cell_type":"code","source":["output_path = folder_name\n","results_df_all = pd.read_csv(f'{output_path}/results.csv')\n","results_df_all = results_df_all.drop(columns=['Unnamed: 0'])\n","# only keep results where ifconverged = 1\n","results_df_all_selected = results_df_all[results_df_all['ifconverged'] == 1]\n","#results_df_all_selected = results_df_all[results_df_all['ifconverged'] == 1]\n","# show how many results we have\n","\n","\n","\n","#Only keep the colums:\n","#Only keep the colums:\n","columns = ['convergence_point', 'initial_sleep_onset_threshold', 'sliding_window_size', 'f1_weighted', 'f1_pre_sleep',\n","       'f1_awake', 'precision',\n","       'recall', 'auc', 'one_crossing_percentage',\n","       'mean_time_of_prediction',\n","       'std_time_of_prediction']\n","results_df_all_selected = results_df_all_selected[columns]\n","\n","\n","\n","sorted_results = results_df_all_selected.sort_values(by=['f1_weighted'], ascending=False)\n","# make the values in the table only have 2 decimal places after saving\n","sorted_results = sorted_results.round(2)\n","\n","headers = ['t_{PS}', 't_0_{PS}', 't_{SW}', 'F1_W', 'F1_{PS}','F1_{AW}', 'P', 'S', 'AUC', 'R_ps', 'Mean(t_{pred})', 'STD(t_{pred})']\n","sorted_results.columns = headers\n","# Save only the top 3 results\n","sorted_results = sorted_results.head(5)\n","\n","sorted_results.to_latex(f'{output_path}/results_top5_f1.tex', index=False, float_format=\"%.2f\")\n","\n","print(sorted_results.head(5))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bRc_O0_JmcnA","executionInfo":{"status":"ok","timestamp":1693904967553,"user_tz":-60,"elapsed":457,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}},"outputId":"db69ed32-6cac-4c78-9996-7f67257fde3d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["    t_{PS}  t_0_{PS}  t_{SW}  F1_W  F1_{PS}  F1_{AW}     P     S   AUC   R_ps  \\\n","8        4         2     1.5  0.85     0.90     0.54  0.46  0.66  0.84  13.13   \n","2        4         2     0.5  0.83     0.89     0.50  0.40  0.66  0.83  39.39   \n","5        4         2     1.0  0.83     0.88     0.51  0.40  0.69  0.83  42.42   \n","30       6         4     3.0  0.81     0.87     0.62  0.56  0.69  0.84  28.28   \n","17       6         2     1.5  0.79     0.85     0.57  0.49  0.68  0.81  40.40   \n","\n","    Mean(t_{pred})  STD(t_{pred})  \n","8            11.42           4.80  \n","2             7.44           5.95  \n","5             6.77           6.43  \n","30           13.28           8.81  \n","17            9.30           5.62  \n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-47-7635dfc7118c>:31: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n","  sorted_results.to_latex(f'{output_path}/results_top5_f1.tex', index=False, float_format=\"%.2f\")\n"]}]},{"cell_type":"code","source":["output_path = folder_name\n","results_df_all = pd.read_csv(f'{output_path}/results.csv')\n","results_df_all = results_df_all.drop(columns=['Unnamed: 0'])\n","# only keep results where ifconverged = 1\n","results_df_all_selected = results_df_all[results_df_all['ifconverged'] == 1]\n","#results_df_all_selected = results_df_all[results_df_all['ifconverged'] == 1]\n","# show how many results we have\n","\n","\n","#Only keep the colums:\n","#Only keep the colums:\n","columns = ['convergence_point', 'initial_sleep_onset_threshold', 'sliding_window_size','one_crossing_percentage',\n","       'mean_time_of_prediction',\n","       'std_time_of_prediction', 'f1_weighted', 'f1_pre_sleep',\n","       'f1_awake', 'precision',\n","       'recall', 'auc']\n","results_df_all_selected = results_df_all_selected[columns]\n","\n","\n","\n","sorted_results = results_df_all_selected.sort_values(by=['one_crossing_percentage'], ascending=False)\n","# make the values in the table only have 2 decimal places after saving\n","sorted_results = sorted_results.round(2)\n","\n","headers = ['t_{PS}', 't_0_{PS}', 't_{SW}', 'R_ps', 'Mean(t_{pred})', 'STD(t_{pred})', 'F1_W', 'F1_{PS}','F1_{AW}', 'P', 'S', 'AUC']\n","sorted_results.columns = headers\n","# Save only the top 3 results\n","sorted_results = sorted_results.head(5)\n","\n","sorted_results.to_latex(f'{output_path}/results_top5_crossing.tex', index=False, float_format=\"%.2f\")\n","\n","print(sorted_results.head(5))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MYaJgZumnm_K","executionInfo":{"status":"ok","timestamp":1693914216810,"user_tz":-60,"elapsed":466,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}},"outputId":"2a3cab75-5276-4a0c-aab6-0f35454b6f25"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["     t_{PS}  t_0_{PS}  t_{SW}   R_ps  Mean(t_{pred})  STD(t_{pred})  F1_W  \\\n","94       12         6     1.0  72.73           13.78           6.97  0.66   \n","36        8         2     1.0  71.72           10.39           6.96  0.74   \n","102      12         6     3.0  71.72           13.47           5.12  0.69   \n","53        8         6     1.0  66.67           12.71           8.10  0.70   \n","99       12         6     2.0  65.66           12.97           6.13  0.69   \n","\n","     F1_{PS}  F1_{AW}     P     S   AUC  \n","94      0.68     0.63  0.57  0.70  0.72  \n","36      0.80     0.58  0.51  0.69  0.79  \n","102     0.69     0.69  0.63  0.75  0.76  \n","53      0.75     0.56  0.46  0.73  0.77  \n","99      0.73     0.64  0.64  0.65  0.73  \n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-50-5c2e53519f2a>:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n","  sorted_results.to_latex(f'{output_path}/results_top5_crossing.tex', index=False, float_format=\"%.2f\")\n"]}]},{"cell_type":"code","source":["convergence_points = [4,12]\n","initial_sleep_onset_thresholds = [2, 6]\n","sliding_window_sizes = [1.5, 3]"],"metadata":{"id":"JpBEfJ-bDn2H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test the best models on the test set"],"metadata":{"id":"J60WxAvyC3_7"}},{"cell_type":"markdown","source":["### Helper functions"],"metadata":{"id":"6MMTa3yVDR1x"}},{"cell_type":"code","source":["def all_test_classification_model(model, label,  test_labels, test_end_points,\n","                                test_sleep_stages,\n","                                test_loader, device, criterion,\n","                                index, ifprobabilities,\n","                                ifplot, ifsaveplots, save_directory_epoch,\n","                                awake_threshold, sleep_onset_threshold, window_size_minutes):\n","    if ifprobabilities:\n","        test_predictions, test_actuals, test_prediction_probabilities = test_classification(model, test_loader, device, criterion, index, threshold=0.5, ifprobabilities=ifprobabilities, ifplot = ifplot)\n","\n","        test_metrics = evaluate_classification(true_labels=test_actuals, predictions=test_predictions,\n","                                               prediction_probabilities=test_prediction_probabilities,\n","                                               index=index, columns = label, ifsaveplots=ifsaveplots,\n","                                               output_path=save_directory_epoch, ifplot=ifplot)\n","\n","        if ifplot:\n","                print('------------------------------------------- No averaging ----------------------------------------')\n","                print('Accuracy')\n","        accuracies, precision, recall, f1 = check_accuracy_timeline(test_predictions, test_labels, test_end_points,\n","                                                                    test_sleep_stages, window_for_averaging= 1, awake_window=awake_threshold,\n","                                                                    pre_sleep_window=sleep_onset_threshold, rnn_window = window_size_minutes,\n","                                                                    ifsaveplots = ifsaveplots, output_path= save_directory_epoch, ifplot = ifplot)\n","        if ifplot:\n","            print('Probability of sleep as predicted by the model')\n","        y_pred_sorted = check_accuracy_timeline(test_prediction_probabilities, test_labels, test_end_points,\n","                                                test_sleep_stages, window_for_averaging= 1, if_proba = True,\n","                                                classes = index, awake_window=awake_threshold, pre_sleep_window=sleep_onset_threshold,\n","                                                rnn_window = window_size_minutes, ifsaveplots = ifsaveplots, output_path= save_directory_epoch,\n","                                                ifplot = ifplot)\n","        if ifplot:\n","            print('-------------------------------Averaging of accuracy over 30 seconds---------------------------------')\n","            print('Accuracy')\n","        accuracies, precision, recall, f1 = check_accuracy_timeline(test_predictions, test_labels, test_end_points,\n","                                                                    test_sleep_stages, window_for_averaging= 5,\n","                                                                    awake_window=awake_threshold, pre_sleep_window=sleep_onset_threshold,\n","                                                                    rnn_window = window_size_minutes, ifsaveplots = ifsaveplots,\n","                                                                    output_path= save_directory_epoch, ifplot = ifplot)\n","        if ifplot:\n","            print('Probability of sleep as predicted by the model')\n","        y_pred_sorted = check_accuracy_timeline(test_prediction_probabilities, test_labels, test_end_points,\n","                                                test_sleep_stages, window_for_averaging= 5, if_proba = True,\n","                                                classes = index,  awake_window=awake_threshold,\n","                                                pre_sleep_window=sleep_onset_threshold, rnn_window = window_size_minutes,\n","                                                ifsaveplots = ifsaveplots, output_path= save_directory_epoch, ifplot = ifplot)\n","\n","        return  test_predictions, test_actuals, test_prediction_probabilities, test_metrics\n","\n","    else:\n","        test_predictions, test_actuals = test_classification(model, test_loader, device, criterion, index, threshold=0.5, ifprobabilities=ifprobabilities, ifplot = ifplot)\n","\n","\n","        test_metrics = evaluate_classification(true_labels=test_actuals, predictions=test_predictions,\n","                                               prediction_probabilities=None,\n","                                               index=index, columns = label, ifsaveplots=ifsaveplots,\n","                                               output_path=save_directory_epoch, ifplot=ifplot)\n","\n","        return  test_predictions, test_actuals, None, test_metrics\n"],"metadata":{"id":"lWaBfE1rDRj-","executionInfo":{"status":"ok","timestamp":1693956097623,"user_tz":-60,"elapsed":3,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["from sklearn.utils.class_weight import compute_class_weight\n","import warnings\n","def test_loop_RNN_regclass_schedule(test_data, models_to_test, convergence_points, initial_sleep_onset_thresholds, sliding_window_sizes,\n","                                    filling_limit, method, batch_size, results_saving_dir = None, ifresetresults = True, random_seed = 42):\n","\n","    ################## INITIALISATION OF RESULTS STORAGE ##################\n","    warnings.filterwarnings('ignore', category=RuntimeWarning)\n","    warnings.filterwarnings('ignore', category=UserWarning)\n","\n","    #if results_saving_dir is None:\n","     #   results_saving_dir = output_path\n","\n","    save_directory = results_saving_dir\n","\n","    if not os.path.exists(save_directory):\n","        os.makedirs(save_directory)\n","\n","    # Initialize a DataFrame to store results\n","    columns = ['convergence_point', 'initial_sleep_onset_threshold',\n","                'sliding_window_size', 'accuracy', 'precision', 'recall', 'f1_weighted',\n","                'f1_macro', 'f1_pre_sleep', 'f1_awake', 'auc',\n","                'one_crossing_percentage', 'mean_time_of_prediction',\n","                'std_time_of_prediction', 'one_crossing_percentage_smoothed',\n","                'mean_time_of_prediction_smoothed', 'std_time_of_prediction_smoothed', 'Accuracy'\n","                'MAE', 'MSE', 'RMSE', 'R2', 'custom_MSE']\n","\n","    if ifresetresults:\n","        results_df = pd.DataFrame(columns=columns)\n","        results_df.to_csv(os.path.join(results_saving_dir, 'results_final.csv'))\n","\n","    for model, convergence_point, initial_sleep_onset_threshold, sliding_window_size in zip(models_to_test, convergence_points, initial_sleep_onset_thresholds, sliding_window_sizes):\n","\n","      ################## PREPARE DATA FOR TRAINING AND TESTING ##################\n","      # Firstly, get the datasets for a classification and regressions problem\n","      # Create a new column 'old_label' to store the original label\n","      test_data_bothlabels = test_data.copy()\n","\n","      test_data_bothlabels['Old_label'] = test_data_bothlabels['Label'].copy()\n","\n","      test_sequences, test_labels, test_end_points, test_sleep_stages, index, label = prepare_classification_data(mydata_train = test_data_bothlabels, sleep_onset_threshold = convergence_point,\n","                                                                                                                          awake_threshold = convergence_point,\n","                                                                                                                   method = method, filling_limit = filling_limit,\n","                                                                                                                  window_size_minutes = sliding_window_size,\n","                                                                                                                  ifmissing = True,\n","                                                                                                                  random_seed = random_seed, resampling_method = None)\n","\n","      test_labels_and_end_points = [(test_labels[i], test_end_points[i]) for i in range(len(test_labels))]\n","\n","      # Create the test dataset and dataloader with labels and end points (for the regression head)\n","      test_dataset = SleepDataset(test_sequences, test_labels_and_end_points )\n","      test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","      model_parameters = {'convergence_point': convergence_point,\n","                          'initial_sleep_onset_threshold': initial_sleep_onset_threshold,\n","                          'sliding_window_size': sliding_window_size}\n","\n","      criterion= nn.BCEWithLogitsLoss()\n","\n","      model_string = f'{convergence_point}_convpoint_{initial_sleep_onset_threshold}_init_{sliding_window_size}_window'\n","      save_directory_model =  save_directory + '/' + model_string\n","      if not os.path.exists(save_directory_model):\n","          os.makedirs(save_directory_model)\n","\n","\n","      ########################## TRAIN AND EVALUATE  #############################\n","      test_predictions_class, test_actuals_class, test_prediction_probabilities_class, test_metrics_class = all_test_classification_model(model = model,\n","                                                                                                                                          label = label, test_labels = test_labels,\n","                                                                                                                                          test_end_points = test_end_points,\n","                                                                                                                                          test_sleep_stages= test_sleep_stages,\n","                                                                                                                                          test_loader = test_loader, device = device,\n","                                                                                                                                          criterion = criterion, index = index,\n","                                                                                                                                          ifprobabilities = True,\n","                                                                                                                                          ifplot = False,\n","                                                                                                                                          ifsaveplots=True,\n","                                                                                                                                          save_directory_epoch = save_directory_model,\n","                                                                                                                                          awake_threshold= convergence_point,\n","                                                                                                                                          sleep_onset_threshold=convergence_point,\n","                                                                                                                                          window_size_minutes= sliding_window_size)\n","      # Get the stats for probability change points and mean prediction times in participants on the validaiton set\n","      participant_dataloaders,  participant_dict = test_on_random_participants(data = test_data_bothlabels, model =  model, criterion = criterion, random_seed = random_seed,\n","                                                                                          ifplot = False, device = device, output_path_new =  save_directory_model, index = index,\n","                                                                                          convergence_point = convergence_point,\n","                                                                                          awake_window = convergence_point, pre_sleep_window = convergence_point,\n","                                                                                          ifsaveplots = True, window_size_minutes = sliding_window_size, ifmissing = True,\n","                                                                                          filling_limit = filling_limit, method = method, resampling_method = None)\n","\n","\n","      # joing two dictionaries\n","      test_metrics = {**model_parameters, **test_metrics_class,  **participant_dict}\n","      # populate the results_df dataframe with the results from the current epoch\n","      new_row = pd.DataFrame(test_metrics, index = [0])\n","\n","      # Open the results dataframe and add the new results\n","      results_df_all = pd.read_csv(os.path.join(results_saving_dir, 'results_final.csv'))\n","      results_df_all = pd.concat([results_df_all, new_row], ignore_index=True)\n","      results_df_all.to_csv(os.path.join(results_saving_dir, 'results_final.csv'), index=False)\n","      del results_df_all\n","      del new_row\n","      del test_metrics\n","\n","      # Clean up the cuda memory\n","      torch.cuda.empty_cache()\n","      # close all the plots\n","      plt.close('all')\n","      # clean up the memory\n","      gc.collect()\n","\n"],"metadata":{"id":"7Wp3tTx4C8pa","executionInfo":{"status":"ok","timestamp":1693956097623,"user_tz":-60,"elapsed":3,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["def extract_models_from_folder(folder_name,  convergence_points, initial_sleep_onset_thresholds, sliding_window_sizes, epoch = 2):\n","  models = []\n","  for convergence_point, initial_sleep_onset_threshold, sliding_window in zip(convergence_points, initial_sleep_onset_thresholds, sliding_window_sizes):\n","      model_dir = folder_name + '/' + f'convergence_point_{convergence_point}_starting_point_{initial_sleep_onset_threshold}_window_size_{sliding_window}'\n","      epoch_dir = model_dir + '/epoch_' + str(epoch)\n","      model_name = f'model_classification_presleep_{convergence_point}_awake_{convergence_point}_window_{sliding_window}.pt'\n","      model_dict = torch.load(epoch_dir + '/' + model_name)\n","      model = SleepOnsetRNNClassifier(input_size=86, hidden_size=128,\n","                                                     num_layers=2, dropout=0.0, l2=0.0).to(device)\n","      model.load_state_dict(model_dict)\n","\n","      models.append(model)\n","\n","  return models\n"],"metadata":{"id":"naxx4eHmDXt2","executionInfo":{"status":"ok","timestamp":1693956097623,"user_tz":-60,"elapsed":2,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":["### Actually test it"],"metadata":{"id":"4hDHskXfDc6e"}},{"cell_type":"code","source":["folder_name = '/content/drive/MyDrive/sleep onset datasets/FIXED_regression_head_rnn_schedule_learning_mixed_onset'\n","convergence_points = [4,12]\n","initial_sleep_onset_thresholds = [2, 6]\n","sliding_window_sizes = [1.5, 1]"],"metadata":{"id":"DWHYz4FlDexa","executionInfo":{"status":"ok","timestamp":1693956097623,"user_tz":-60,"elapsed":2,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["models_to_test = extract_models_from_folder(folder_name,  convergence_points, initial_sleep_onset_thresholds, sliding_window_sizes)"],"metadata":{"id":"sB64IFfZDhLK","executionInfo":{"status":"ok","timestamp":1693956108967,"user_tz":-60,"elapsed":11346,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["results_saving_dir = folder_name + '/test_best_models'\n","test_loop_RNN_regclass_schedule(data_test, models_to_test, convergence_points, initial_sleep_onset_thresholds, sliding_window_sizes,\n","                                filling_limit = 40, method = 'LOCF', batch_size = 64, results_saving_dir = results_saving_dir, ifresetresults = True, random_seed = 42)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"96EO8c1CDjmI","executionInfo":{"status":"ok","timestamp":1693957926709,"user_tz":-60,"elapsed":1548843,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}},"outputId":"aada65e7-1b09-4261-df1c-3cac48500153"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["Participant 1 of 55\n","Participant 2 of 55\n","Participant 3 of 55\n","Participant 4 of 55\n","Participant 5 of 55\n","Participant 6 of 55\n","Participant 7 of 55\n","Participant 8 of 55\n","Participant 9 of 55\n","Participant 10 of 55\n","Participant 11 of 55\n","Participant 12 of 55\n","Participant 13 of 55\n","Participant 14 of 55\n","Participant 15 of 55\n","Participant 16 of 55\n","Participant 17 of 55\n","Participant 18 of 55\n","Participant 19 of 55\n","Participant 20 of 55\n","Participant 21 of 55\n","Participant 22 of 55\n","Participant 23 of 55\n","Participant 24 of 55\n","Participant 25 of 55\n","Participant 26 of 55\n","Participant 27 of 55\n","Participant 28 of 55\n","Participant 29 of 55\n","Participant 30 of 55\n","Participant 31 of 55\n","Participant 32 of 55\n","Participant 33 of 55\n","Participant 34 of 55\n","Participant 35 of 55\n","Participant 36 of 55\n","Participant 37 of 55\n","Participant 38 of 55\n","Participant 39 of 55\n","Participant 40 of 55\n","Participant 41 of 55\n","Participant 42 of 55\n","Participant 43 of 55\n","Participant 44 of 55\n","Participant 45 of 55\n","Participant 46 of 55\n","Participant 47 of 55\n","Participant 48 of 55\n","Participant 49 of 55\n","Participant 50 of 55\n","Participant 51 of 55\n","Participant 52 of 55\n","Participant 53 of 55\n","Participant 54 of 55\n","Participant 55 of 55\n","Percentage of participants with only one crossing: 34.54545454545455%\n","Mean time of prediction for participants with only one crossing: 7.173684210526316 minutes\n","STD of time of prediction for participants with only one crossing: 7.203642507368145 minutes\n","Percentage of participants with only one crossing (smoothed): 34.54545454545455%\n","Mean time of prediction for participants with only one crossing (smoothed): 9.121052631578944 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 8.833642566757494 minutes\n","Participant 1 of 55\n","Participant 2 of 55\n","Participant 3 of 55\n","Participant 4 of 55\n","Participant 5 of 55\n","Participant 6 of 55\n","Participant 7 of 55\n","Participant 8 of 55\n","Participant 9 of 55\n","Participant 10 of 55\n","Participant 11 of 55\n","Participant 12 of 55\n","Participant 13 of 55\n","Participant 14 of 55\n","Participant 15 of 55\n","Participant 16 of 55\n","Participant 17 of 55\n","Participant 18 of 55\n","Participant 19 of 55\n","Participant 20 of 55\n","Participant 21 of 55\n","Participant 22 of 55\n","Participant 23 of 55\n","Participant 24 of 55\n","Participant 25 of 55\n","Participant 26 of 55\n","Participant 27 of 55\n","Participant 28 of 55\n","Participant 29 of 55\n","Participant 30 of 55\n","Participant 31 of 55\n","Participant 32 of 55\n","Participant 33 of 55\n","Participant 34 of 55\n","Participant 35 of 55\n","Participant 36 of 55\n","Participant 37 of 55\n","Participant 38 of 55\n","Participant 39 of 55\n","Participant 40 of 55\n","Participant 41 of 55\n","Participant 42 of 55\n","Participant 43 of 55\n","Participant 44 of 55\n","Participant 45 of 55\n","Participant 46 of 55\n","Participant 47 of 55\n","Participant 48 of 55\n","Participant 49 of 55\n","Participant 50 of 55\n","Participant 51 of 55\n","Participant 52 of 55\n","Participant 53 of 55\n","Participant 54 of 55\n","Participant 55 of 55\n","Percentage of participants with only one crossing: 20.0%\n","Mean time of prediction for participants with only one crossing: 24.20909090909091 minutes\n","STD of time of prediction for participants with only one crossing: 3.6360454406237817 minutes\n","Percentage of participants with only one crossing (smoothed): 20.0%\n","Mean time of prediction for participants with only one crossing (smoothed): 23.29090909090909 minutes\n","STD of time of prediction for participants with only one crossing (smoothed): 4.586631670697422 minutes\n"]}]},{"cell_type":"code","source":["results_saving_dir = folder_name + '/test_best_models'\n","results_final_test = pd.read_csv(f'{results_saving_dir}/results_final.csv')\n","columns =  ['convergence_point', 'initial_sleep_onset_threshold', 'sliding_window_size',  'one_crossing_percentage_smoothed',\n","       'mean_time_of_prediction_smoothed',\n","       'std_time_of_prediction_smoothed',  'f1_weighted', 'f1_pre_sleep',\n","       'f1_awake', 'precision',\n","       'recall', 'auc']\n","results_final_test_selected = results_final_test[columns]\n","\n","sorted_results = results_final_test_selected.sort_values(by=['one_crossing_percentage_smoothed'], ascending=False)\n","sorted_results = sorted_results.round(2)\n","headers = ['t_{PS}', 't_0_{PS}', 't_{SW}', 'R_ps', 'Mean(t_{pred})', 'STD(t_{pred})', 'F1_W', 'F1_{PS}','F1_{AW}', 'P', 'S', 'AUC']\n","sorted_results.columns = headers\n","\n","sorted_results.to_latex(f'{folder_name}/results_test_best.tex', index=False, float_format=\"%.2f\")\n","print(sorted_results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9MNDat09PzK9","executionInfo":{"status":"ok","timestamp":1693957927615,"user_tz":-60,"elapsed":922,"user":{"displayName":"Anastasia Ilina","userId":"11612942880361505799"}},"outputId":"f404e8b9-dcd1-4f88-a5fc-7bd53b462c28"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-56-a4d7857caf55>:15: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n","  sorted_results.to_latex(f'{folder_name}/results_test_best.tex', index=False, float_format=\"%.2f\")\n"]},{"output_type":"stream","name":"stdout","text":["   t_{PS}  t_0_{PS}  t_{SW}   R_ps  Mean(t_{pred})  STD(t_{pred})  F1_W  \\\n","0       4         2     1.5  34.55            9.12           8.83  0.80   \n","1      12         6     1.0  20.00           23.29           4.59  0.31   \n","\n","   F1_{PS}  F1_{AW}     P     S   AUC  \n","0     0.85     0.28  0.19  0.53  0.74  \n","1     0.16     0.56  0.40  0.97  0.64  \n"]}]}],"metadata":{"colab":{"collapsed_sections":["YkCnikvOVK8A","ElvUVfxvVJo4","WrpCngQKVJpJ"],"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}